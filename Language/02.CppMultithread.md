# C++11 多线程编程

C++11 新标准中引入了四个头文件来支持多线程编程，他们分别是 atomic thread mutex condition_variable future。

>- atomic：声明了两个类 , std::atomic 和 std::atomic_flag，另外还声明了一套 C 风格的原子类型和与 C 兼容的原子操作的函数。
>- thread：声明了 std::thread 类，另外 std::this_thread 命名空间也在该头文件中。
>- mutex：声明了与互斥量 (mutex) 相关的类，包括 std::mutex 系列类，std::lock_guard std::unique_lock 以及其他的类型和函数。
>- condition_variable：声明了与条件变量相关的类，包括 std::condition_variable 和 std::condition_variable_any。
>- future：该头文件主要声明了 std::promise,std::package_task 两个 Provider 类，以及 std::future 和 std::shared_future 两个 future 类，另外还有一些与之相关的类型和函数，std::async() 函数就声明在此头文件中。

## 线程 thread

>- std::thread()，创建线程一般会绑定一个底层的线程。若该 thread 还绑定好函数对象，则即刻将该函数运行于 thread 的底层线程。
>- joinable()：是否可以阻塞至该 thread 绑定的底层线程运行完毕。
>- join()：本线程阻塞直至该 thread 的底层线程运行完毕。
>- detach()：该 thread 绑定的底层线程分离出来，任该底层线程继续运行，thread 失去对该底层线程的控制。

## 互斥变量 mutex

为了避免多线程对共享变量的一段操作会发生冲突，引入了互斥体和锁。

>- std::mutex，互斥体，一般搭配锁使用，也可自己锁住自己（lock(),unlock()）。若互斥体被第二个锁请求锁住，则第二个锁所在线程被阻塞直至第一个锁解锁。
>- std::lock_guard，构造时请求上锁，释放时解锁，性能耗费较低。适用区域的多线程互斥操作。
>- std::unique_lock，更多功能、更灵活的锁，随时可解锁或重新锁上（减少锁的粒度），性能耗费比前者高一点点。适用灵活的区域的多线程互斥操作。一般来说使用 unique_lock 例子有：延迟锁定，锁的所有权需要在不同的作用域之间转移。

```cpp
// 为了避免死锁，常见的建议是始终使用相同的顺序锁定多个互斥变量。
// 锁定两个互斥锁的使用示例

struct bank_account {
    explicit bank_account(int balance) : balance(balance) {}
    int balance;
    std::mutex m;
};

void transfer(bank_account &from, bank_account &to, int amount) {
    // 锁定两个互斥而不死锁，提供了关于锁定给定互斥元的全或者无语义。
    std::lock(from.m, to.m);

    // 保证二个已锁定互斥在作用域结尾解锁
    // adopt_lock 告知 lock_gurad 互斥变量已经锁定，不要在构造函数中尝试锁定互斥变量
    std::lock_guard<std::mutex> lock1(from.m, std::adopt_lock);
    std::lock_guard<std::mutex> lock2(to.m, std::adopt_lock);

// 等价方法：

// defer_lock 用于表示在构造函数中，互斥变量应保持未锁定。
//    std::unique_lock<std::mutex> lock1(from.m, std::defer_lock);
//    std::unique_lock<std::mutex> lock2(to.m, std::defer_lock);

// 注意：在 unique_lock 对象上（而非互斥变量）上调用 std::lock()
//    std::lock(lock1, lock2);

    from.balance -= amount;
    to.balance += amount;
}

int main() {
    bank_account my_account(100);
    bank_account your_account(50);

    // thread 的参数默认都是值传递，使用引用语义则需要调用 std::ref()，也可能调用 std::move() 提高性能
    std::thread t1(transfer, std::ref(my_account), std::ref(your_account), 10);
    std::thread t2(transfer, std::ref(your_account), std::ref(my_account), 5);

    t1.join();
    t2.join();
}
```

## once_flag call_once

std::once_flag 和 std::call_once() 用于处理 once 操作。call_once() 比显示使用互斥体通常会有更低的开销，特别是初始化已经完成的时候。

```cpp
std::shared_ptr<resource> res;
std::once_flag flag;

void init() {
    res.reset(new resource);
}

void foo(){
    std::call_once(flag, init);
    res->do_something();
}
```

进一步的，c++11 中的局部静态变量是 thread safe，对于需要单一全局实例的场合，可以用作 call_once() 的替代品。

```cpp
class foo;

foo* get_foo_instance() {
    static foo instance;  // 初始化保证是线程安全的
    return &instance;
}
```

## 条件变量 condition_variable

条件变量一般是用来实现多个线程的等待队列，即主线程通知（notify）有活干了，则等待队列中的其它线程就会被唤醒。条件变量在使用时涉及到重复的加锁解锁，所以传入的互斥对像是 unique_lock，而非 lock_guard。

>- cond.wait(std::unique_lock\<std::mutex\>& lock, Predicate pred = \[\](){return true;}); pred() 为 true 时直接返回，pred() 为 false 时，lock 必须满足已被当前线程锁定的前提。执行原子地释放锁定，阻塞当前线程，并将其添加到等待 *this 的线程列表中。
>- notify_one()/notify_all()：激活某个或者所有等待的线程，被激活的线程重新获得锁。

虚假唤醒：处于等待的添加变量可以通过 notify_one()/notify_all() 进行唤醒，调用函数进行信号的唤醒时，处于等待的条件变量会重新进行互斥锁的竞争。没有得到互斥锁的线程就会发生等待转移（wait morphing），从等待信号量的队列中转移到等待互斥锁的队列中，一旦获取到互斥锁的所有权就会接着向下执行，但是此时其他线程已经执行并重置了执行条件 (例如一个活只需要两个线程来干，通知完两个线程后重置执行条件)，这可能导致该线程执行引发未定义的错误。

```cpp
// 不能应对虚假唤醒
if(pred()){
   cv.wait(lock);
}
// 利用 while 重复判断执行条件，可以应对虚假唤醒
while(!pred()){
   cv.wait(lock);
}
// C++11 提供了更方便的语法，将判断条件作为一个参数，实际上等价于前者
cv.wait(lock, pred);
```

## 获取者 future

用于访问共享状态（即获取值）。当 future 的状态还不是 ready 时就调用一个绑定的 promise, packaged_task 等的析构函数，会在期望里存储一个异常。std::future 有局限性，在很多线程等待时，只有一个线程能获取等待结果。

```cpp
std::future<int> f = std::async(func);
int res = f.get();
```

>- share()：分享同一个共享状态给另一个 future。
>- wait()：若共享状态不是 ready，则阻塞直至 ready。
>- get()：获得共享状态的值，若共享状态不是 ready，则阻塞直至 ready。

std::shared_future

当需要多个线程等待相同的事件的结果，即多处访问同一个共享状态，需要用 std::shared_future 来替代 std::future。shared_future 与 future 类似，但 shared_future 可以拷贝、多个 shared_future 可以共享某个共享状态的最终结果，即共享状态的某个值或者异常。shared_future 可通过某个 future 对象隐式转换，或通过 future::share() 显示转换，无论哪种转换，被转换的那个 future 对象都会变为 not-valid。

## 提供者 promise -> future

构造时产生一个未就绪的共享状态（包含存储的 T 值和是否就绪的状态）。可设置 T 值，并让状态变为 ready。std::promise 允许 move 语义（右值构造，右值赋值），但不允许拷贝（拷贝构造、赋值），std::future 亦然。std::promise 和 std::future 合作共同实现了多线程间通信。

```cpp
void read(std::future<std::string> *future) {
    // future 会一直阻塞，直到有值到来
    std::cout << future->get() << std::endl;
}

int main() {
    // promise 相当于生产者
    std::promise<std::string> promise;
    // future 相当于消费者 , 右值构造
    std::future<std::string> future = promise.get_future();
    // 另一线程中通过 future 来读取 promise 的值
    std::thread thread(read, &future);
    // 让 read 等一会儿:)
    std::this_thread::sleep_for(seconds(1));
    // 设置 value
    promise.set_value("hello future");
    // 等待线程执行完成
    thread.join();

    return 0;
}
```

>- get_future()：共享状态绑定到 future 对象。一个 std::promise 实例只能与一个 std::future 关联共享状态，当在同一个 std::promise 上反复调用 get_future 会抛出 future_error 异常。
>- set_value()：设置共享状态的 T 值，并让状态变为 ready，则绑定的 future 对象可 get()。set_value 只能被调用一次，多次调用会抛出 std::future_error 异常。
>- set_exception()：为 promise 设置异常，此后 promise 的共享状态变标志变为 ready。
>- set_value_at_thread_exit()：设置共享状态的值，但是不将共享状态的标志设置为 ready，当线程退出时该 promise 对象会自动设置为 ready。调用 future::get 的线程会被阻塞；当持有 promise 的线程退出时，调用 future::get 的线程解除阻塞，同时 get 返回 set_value_at_thread_exit 所设置的值。该函数已经设置了 promise 共享状态的值，如果在线程结束之前有其他设置或者修改共享状态的值的操作，则会抛出 future_error(promise_already_satisfied)。

如果 promise 直到销毁时，都未设置过任何值，则 promise 会在析构时自动设置为 std::future_error，这会造成 std::future.get() 抛出 std::future_error 异常。

## 可异步执行的打包任务 packaged_task -> future

构造时绑定一个函数对象，也产生一个未就绪的共享状态。通过 thread 启动或者仿函数形式启动该函数对象。但是相比 promise，没有提供 set_value() 公用接口，而是当执行完绑定的函数对象，其执行结果返回值或所抛异常被存储于能通过 std::future 对象访问的共享状态中。packaged_task 对象需要交付给 thread 运行或者显示调用。

>- get_future()：共享状态绑定到 future 对象。
>- valid()：检查当前 packaged_task 是否和一个有效的共享状态相关联，默认构造函数生成的 packaged_task 对象返回 false，除非中间进行了 move 赋值操作或者 swap 操作。
>- make_ready_at_thread_exit()：该函数会调用被包装的任务，并向任务传递参数，类似 std::packaged_task 的 operator() 成员函数。但是与 operator() 函数不同的是，make_ready_at_thread_exit 并不会立即设置共享状态的标志为 ready，而是在线程退出时设置共享状态的标志。该函数已经设置了 promise 共享状态的值，如果在线程结束之前有其他设置或者修改共享状态的值的操作，则会抛出 std::future_error(promise_already_satisfied)。
>- reset()：packaged_task 相比与 promice 是可以重复使用的。调用 reset 后，需要重新 get_future，以便获取下次执行的结果。由于是重新构造了 promise，因此 reset 操作并不会影响之前调用的 make_ready_at_thread_exit 结果，也即之前的定制的行为在线程退出时仍会发生。
>- operator()(Args... args)：调用该 packaged_task 对象所包装的对象 (通常为函数指针，函数对象，lambda 表达式等)，传入的参数为 args。如果成功调用 packaged_task 所包装的对象，则返回值被保存在 future 中；如果调用 packaged_task 所包装的对象失败且抛出了异常，则异常也会被保存在 future 中。以上两种情况都使共享状态的标志变为 ready。

```cpp
int sum(int a, int b) {
    return a + b;
}

int main() {
    std::packaged_task<int(int,int)> task(sum);
    std::future<int> future = task.get_future();

    // std::promise 一样，std::packaged_task 支持 move，但不支持拷贝
    // std::thread 的第一个参数不止是函数，还可以是一个可调用对象，即支持 operator()(Args...) 操作
    std::thread t(std::move(task), 1, 2);
    // 等待异步计算结果
    std::cout << "1 + 2 => " << future.get() << std::endl;

    t.join();
    return 0;
}
```

## 自动异步执行的任务 async -> future

std::async(std::launch::async | std::launch::deferred, Func, Args...)

异步执行一个函数，其函数执行完后的返还值绑定给使用 std::async 的 futrue（其实是封装了 thread, packged_task 的功能，使异步执行一个任务更为方便）。若用创建 std::thread 执行异步行为，硬件底层线程可能不足，产生错误。而 std::async 将这些底层细节掩盖住，如果使用默认参数则与标准库的线程管理组件一起承担线程创建和销毁、避免过载、负责均衡的责任。所以尽量使用以任务为驱动的 async 操作设计，而不是以线程为驱动的 thread 设计。

std::async 中的第一个参数是启动策略，它控制 std::async 的异步行为，可以用三种不同的启动策略来创建。

>- std::launch::async 参数 保证异步行为，即传递函数将在单独的线程中执行。
>- std::launch::deferred 参数 当其他线程调用 get()/wait() 来访问共享状态时，将调用非异步行为。
>- std::launch::async | std::launch::deferred 参数 是默认行为。有了这个启动策略，它可以异步运行或不运行，这取决于系统的负载。

```cpp
std::future<std::string> resultFromDB = std::async(std::launch::async, fetchDataFromDB, "Data1");
std::string data1 = resultDromDB.get();
```

## 时间库 chrono

多线程中的阻塞调用，比如 wait() 系列，基本上都提供了有时间限制的重载版本：wait_for() 等待一段时间，wait_until() 等待至指定的时间点。休眠线程使用 std::this_thread::sleep_for() 或者 std::this_thread::sleep_until()。基于时间段的等待使用类库内部的匀速时钟来衡量时间。

时间段和时间都有相应的转换函数 duration_cat<>()/time_point_cast<>()。时间段和时间点都不提供流式输出，时间段提供 count() 方法，时间点提供 since_time_since_epoch() 方法。

```cpp
// 时间段
std::chrono::duration<rep, period> d(n);
// period 表示一个单位时间周期，类型 std::ratio<x, y> 表示 x/y * 1s -> std::ratio<1, 25> 即 1/25 s ，ratio<1, -2> 表示单位时间是-0.5 秒。
// rep 是一个精度类型，比如 int float 等，表示 rep 个 period。上述声明的 d(n) 即表示 n 个 period 的时间长度。

// 预定义的时间长度单位
typedef duration <Rep, ratio<3600,1>> hours;
typedef duration <Rep, ratio<60,1>> minutes;
typedef duration <Rep, ratio<1,1>> seconds;
typedef duration <Rep, ratio<1,1000>> milliseconds;
typedef duration <Rep, ratio<1,1000000>> microseconds;
typedef duration <Rep, ratio<1,1000000000>> nanoseconds;

// 举个例子
timespec t1, t2;
clock_gettime(CLOCK_MONOTONIC, &t1);
std::chrono::duration<float, std::ratio<60, 1>> s60(0.1); // 0.1 * 1/60 s -> 6s
std::this_thread::sleep_for(s60 + std::chrono::seconds(1)); // 1s
std::chrono::milliseconds  ms = std::chrono::duration_cast<std::chrono::milliseconds > (s60); // duration_cast
clock_gettime(CLOCK_MONOTONIC, &t2);
cout << t2.tv_sec - t1.tv_sec << endl; // 7s
```

```cpp
// 时间点

// 始终的当前时间可以通过该时钟类型的静态成员函数 now() 来获取
std::chrono::system_clock::time_point tp = std::chrono::system_clock::now();
cout << tp.time_since_epoch().count() << endl; // 输出该时间点自 1970 年
// 时钟类型有
// system_clock：当前系统范围 (即对各进程都一致) 的一个实时的日历时钟，可以和 time_t 相互转换
// steady_clock：当前系统实现的一个维定时钟，该时钟的每个时间嘀嗒单位是均匀的
// high_resolution_clock：当前系统实现的一个高分辨率时钟
```

## 内存模型

原子类型的大多数 API 都需要程序员提供一个 std::memory_order（可译为内存序，访存顺序）的枚举类型值作为参数，比如：atomic_store，atomic_load，atomic_exchange，atomic_compare_exchange 等 API 的最后一个形参为 std::memory_order order，默认值是 std::memory_order_seq_cst（顺序一致性）。一般来讲，内存模型可分为静态内存模型和动态内存模型，静态内存模型主要涉及类的对象在内存中是如何存放的。动态内存模型可理解为存储一致性模型，主要是从行为（behavioral）方面来看多个线程对同一个对象同时（读写）操作时（concurrency）所做的约束，动态内存模型理解起来稍微复杂一些，涉及了内存，Cache，CPU 各个层次的交互，尤其是在共享存储系统中，为了保证程序执行的正确性，就需要对访存事件施加严格的限制。

文献中常见的存储一致性模型包括顺序一致性模型，处理器一致性模型，弱一致性模型，释放一致性模型，急切更新释放一致性模型、懒惰更新释放一致性模型，域一致性模型以及单项一致性模型。不同的存储一致性模型对访存事件次序的限制不同，因而对程序员的要求和所得到的的性能也不一样。存储一致性模型对访存事件次序施加的限制越弱，我们就越有利于提高程序的性能，但编程实现上更困难。

顺序一致性模型由 Lamport 于 1979 年提出。顺序一致性模型最好理解但代价太大。如果在共享存储系统中多机并行执行的结果等于把每一个处理器所执行的指令流按照某种方式顺序地交织在一起在单机上执行的结果，则该共享存储系统是顺序一致性的。

顺序一致性不仅在共享存储系统上适用，在多处理器和多线程环境下也同样适用。而在多处理器和多线程环境下理解顺序一致性包括两个方面，(1) 从多个线程平行角度来看，程序最终的执行结果相当于多个线程某种交织执行的结果，(2) 从单个线程内部执行顺序来看，该线程中的指令是按照程序事先已规定的顺序执行的 (即不考虑运行时 CPU 乱序执行和 Memory Reorder)。

现代的 CPU 大都支持多发射和乱序执行，在乱序执行时，指令被执行的逻辑可能和程序汇编指令的逻辑不一致，在单线程条件下，CPU 的乱序执行不会带来大问题，但是在多核多线程时代，当多线程共享某一变量时，不同线程对共享变量的读写就应该格外小心，不适当的乱序执行可能导致程序运行错误。因此，CPU 的乱序执行也需要作出适当的约束。

那么这个约束是什么呢？答曰：内存模型。C++ 程序员要想写出高性能的多线程程序必须理解内存模型，编译器会给你的程序做优化 (静态)，CPU 为了提升性能也有乱序执行 (动态)，总之，程序在最终执行时并不会按照你之前的原始代码顺序来执行，因此内存模型是程序员、编译器，CPU 之间的契约，遵守契约后大家就各自做优化，从而尽可能提高程序的性能。

```cpp
enum memory_order {
    memory_order_relaxed, // 强烈建议不要使用
    memory_order_consume, // c++17 起 不建议使用 规范修订中
    memory_order_acquire,
    memory_order_release,
    memory_order_acq_rel, // 如果你是大佬 可以试试
    memory_order_seq_cst  // 默认
};
```

std::memory_order 规定了普通访存操作和相邻的原子访存操作之间的次序是如何安排的，在多核系统中，当多个线程同时读写多个变量时，其中的某个线程所看到的变量值的改变顺序可能和其他线程写入变量值的次序不相同。同时不同的线程所观察到的某变量被修改次序也可能不相同。然而，如果保证所有对原子变量的操作都是顺序的话，可能对程序的性能影响很大，因此可以通过 std::memory_order 来指定编译器对访存次序所做的限制。在原子类型的 API 中可以通过额外的参数指定该原子操作的访存次序 (内存序)，默认的内存序是 std::memory_order_seq_cst。

上述访存次序可以分为 3 类，顺序一致性模型 (std::memory_order_seq_cst)，Acquire-Release 模型 (std::memory_order_consume, std::memory_order_acquire, std::memory_order_release, std::memory_order_acq_rel) 和 Relax 模型 (std::memory_order_relaxed)。

>- memory_order_seq_cst 顺序一致性模型，这个是默认提供的最强的一致性模型。
>- memory_order_release/acquire/consume 提供 release、acquire 或者 consume, release 语意的一致性保障。在这种序列模型下，原子 load 操作是 acquire 操作 (memory_order_acquire), 原子 store 操作是 release 操作 (memory_order_release), 原子 read_modify_write 操作 (如 fetch_add() exchange()) 可以是 acquire, release 或两者皆是 (memory_order_acq_rel)。同步是成对出现的，它出现在一个进行 release 操作和一个进行 acquire 操作的线程间。一个 release 操作 syncrhonized-with 一个想要读取刚才被写的值的 acquire 操作。这意味着不同线程仍然看到了不一样的次序，但这次序是被限制过了的。当在此再引入 happens-before 关系时，就可以实现更大规模的顺序关系。memory_order_consume 没搞懂。
>- memory_order_relaxed 提供松散一致性模型保障，不提供 operation order 保证。

三种不同的内存模型在不同类型的 CPU 上 (如 X86，ARM，PowerPC 等) 所带来的代价也不一样。例如，在 X86 或者 X86-64 平台下，Acquire-Release 类型的访存序不需要额外的指令来保证原子性，即使顺序一致性类型操作也只需要在写操作（Store）时施加少量的限制，而在读操作（Load）则不需要花费额外的代价来保证原子性。

顺序一致性符合直觉，非顺序一致性下，事件不再有单一的全局顺序，线程不必和事件的顺序一致。在没有其他的顺序约束时，唯一的要求时所有的线程对于每个独立变量的修改顺序达成一致。

### 理解 C++ 的 Memoy order

[link](http://senlinzhan.github.io/2017/12/04/cpp-memory-order/ " 原文 ")

如果不使用任何同步机制，在多线程中读写同一个变量，那么程序的结果是难以预料的。简单来说，编译器以及 CPU 的一些行为，会影响到程序的执行结果：即使是简单的语句也不保证是原子操作；CPU 可能会调整指令的执行顺序；在 CPU cache 的影响下，一个 CPU 执行了某个指令，不会立即被其它 CPU 看见。原子操作说的是，一个操作的状态要么就是未执行，要么就是已完成，不会看见中间状态。

多线程读写同一变量需要使用同步机制，最常见的同步机制就是 std::mutex 和 std::atomic。然而，从性能角度看，通常使用 std::atomic 会获得更好的性能。默认情况下，std::atomic 使用的是 Sequentially-consistent ordering（memory_order_seq_cst）。但在某些场景下，合理使用其它 ordering，可以让编译器优化生成的代码，从而提高性能。

#### Relaxed ordering

在这种模型下，std::atomic 的 load() 和 store() 都要带上 memory_order_relaxed 参数。Relaxed ordering 仅仅保证 load() 和 store() 是原子操作，除此之外，不提供任何跨线程的同步。

```cpp
std::atomic<int> x = 0;     // global variable
std::atomic<int> y = 0;     // global variable

// Thread-1:
r1 = y.load(memory_order_relaxed); // A
x.store(r1, memory_order_relaxed); // B

// Thread-2:
r2 = x.load(memory_order_relaxed); // C
y.store(42, memory_order_relaxed); // D
```

执行完上面的程序，可能出现 r1 == r2 == 42。理解这一点并不难，因为编译器允许调整 C 和 D 的执行顺序。如果程序的执行顺序是 D -> A -> B -> C，那么就会出现 r1 == r2 == 42。

如果某个操作只要求是原子操作，除此之外，不需要其它同步的保障，就可以使用 Relaxed ordering。程序计数器是一种典型的应用场景：

```cpp
#include <cassert>
#include <vector>
#include <iostream>
#include <thread>
#include <atomic>

std::atomic<int> cnt = {0};

void f() {
    for (int n = 0; n < 1000; ++n) {
        cnt.fetch_add(1, std::memory_order_relaxed);
    }
}

int main() {
    std::vector<std::thread> v;
    for (int n = 0; n < 10; ++n) {
        v.emplace_back(f);
    }
    for (auto& t : v) {
        t.join();
    }
    assert(cnt == 10000);
    return 0;
}
```

#### Release-Acquire ordering

在这种模型下，store() 使用 memory_order_release，而 load() 使用 memory_order_acquire。这种模型有两种效果，第一种是可以限制 CPU 指令的重排：在 store() 之前的所有读写操作，不允许被移动到这个 store() 的后面。在 load() 之后的所有读写操作，不允许被移动到这个 load() 的前面。

除此之外，还有另一种效果：假设 Thread-1 store() 的那个值，成功被 Thread-2 load() 到了，那么 Thread-1 在 store() 之前对内存的所有写入操作，此时对 Thread-2 来说，都是可见的。

```cpp
std::atomic<int> x = 0;     // global variable
std::atomic<int> y = 0;     // global variable

// Thread-1:
r1 = y.load(memory_order_relaxed); // A
x.store(r1, memory_order_release); // B

// Thread-2:
r2 = x.load(memory_order_acquire); // C
y.store(42, memory_order_relaxed); // D
```

首先 A 不允许被移动到 B 的后面。同样 D 也不允许被移动到 C 的前面。当 C 从 while 循环中退出了，说明 C 读取到了 B store() 的那个值，此时，Thread-2 保证能够看见 Thread-1 执行 B 之前的所有写入操作（也即是 A）。

```cpp
#include <thread>
#include <atomic>
#include <cassert>
#include <string>

std::atomic<bool> ready{false};
int data = 0;

void producer() {
    data = 100;
    ready.store(true, std::memory_order_release);
}

void consumer() {
    while (!ready.load(std::memory_order_acquire));
    assert(data == 100);
}

int main() {
    std::thread t1(producer);
    std::thread t2(consumer);
    t1.join();
    t2.join();
    return 0;
}
```

## 原子类型 atomic

最简单的原子类型: atomic_flag。atomic_flag 一种简单的原子布尔类型，只支持两种操作，test_and_set() 和 clear()。test_and_set() 函数检查 std::atomic_flag 标志，如果 std::atomic_flag 之前没有被设置过，则设置 std::atomic_flag 的标志，并返回先前该 std::atomic_flag 对象是否被设置过，如果之前 std::atomic_flag 对象已被设置，则返回 true，否则返回 false。clear() 恢复其为 clear 状态。

结合 std::atomic_flag::test_and_set() 和 std::atomic_flag::clear()，std::atomic_flag 对象可以当作一个简单的自旋锁使用。

```cpp
class spinlock_mutex {
public:
    spinlock_mutex():flag(ATOMIC_FLAG_INIT){}

public:
    void lock(){
        // set 失败返回原来的值 -> true
        // 则继续循环直到操作成功过返回原来的值 -> false
        // 真的有点反人类呢
        while(flag.test_and_set()){}
    }

    void unlock(){
        flag.clear();
    }
private:
    std::atomic_flag flag;

};
```

std::atomic 是模板类，一个模板类型为 T 的原子对象中封装了一个类型为 T 的值。原子类型对象的主要特点就是从不同线程访问不会导致数据竞争。

```cpp
template <class T> struct atomic;

// 基本成员函数

// 判断该对象是否具备 lock-free 特性。
bool is_lock_free();

// 修改被封装的值。
void store (T val, memory_order sync = memory_order_seq_cst);
// 读取被封装的值。
T load (memory_order sync = memory_order_seq_cst);

// 与 load 功能类似，也是读取被封装的值，operator T() 是类型转换 (type-cast) 操作。
operator T(); // 比如说一个 std::atomic<int> foo, foo == 0 这条语句就是取值和隐式类型转换。

// 读取并修改被封装的值，exchange 会将 val 指定的值替换掉之前该原子对象封装的值，并返回之前该原子对象封装的值。
T exchange (T val, memory_order sync = memory_order_seq_cst);

// 比较并交换被封装的值 (weak) 与参数 expected 所指定的值是否相等。
// 相等则用 val 替换原子对象的旧值，返回 true；
// 不相等，则用原子对象的旧值替换 expected，返回 false； -> 值返回的骚操作
bool compare_exchange_weak (T& expected, T val, memory_order sync = memory_order_seq_cst);
bool compare_exchange_strong (T& expected, T val, memory_order sync = memory_order_seq_cst);
// 这个重载版本，memory_order 的选择取决于比较的结果。
// 该函数直接比较原子对象所封装的值与参数 expected 的物理内容，所以某些情况下，对象的比较操作在使用 operator==() 判断时相等，但 compare_exchange_weak 判断时却可能失败，因为对象底层的物理内容中可能存在位对齐或其他逻辑表示相同但是物理表示不同的值 (比如 true 和 2 或 3，它们在逻辑上都表示 " 真 "，但在物理上两者的表示并不相同)。
bool compare_exchange_weak (T& expected, T val, memory_order success, memory_order failure);
bool compare_exchange_strong (T& expected, T val, memory_order success, memory_order failure);

// weak 版本的 CAS 允许偶然出乎意料的返回（比如在字段值和期待值一样的时候却返回了 false），不过在一些循环算法中，这是可以接受的。通常它比起 strong 有更高的性能。

/*

想要性能，使用 compare_exchange_weak+ 循环来处理。
想要简单，使用 compare_exchange_strong。
如果是 x86 平台，两者没区别
如果想在移值的时候，拿到高性能，用 compare_exchange_weak。

*/
```

C++11 标准库中的 std::atomic 针对整形 (integral) 和指针类型的特化版本新增了一些算术运算和逻辑运算操作。

```cpp
// 将原子对象的封装值加 val，并返回原子对象的旧值。如果第二个参数不指定，等价于 +=
integral fetch_add(integral, memory_order = memory_order_seq_cst);
// 将原子对象的封装值减 val，并返回原子对象的旧值。如果第二个参数不指定，等价于 -=
integral fetch_sub(integral, memory_order = memory_order_seq_cst);
// 只适用于整型
integral fetch_and(integral, memory_order = memory_order_seq_cst);
// 只适用于整型
integral fetch_or(integral, memory_order = memory_order_seq_cst);
// 只适用于整型
integral fetch_xor(integral, memory_order = memory_order_seq_cst);

integral operator++(int);
integral operator--(int);
integral operator++();
integral operator--();
integral operator+=(integral);
integral operator-=(integral);
integral operator&=(integral);
integral operator|=(integral);
integral operator^=(integral);
```

## thread_local

thread_local 变量在线程创建时生成 (不同编译器实现略有差异，但在线程内变量第一次使用前必然已构造完毕)。线程结束时被销毁 (析构，利用析构特性，thread_local 变量可以感知线程销毁事件)。每个线程都拥有其自己的变量副本。thread_local 可以和 static 或 extern 联合使用，这将会影响变量的链接属性。

```cpp
class A {
public:
  A() {
    std::cout << std::this_thread::get_id()
              << " " << __FUNCTION__
              << "(" << (void *)this << ")"
              << std::endl;
  }
  ~A() {
    std::cout << std::this_thread::get_id()
              << " " << __FUNCTION__
              << "(" << (void *)this << ")"
              << std::endl;
  }
  // 线程中，第一次使用前初始化
  void doSth() {}
};

thread_local A a;

int main() {
  a.doSth();
  std::thread t([]() {
    std::cout << "Thread: "
              << std::this_thread::get_id()
              << " entered" << std::endl;
    a.doSth();
  });
  t.join();
  return 0;
}

...
$> g++ -std=c++11 -o debug/tls.out ./thread_local.cpp
$> ./debug/tls.out
01 A(0xc00720)
Thread: 02 entered
02 A(0xc02ee0)
02 ~A(0xc02ee0)
01 ~A(0xc00720)
$>
```

## 为并发设计数据结构的准则：保证存取是安全的以及允许真正的并发存取

1. 保证当数据结构不变性被别的线程破坏时的状态不会被被别的线程看到。
2. 注意避免数据结构借口所固有的竞争，通过为完整操作提供函数，而不是提供操作步骤。
3. 注意当出现例外时，数据结构是怎么样来保证不变性不被破坏的。
4. 当使用数据结构时，通过限制锁的范围和避免使用嵌套锁，来降低死锁的机会。

## 编写无锁数据结构的准则

1. 使用 std::memory_order_seq_cst 作为原型，避免过早优化。
2. 使用无锁内存回收模式。无锁代码的最大问题之一就是管理内存。
3. 当心 ABA 问题。线程 1 读取原子变量 x 并在后续的调用中阻塞，中途变量发生变化但又改回去了，线程 1 重新获得 x 并执行了比较 / 交换操作，但这个值可能不是原来的值了。避免这种问题的方法就是在变量 x 上加一个 ABA 计数器。
4. 识别忙于等待的循环以及辅助其他线程。

## 线程池

最简单的线程池

线程池最简单的形式是含有一个固定数量的工作线程（典型的数量是 std::thread::hardware_concurrency() 的返回值）来处理任务。当有任务需要处理时，调用一个函数将任务放到等待队列中；每个工作线程都是从该队列中取出任务，执行完之后继续取出更多的任务来处理。

如果所有的任务是完全独立的，工作线程之间互不依赖，线程池可以设计的非常简单。但存在许多简单线程池无法满足需求的情况，在这些情况下可能会引发一些问题，比如死锁。

等待提交给线程池的任务

在提交任务时，可以让提交函数返回一个任务句柄，利用这个句柄等待任务结束。这个任务句柄包装了条件变量或者其他简化使用线程池的代码。比如使用 std::packaged_task 提交任务，并返回一个 std::future 来等保存任务的返回值和允许调用者等待任务结束。

等待其他任务的任务

线程池的线程数量有限，如果线程上的任务都在等待一个未被分配到线程的任务，则会发生死锁。为了解决这种情况，可以让线程在等待某个未完成任务时，重新取一个待处理的任务来解决这个问题。这个工作可以交给线程是来实现，即提供类似于 run_pending_task 这样的接口。

避免工作队列上的竞争

当所有工作线程共享同一个工作队列时，高并发情况下工作队列的竞争会越来越多。即使使用无锁队列，虽然没有显示的等待，但是乒乓缓存会非常耗时。避免乒乓缓存的一个方法是在每一个线程都是用一个单独的工作队列，每个线程将新的任务添加到它自己的队列中，只有当自己的队列为空时，才从全局的工作队列取任务。可以使用 thread_local 变量来保证每个线程有一个自己的工作队列再加上一个全局的工作队列。这样能够很好的降低对全局工作队列的竞争，但是当任务分布不均匀时，可能导致一些线程的私有队列中有大量的任务，另外一些则没有任务处理。解决的办法是允许线程在自己的私有队列以及全局队列中都没有任务时，从其他线程的队列中窃取工作。

中断线程

没看明白，boost::thread 提供了相关的 api。

## 并行算法

三个标注执行策略：

>- std::execution::sequenced_policy
>- std::execution::parallel_policy
>- std::execution::parallel_unsequenced_policy

这些类都定义在 execution 头文件中。这个头文件中也定义了三个相关的策略对象可以传递到算法中：std::execution::seq std::execution::par std::execution::par_unseq。除了复制这三种类型的对象外，不能以自己的方式对执行策略对象进行构造，因为它们可能有一些特殊的初始化要求。实现还会定义一些其他的执行策略，但开发者不能自定义执行策略。

将执行策略传递给标准算法库中的算法，那么算法的行为就由执行策略控制。这会有几方面的影响：

>- 算法复杂化 除了对并行的管理调度开销外，并行算法的核心操作将会被多次执行。复杂度的变化根据每个算法不同会有所变化，取决于库实现和平台，而不是传递给算法的数据。
>- 抛出异常时的行为 具有执行策略的算法在执行期间触发异常，则结果由执行策略确定。如果有异常未被捕获，那么标准执行策略都会调用 std::terminate。如果标准库无法提供给内部操作足够的资源，则在无执行策略算法执行时，会触发 std::bad_alloc 异常。
>- 算法执行的位置、方式和时间 相应执行策略指定使用那些代理来执行算法，无论这些代理是 “普通” 线程、向量流、GPU 线程，还是其他的什么。执行策略还将对算法步骤进行执行时的约束和安排：是否以特定的顺序运行，算法步骤之间是否可以交错，或彼此并行运行等。

有执行策略和没有执行策略的函数列表间有一个重要的区别，这只会影响到一些算法：如果 “普通” 算法允许输入迭代器或输出迭代器，那执行策略的重载则需要前向迭代器。因为输入迭代器是单向迭代的：只能访问当前元素，并且不能将迭代器存储到以前的元素。输出迭代器只允许写入当前元素：不能在写入后面的元素后，后退再写入前面的元素。

C++ 标准库定义了五类迭代器：输入迭代器、输出迭代器、正向迭代器、双向迭代器和随机访问迭代器。

>- 输入迭代器是用于检索值的单向迭代器。通常用于控制台或网络的输入，或生成序列。该迭代器的任何副本都是无效的。
>- 输出迭代器是用于向单向迭代器写入值。通常输出到文件或向容器添加值。该迭代器会使该迭代器的任何副本失效。
>- 前向迭代器是通过数据不变进行单向迭代的多路径迭代器。虽然迭代器不能返回到前一个元素，但是可以存储前面元素的副本，并使用它们引用。前向迭代器返回对元素的实际引用，因此可以用于读写（如果目标是不是常量）。
>- 双向迭代器是像前向迭代器一样的多路径迭代器，但是它也可以后向访问之前的元素。
>- 随机访问迭代器是可以像双向迭代器一样前进和后退的多路径迭代器，是它们比单个元素大的跨距前进和后退，并且可以使用数组索引运算符，在偏移位置直接访问元素。

```cpp
// 输入迭代器 只能前向读取的迭代器，支持 ++，不能--
#include <iostream>
#include <iterator>

using namespace std;

int main() {
    istream_iterator<int> it(cin);
    istream_iterator<int> eof;
    while (it != eof) {
        cout << *it << endl;
        ++it;
    }
    cout << "over" << endl;
    return 0;
}

// 输出迭代器 只能前向写入的迭代器，支持 ++，不能--
#include <iostream>
#include <iterator>

using namespace std;

int main() {
    ostream_iterator<int> it(cout);
    for (int i = 0; i < 5; i++) {
        *it = i;
        it++;
    }
    cout << endl;
}
```

### std::execution::sequenced_policy

顺序策略并不是并行策略：它使用强制的方式实现，在执行线程上函数的所有操作。但它仍然是一个执行策略，因此对算法的复杂性和异常影响与其他标准执行策略相同。以该执行策略类型为一种独有类型，对并行算法重载消歧义，并要求并行算法的执行可以不并行化。以此策略调用（通常以 std::execution::seq 指定）的并行算法中，元素访问函数的调用在调用方线程中是非确定顺序的。

```cpp
// not work
std::vector<int> v(1000);
int count=0;
std::for_each(std::execution::seq,v.begin(),v.end(),
  [&](int& x){ x=++count; });
```

### std::execution::parallel_policy

并行策略提供了在多个线程下运行的算法版本。以该执行策略类型为一种独有类型，对并行算法重载消歧义，并指示并行算法的执行可以并行化。以此策略调用（通常以 std::execution::par 指定）的并行算法中，元素访问函数的调用允许在调用方线程，或由库隐式创建的线程中执行，以支持并行算法执行。任何执行于同一线程中的这种调用彼此间是非确定顺序的。

这就对算法所使用的迭代器、相关值和可调用对象有了额外的要求：想要并行调用，他们间就不能有数据竞争，也不能依赖于线程上运行的其他操作，或者依赖的操作不能在同一线程上。

```cpp
// not work
std::vector<int> v(1000);
std::atomic<int> count=0;
std::for_each(std::execution::seq,v.begin(),v.end(),
  [&](int& x){ x=++count; });
```

### std::execution::parallel_unsequenced_policy

并行不排序策略提供了最大程度的并行化算法，用以得到对算法使用的迭代器、相关值和可调用对象的严格要求。使用并行不排序策略调用的算法，可以在任意线程上执行，这些线程彼此间没有顺序。以该执行策略类型为一种独有类型，对并行算法重载消歧义，并指示并行算法的执行可以并行化、向量化，或在线程间迁移（例如用亲窃取的调度器）。容许以此策略调用的并行算法中的元素访问函数调用在未指定线程中以无序方式执行，并相对于每个线程中的另一调用无顺序。

使用并行不排序策略时，算法使用的迭代器、相关值和可调用对象不能使用任何形式的同步，也不能调用任何需要同步的函数。也就是，必须对相关的元素或基于该元素可以访问的数据进行操作，并且不能修改线程之间或元素之前共享的状态。

这意味着不能使用互斥量或原子变量，或前面章节中描述的任何其他同步机制，以确保多线程的访问是安全的；相反，必须依赖于算法本身，而不是使用多个线程访问同一个元素，并在调用并行算法之外使用外部同步，从而避免其他线程访问数据。

### others

#### detach()

std::thread::detach just allows the wrapper to be destroyed without throwing an exception; the actual thread's stack and OS resource reclamation happens in the thread's execution context after your thread's main function exits, and that should be 100% automatic.

detach() 一个线程后，该线程抛出异常不会导致主线程退出；但是主线程退出依然后使得 detach() 后的线程自动销毁回收。所以应当尽量使用 join() 来保证程序运行始终在预期的状态。

### tips

线程是宝贵的，一个程序可以使用几个或者十几个线程，但是一台机器上不应同时运行几百个、几千个用户线程，这会增加内核调度器的负担，降低整体性能。

线程的创建和销毁是有代价的，一个程序最好在一开始创建所需的线程，并一直反复使用。不要在运行期间反复创建和销毁线程，如果有必要这么做，也要尽量降低频率（一分钟一次或者更低）。

每个线程应该有明确的职责，例如 IO 线程、计算线程等。

线程之间的交互应该尽量简单，理想情况下线程之间只用消息传递（例如阻塞队列）方式交互。如果使用锁，那么最好避免一个线程持有超过一把的锁，这样可以彻底防止死锁。

要预先考虑清除一个 mutable shared 对象将会暴露给哪些线程，每个线程是读还是写，读写有无可能并发进行。

## false sharing

SMP（对称多处理）架构简单的说就是多个 CPU 核，共享同一个内存和总线。L1 cache 也叫芯片缓存，一般是 CPU Core 私有的，即每个 CPU 核一个，L2 cache 可能是私有的也可能是部分共享的，L3 cache 则多数是共享的。false-sharing 是在 SMP 的架构下常见的问题。

CPU 利用 cache 和内存之间交换数据的最小粒度不是字节，而是称为 cache line 的一块固定大小的区域，缓存行是内存交换的实际单位。缓存行是 2 的整数幂个连续字节，一般为 32-256 个字节，最常见的缓存行大小是 64 个字节。

在写多线程代码时，为了避免使用锁，通常会采用这样的数据结构：根据线程的数目，安排一个数组， 每个线程一个项，互相不冲突。从逻辑上看这样的设计无懈可击，但是实践的过程可能会发现有些场景下非但没提高执行速度，反而会性能很差，而且年轻司机通常很难定位问题所在。

问题在于 cpu 的 cache line，当多线程修改互相独立的变量时，如果这些变量共享同一个缓存行，就会无意中影响彼此的性能，这就是伪共享，即 false-sharing。

false-sharing 避免方法：把每个项凑齐 cache line 的长度，即可实现隔离，虽然这不可避免的会浪费一些内存。对于共享数组而言，增大数组元素的间隔使得由不同线程存取的数组元素位于不同的 cache line 上，使一个核上的 Cache line 修改不会影响其它核；或者在每个线程中创建全局数组的本地拷贝，然后执行结束后再写回全局数组，此方法比较粗暴不优雅。对于共享结构体而言，使每个结构体成员变量按照 Cache Line 大小（一般 64B）对齐。可能需要使用#pragma 宏。

## thread name

```cpp
#include <sys/prctl.h>
int prctl(int option, unsigned long arg2, unsigned long arg3, unsigned long arg4, unsigned long arg5);\

// prctl()  is  called  with a first argument describing what to do (with values defined in <linux/prctl.h>), and further arguments with a significance depending on the first one.
```

PR_SET_NAME (since Linux 2.6.9) Set the name of the calling thread, using the value in the location pointed to by (char *) arg2.  The name can be up to 16 bytes long, and should be null- terminated if it contains fewer bytes.  This is the same attribute that can be set via pthread_setname_np(3) and  retrieved  using `pthread_getname_np()`. The attribute is likewise accessible via /proc/self/task/[tid]/comm, where tid is the name of the calling thread.

PR_GET_NAME (since Linux 2.6.11) Return  the  name of the calling thread, in the buffer pointed to by (char *) arg2.  The buffer should allow space for up to 16 bytes; the returned string will be null-terminated if it is shorter than that.

```cpp
#define _GNU_SOURCE             /* See feature_test_macros(7) */
#include <pthread.h>
int pthread_setname_np(pthread_t thread, const char *name);
int pthread_getname_np(pthread_t thread, const char *name, size_t len);
```

## thread 标识

```cpp
// 获取进程 ID
#include <unistd.h>  
pid_t getpid(void);

// The gettid() system call first appeared on Linux in kernel 2.4.11.
// gettid() returns the thread ID of the current process. All processes in the same thread group have the same PID, but each one has a unique TID.
// Glibc does not provide a wrapper for this system call; call it using syscall.The thread ID returned by this call is not the same thing as a POSIX thread ID (i.e., the opaque value returned by pthread_self(3)).
// pid_t int 类型
#include <syscall.h>
pid_t gettid() {
    return (pid_t)syscall(__NR_gettid);
}

// Thread IDs are guaranteed to be unique only within a process.  A thread ID may be reused after a terminated thread has been joined, or a detached thread has terminated.
// pthread_t long int 类型
// 使用 gettid() 而非 pthread_self()
#include <pthread.h>
pthread_t pthread_self(void);
```
