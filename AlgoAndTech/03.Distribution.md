# 分布式算法

## Paxos

[原文](https://mp.weixin.qq.com/s?__biz=MzI4NDMyNTU2Mw==&mid=2247483695&idx=1&sn=91ea422913fc62579e020e941d1d059e#rd)

### 什么是paxos

一致性协议

Paxos是一个一致性协议。什么叫一致性？一致性有很多种，也从强到弱分了很多等级，如线性一致性，因果一致性，最终一致性等等。什么是一致？这里举个例子，三台机器，每台机器的磁盘存储为128个字节，如果三台机器这128个字节数据都完全相同，那么可以说这三台机器是磁盘数据是一致的，更为抽象的说，就是多个副本确定同一个值，大家记录下来同一个值，那么就达到了一致性。

Paxos能达到什么样的一致性级别？这是一个较为复杂的问题。一致性往往不取决与客观存在的事实，如3台机器虽然拥有相同的数据，但是数据的写入是一个过程，有时间的先后，而更多的一致性取决于观察者，观察者看到的并未是最终的数据。

为何说是一个协议而不是一个算法，可以这么理解，算法是设计出来服务于这个协议的，如同法律是协议，那么算法就是各种机构的执行者，使得法律的约束能得到保证。Paxos的协议其实很简单，就三条规定，我认为这三条规定也是paxos最精髓的内容，各个执行者奋力的去保护这个协议，使得这个协议的约束生效，自然就得到了一致性。

分布式环境

为何要设计出这么一套协议，其他协议不行么。如最容易想到的，一个值A，往3台机器都写一次，这样一套简单的协议，能不能达到一致性的效果？这里就涉及到另外一个概念，Paxos一致性协议是在特定的环境下才需要的，这个特定的环境称为异步通信环境。而恰恰，几乎所有的分布式环境都是异步通信环境，在计算机领域面对的问题，非常需要Paxos来解决。

异步通信环境指的是消息在网络传输过程中，可能发生丢失、延迟、乱序现象。在这种环境下，上面提到的三写协议就变得很鸡肋了。消息乱序是一个非常恶劣的问题，这个问题导致大部分协议在分布式环境下都无法保证一致性，而导致这个问题的根本原因是网络包无法控制超时，一个网络包可以在网络的各种设备交换机等停留数天，甚至数周之久，而在这段时间内任意发出的一个包，都会跟之前发出的包产生乱序现象。无法控制超时的原因更多是因为时钟的关系，各种设备以及交换机时钟都有可能错乱，无法判断一个包的真正到达时间。

异步通信环境并非只有paxos能解决一致性问题，经典的两阶段提交也能达到同样的效果，但是分布式环境里面，除了消息网络传输的恶劣环境，还有另外一个让人痛心疾首的，就是机器的宕机，甚至永久失联。在这种情况下，两阶段提交将无法完成一个一致性的写入，而paxos，只要多数派机器存活就能完成写入，并保证一致性。

至此，总结一下paxos就是一个在异步通信环境，并容忍在只有多数派机器存活的情况下，仍然能完成一个一致性写入的协议。

提议者

前面讲了这么多都是协议协议，在分布式环境当中，协议作用就是每台机器都要扮演一个角色，这个角色严格遵守这个协议去处理消息。在paxos论文里面这个角色称之为Acceptor，这个很好理解。大家其实更关心另外一个问题，到底谁去发起写入请求，论文里面介绍发起写入请求的角色为提议者，称之为Proposer，Proposer也是严格遵守paxos协议，通过与各个Acceptor的协同工作，去完成一个值的写入。在paxos里面，Proposer和Acceptor是最重要的两个角色。

### Paxos是用来干什么的

确定一个值

既然说到写入数据，到底怎么去写？写一次还是写多次，还是其他？这也是我一开始苦恼的问题，相信很多人都会很苦恼。

这里先要明确一个问题，paxos到底在为谁服务？更确定来说是到底在为什么数据服务？还是引上面的例子，paxos就是为这128个字节的数据服务，paxos并不关心外面有多少个提议者，写入了多少数据，写入的数据是不是一样的，paxos只会跟你说，我确定了一个值，当这个值被确定之后，也就是这128个字节被确定了之后，无论外面写入什么，这个值都不会改变再改变了，而且三台机确定的值肯定是一样的。

说到这估计肯定会有人蒙逼了，说实话我当时也蒙逼了，我要实现一个存储服务啊，我要写入各种各样的数据啊，你给我确定这么一个值，能有啥用？但先抛开这些疑问，大家先要明确这么一个概念，paxos就是用来确定一个值用的，而且大家这里就先知道这么个事情就可以了，具体paxos协议是怎样的，怎么通过协议里面三条规定来获得这样的效果的，怎么证明的等等理论上的东西，都推荐去大家去看看论文，但是先看完本文再看，会得到另外的效果。

如下图，有三台机器（后面为了简化问题，不做特别说明都是以三台机器作为讲解例子），每台机器上运行这Acceptor来遵守paxos协议，每台机器的Acceptor为自己的一份Data数据服务，可以有任意多个Proposer。当paxos协议宣称一个值被确定（Chosen）后，那么Data数据就会被确定，并且永远不会被改变。

![Poxas](../Resource/Paxos-1.webp)

Proposer只需要与多数派的Acceptor交互，即可完成一个值的确定，但一旦这个值被确定下来后，无论Proposer再发起任何值的写入，Data数据都不会再被修改。Chosen value即是被确定的值，永远不会被修改。

确定多个值

对我们来说，确定一个值，并且当一个值确定后是永远不能被修改的，很明显这个应用价值是很低的。虽然我都甚至还不知道确定一个值能用来干嘛，但如果我们能有办法能确定很多个值，那肯定会比一个值有用得多。我们先来看下怎么去确定多个值。

上文提到一个三个Acceptor和Proposer各自遵守paxos协议，协同工作最终完成一个值的确定。这里先定义一个概念，Proposer，各个Acceptor，所服务的Data共同构成了一个大的集合，这个集合所运行的paxos算法最终目标是确定一个值，我们这里称这个集合为一个paxos instance，即一个paxos实例。

一个实例可以确定一个值，那么多个实例自然可以确定多个值，很简单的模型就可以构建出来，只要我们同时运行着多个实例，那么我们就能完成确定多个值的目标。

这里强调一点，每个实例必须是完全独立，互不干涉的。意思就是说Acceptor不能去修改其他实例的Data数据，Proposer同样也不能跨越实例去与其他实例的Acceptor交互。

如下图，三台机器每台机器运行两个实例，每个实例独立运作，最终会产生两个确定的值。这里两个实际可以扩展成任意多个。

![Poxas](../Resource/Paxos-2.webp)

至此，实例(Instance)以成为了我现在介绍paxos的一个基本单元，一个实例确定一个值，多个实例确定多个值，但各个实例独立，互不干涉。

然而比较遗憾的一点，确定多个值，仍然对我们没有太大的帮助，因为里面最可恨的一点是，当一个值被确定后，就永远无法被修改了，这是我们不能接受的。大部分的存储服务可能都需要有一个修改的功能。

有序的确定多个值

我们需要转换一下切入点，也许我们需要paxos确定的值，并不一定是我们真正看到的数据。我们观察大部分存储系统，如LevelDB，都是以AppendLog的形式，确定一个操作系列，而后需要恢复存储的时候都可以通过这个操作系列来恢复，而这个操作系列，正是确定之后就永远不会被修改的。到这已经很豁然开朗了，只要我们通过paxos完成一个多机一致的有序的操作系列，那么通过这个操作系列的演进，可实现的东西就很有想象空间了，存储服务必然不是问题。

如何利用paxos有序的确定多个值？上文我们知道可以通过运行多个实例来完成确定多个值，但为了达到顺序的效果，需要加强一下约束。

首先给实例一个编号，定义为i，i从0开始，只增不减，由本机器生成，不依赖网络。其次，我们保证一台机器任一时刻只能有一个实例在工作，这时候Proposer往该机器的写请求都会被当前工作的实例受理。最后，当编号为i的实例获知已经确定好一个值之后，这个实例将会被销毁，进而产生一个编号为i+1的实例。

基于这三个约束，每台机器的多个实例都是一个连续递增编号的有序系列，而基于paxos的保证，同一个编号的实例，确定的值都是一致的，那么三台机都获得了一个有序的多个值。

下面结合一个图示来详细说明一下这个运作过程以及存在什么异常情况以及异常情况下的处理方式。

![Poxas](../Resource/Paxos-3.webp)

图中A,B,C代表三个机器，红色代表已经被销毁的实例，根据上文约束，最大的实例就是当前正在工作的实例。A机器当前工作的实例编号是6，B机是5，而C机是3。为何会出现这种工作实例不一样的情况？首先解释一下C机的情况，由于paxos只要求多数派存活即可完成一个值的确定，所以假设C出现当机或者消息丢失延迟等，都会使得自己不知道3-5编号的实例已经被确定好值了。而B机比A机落后一个实例，是因为B机刚刚参与完成实例5的值的确定，但是他并不知道这个值被确定了。上面的情况与其说是异常情况，也可以说是正常的情况，因为在分布式环境，发生这种事情是很正常的。

下面分析一下基于图示状态的对于C机的写入是如何工作的。C机实例3处理一个新的写入，根据paxos协议的保证，由于实例3已经确定好一个值了，所以无论写入什么值，都不会改变原来的值，所以这时候C机实例3发起一轮paxos算法的时候就可以获知实例3真正确定的值，从而跳到实例4。但在工程实现上这个事情可以更为简化，上文提到，各个实例是独立，互不干涉的，也就是A机的实例6，B机的实例5都不会去理会C机实例3发出的消息，那么C机实例3这个写入是无法得到多数派响应的，自然无法写入成功。

再分析一下A机的写入，同样实例6无法获得多数派的响应，同样无法写入成功。同样假如B机实例5有写入，也是写入失败的结果，那如何使得能继续写入，实例编号能继续增长呢？这里引出下一个章节。

实例的对齐(Learn)

上文说到每个实例里面都有一个Acceptor的角色，这里再增加一个角色称之为Learner，顾名思义就是找别人学习，她回去询问别的机器的相同编号的实例，如果这个实例已经被销毁了，那说明值已经确定好了，直接把这个值拉回来写到当前实例里面，然后编号增长跳到下一个实例再继续询问，如此反复，直到当前实例编号增长到与其他机器一致。

由于约束里面保证仅当一个实例获知到一个确定的值之后，才能编号增长开始新的实例，那么换句话说，只要编号比当前工作实例小的实例（已销毁的），他的值都是已经确定好的。所以这些值并不需要再通过paxos来确定了，而是直接由Learner直接学习得到即可。

![Poxas](../Resource/Paxos-4.webp)

### 如何应用paxos

状态机

一个有序的确定的值，也就是日志，可以通过定义日志的语义进行重放的操作，那么这个日志是怎么跟paxos结合起来的呢？我们利用paxos确定有序的多个值这个特点，再加上这里引入的一个状态机的概念，结合起来实现一个真正有工程意义的系统。

状态机这个名词大家都不陌生，一个状态机必然涉及到一个状态转移，而paxos的每个实例，就是状态转移的输入，由于每台机器的实例编号都是连续有序增长的，而每个实例确定的值是一样的，那么可以保证的是，各台机器的状态机输入是完全一致的。根据状态机的理论，只要初始状态一致，输入一致，那么引出的最终状态也是一致的。而这个状态，是有无限的想象空间，你可以用来实现非常多的东西。

如下图这个例子是一个状态机结合paxos实现了一个具有多机一致的KV系统。

![Poxas](../Resource/Paxos-5.webp)

实例0-3的值都已经被确定，通过这4个值最终引出(b, ‘jeremy’)这个状态，而各台机器实例系列都是一致的，所以大家的状态都一样，虽然引出状态的时间有先后，但确定的实例系列确定的值引出确定的状态。

下图例子告诉大家Proposer，Acceptor，Learner，State machine是如何协同工作的。

![Poxas](../Resource/Paxos-6.webp)

一个请求发给Proposer，Proposer与相同实例编号为x的Acceptor协同工作，共同完成一值的确定，之后将这个值作为状态机的输入，产生状态转移，最终返回状态转移结果给发起请求者。

### 工程化

我们需要多个角色尽量在一起

上文提到一个实例，需要有Proposer和Acceptor两个角色协同工作，另外还要加以Learner进行辅助，到了应用方面又加入了State machine，这里面势必会有很多状态需要共享。如一个Proposer必须于Acceptor处于相同的实例才能工作，那么Proposer也就必须知道当前工作的实例是什么，又如State machine必须知道实例的chosen value是啥，而chosen value是存储于Acceptor管理的Data数据中的。在概念上，这些角色可以通过任意的通信方式进行状态共享，但真正去实现，我们都会尽量基于简单，高性能出发，一般我们都会将这些角色同时融合在一个机器，一个进程里面。

下图例子是一个工程上比较常规的实现方式。

![Poxas](../Resource/Paxos-7.webp)

这里提出一个新的概念，这里三台机器，每台机器运行着相同的实例i，实例里整合了Acceptor，Proposer，Learner，State machine四个角色，三台机器的相同编号实例共同构成了一个paxos group的概念，一个请求只需要灌进paxos group里面就可以了，跟根据paxos的特点，paxos group可以将这个请求可以随意写往任意一个Proposer，由Proposer来进行提交。Paxos group是一个虚设的概念，只是为了方便解释，事实上是请求随意丢到三台机任意一个Proposer就可以了。

那么具体这四个角色是如何工作的呢。首先，由于Acceptor和Proposer在同一个进程里面，那么保证他们处于同一个实例是很简单的事情，其次，当一个值被确认之后，也可以很方便的传送给State machine去进行状态的转移，最后当出现异常状态，实例落后或者收不到其他机器的回应，剩下的事情就交给Learner去解决，就这样一整合，一下事情就变得简单了。

需要严格的落盘

Paxos协议的运作工程需要做出很多保证(Promise)，这个意思是我保证了在相同的条件下我一定会做出相同的处理，如何能完成这些保证？众所周知，在计算机里面，一个线程，进程，甚至机器都可能随时挂掉，而当他再次启动的时候，磁盘是他恢复记忆的方法，在paxos协议运作里面也一样，磁盘是记录下这些保证条目的介质。

而一般的磁盘写入是有缓冲区的，当机器当机，这些缓冲区仍然未刷到磁盘，那么就会丢失部分数据，导致保证失效，所以在paxos做出这些保证的时候，落盘一定要非常严格，严格的意思是当操作系统告诉我写盘成功，那么无论任何情况都不会丢失。这个我们一般使用fsync来解决问题，也就是每次进行写盘都要附加一个fsync进行保证。

Fsync是一个非常重的操作，也因为这个，paxos最大的瓶颈也是在写盘上，在工程上，我们需要尽量通过各种手段，去减少paxos算法所需要的写盘次数。

万一磁盘fsync之后，仍然丢失或者数据错乱怎么办？这个称之为拜占庭问题，工程上需要一系列的措施检测出这些拜占庭错误，然后选择性的进行数据回滚或者直接丢弃。

需要一个Leader

由于看这篇文章的读者未必知道paxos理论上是如何去确定一个值的，这里简单说明一下，paxos一个实例，支持任意多个Proposer同时进行写入，但是最终确定出来一个相同的值，里面是运用了一些类似锁的方法来解决冲突的，而越多的Proposer进行同时写入，冲突的剧烈程度会更高，虽然完全不妨碍最终会确定一个值，但是性能上是比较差的。所以这里需要引入一个Leader的概念。

Leader就是领导者的意思，顾名思义我们希望有一个Proposer的领导者，优先由他来进行写入，那么当在只有一个Proposer在进行写入的情况下，冲突的概率是极小的，这样性能会得到一个飞跃。这里再次重申一下，Leader的引入，不是为了解决一致性问题，而是为了解决性能问题。

由于Leader解决的是性能问题而非一致性问题，即使Leader出错也不会妨碍正确性，所以我们只需要保证大部分情况下只有一个Proposer在工作就行了，而不用去保证绝对的不允许出现两个Proposer或以上同时工作，那么这个通过一些简单的心跳以及租约就可以做到，实现也是非常简单，这里就不展开解释。

我们需要状态机记录下来输入过的最大实例编号

状态机可以是任何东西，可以是kv，可以是mysql的binlog，在paxos实例运行时，我们可以保证时刻与状态机同步，这里同步的意思是指状态机输入到的实例的最大编号和paxos运行当中认为已经确认好值的实例最大编号是一样的，因为当一个实例已经完成值的确认之后，我们必须确保已经输入到状态机并且进行了状态转移，之后我们才能开启新的实例。但当机器重启或者进程重启之后，状态机的数据可能会由于自身实现问题，或者磁盘数据丢失而导致回滚，这个我们没办法像上文提到的fsync一样进行这么强的约束，所以提出了一种方法，状态机必须严格的记得自己输入过的最大实例编号。

这个记录有什么用？在每次启动的时候，状态机告诉paxos最大的实例编号x，而paxos发现自己最大的已确定值的实例编号是y，而x < y. 那这时候怎么办，只要我们有(x, y]的chosen value，我们重新把这些value一个一个输入到状态机，那么状态机的状态就会更新到y了，这个我们称之为启动重放。

这样对状态机的要求将尽量简单，只需要严格的记录好这么一个编号就可以了。当然不记录，每次从0开始也可以，但这样paxos需要从0开始重放，是一个蠢方法。

异步消息处理模型

上文说到分布式环境是一个异步通信环境，而paxos解决了基于这种环境下的一致性问题，那么一个显而易见的特点就是我们不知道也不确定消息何时到达，是否有序到达，是否到达，我们只需要去遵守paxos协议，严格的处理每一条到达的消息即可，这跟RPC模型比较不一样，paxos的特点是有去无回。

这里先定义一个名词叫paxos消息，这里指的是paxos为了去确定一个值，算法运行过程中需要的通信产生的消息。下图通过一个异步消息处理模型去构建一个响应paxos消息系统，从而完成paxos系统的搭建。

![Poxas](../Resource/Paxos-8.webp)

这里分为四个部分：

>- Request，即外部请求，这个请求直接输入到Proposer里面，由Proposer尝试完成一个值的确定。
>- Network i/o，网络i/o处理，负责paxos内部产生的消息的发送与接收，并且只处理paxos消息，采用私有端口，纯异步，各台机器之前的network i/o模块互相通信。
>- Acceptor，Proposer，Learner。用于响应并处理paxos消息。
>- State machine，状态机，实例确定的值(chosen value)的应用者。

工作流程：

>- 收到Request，由Proposer处理，如需要发送paxos消息，则通过network i/o发送。
>- Net work i/o收到paxos消息，根据消息类型选择Acceptor，Proposer，或Leaner处理，如处理后需要发送paxos消息，则通过network i/o发送。
>- Proposer通过paxos消息获知chosen value，则输入value到State machine完成状态转移，最终通知Request转移结果，完成一个请求的处理。
>- 当paxos完成一个值的确认之后，所有当前实例相关角色状态进行清空并初始化进行下一个编号的实例。

### 生产级的paxos库

RTT与写盘次数的优化

虽然经过我们在工程化上做的诸多要求，我们可以实现出一个基于paxos搭建的，可挂载任意状态机，并且能稳定运行的系统，但性能远远不够。在性能方面需要进行优化，方能上岗。由于上文并未对paxos理论做介绍，这里大概说明一下朴素的paxos算法，确定一个值，在无冲突的情况下，需要两个RTT，以及每台机器的三次写盘。这个性能想象一下在我们在线服务是非常惨烈的。为了达到生产级，最终我们将这个优化成了一个RTT以及每台机器的一次写盘。(2,3)优化到(1,1)，使得我们能真正在线上站稳脚跟。但由于本文的重点仍然不在理论，这里具体优化手段就暂不多做解释。

同时运行多个paxos group

由于我们实例运行的方式是确保i实例的销毁才能运行i+1实例，那么这个请求的执行明显是一个串行的过程，这样对cpu的利用是比较低的，我们得想办法将cpu利用率提升上来。

一个paxos group可以完成一个状态机的输入，但如果我们一台机器同时有多个状态机呢？比如我们可以同时利用paxos实现两种业务，每个业务对应一个状态机，互不关联。那么一个paxos group分配一个端口，我们即可在一台机器上运行多个paxos group，各自端口不同，互相独立。那么cpu利用率将能大幅提升。

比如我们想实现一个分布式的kv，那么对于一台机器服务的key段，我们可以再在里面分割成多个key段，那每个小key段就是一个独立的状态机，每个状态机搭配一个独立paxos group即可完成同时运行。

但一台机器搞几十个，几百个端口也是比较龌龊的手法，所以我们在生产级的paxos库上，实现了基于一个network i/o搭配多组paxos group的结构。

![Poxas](../Resource/Paxos-9.webp)

如上图，每个group里面都有完整的paxos逻辑，只需要给paxos消息增加一个group的标识，通过network i/o的处理，将不同group的消息输送到对应的group里面处理。这样我们一台机器只需要一个私有端口，即可完成多个状态机的并行处理。

至此我们可以获得一个多个paxos group的系统，完整结构如下：

![Poxas](../Resource/Paxos-10.webp)

更快的对齐数据

上文说到当各台机器的当前运行实例编号不一致的时候，就需要Learner介入工作来对齐数据了。Learner通过其他机器拉取到当前实例的chosen value，从而跳转到下一编号的实例，如此反复最终将自己的实例编号更新到与其他机器一致。那么这里学习一个实例的网络延时代价是一个RTT。可能这个延迟看起来还不错，但是当新的数据仍然通过一个RTT的代价不断写入的时候，而落后的机器仍然以一个RTT来进行学习，这样会出现很难追上的情况。

这里需要改进，我们可以提前获取差距，批量打包进行学习，比如A机器Learner记录当前实例编号是x，B机器是y，而x < y，那么B机器通过通信获取这个差距，将(x,y]的chosen value一起打包发送给A机器，A机器进行批量的学习。这是一个很不错的方法。

但仍然不够快，当落后的数据极大，B机器发送数据需要的网络耗时也将变大，那么发送数据的过程中，A机器处于一种空闲状态，由于paxos另外一个瓶颈在于写盘，如果不能利用这段时间来进行写盘，那性能仍然堪忧。我们参考流式传输，采用类似的方法实现Learner的边发边学，B机器源源不断的往A机器输送数据，而A机器只需要收到一个实例最小单元的包体，即可立即解开进行学习并完成写盘。

具体的实现大概是先进行一对一的协商，建立一个Session通道，在Session通道里直接采用直塞的方式无脑发送数据。当然也不是完全的无脑，Session通过心跳机制进行维护，一旦Session断开即停止发送。

如何删除Paxos数据

Paxos数据，即通过paxos确认下来的有序的多个值，后面我们称之这个为paxos log，这些log作为状态机的输入，是一个源源不断的。状态机的状态是有限的，但输入是无限的，但磁盘的空间又是有限的，所以输入必然不能长期保留，我们必须找到方法来把它删除掉。

上文说到我们要求状态机记录下来输入过的最大实例编号，这里定义为Imax，那么每次启动的时候是从这个编号后开始重放paxos log，也就是说小于等于这个编号Imax数据是没用的了，它不会再次使用，可以直接删除掉。但这个想法不够周全，因为paxos是允许少于多数派的机器挂掉的，这个挂掉可能是机器永远离线。而这种情况我们一般是用一台新的机器代替。这台新的机器要干什么？他要从0开始重放paxos log，而这些paxos log从哪里来？肯定是Learner找别的机器拷贝过来的。那别的机器删了怎么办？凉拌。

但也并不是没办法了，我可以把这台机状态机相关的数据全部拷贝到新机，然后就可以从Imax来启动了，那么自然就不需要[0,Imax]的paxos log了。但是状态机的数据是无时无刻不在写入的，一个正在写入的数据去拷贝出来，出现什么情况都是不可预期的，所以这个方法并不能简单的实现，什么？停机拷数据？别逗了。但这个思路给了我们一个启示。

我们需要的是一个状态机的镜像数据，这个数据在我们需要去拷贝的时候是可以随时停止写入的，那么只要我们有了这个镜像数据，我们就可以删除paxos log了。

Checkpoint

这个状态机的镜像数据我们就称之为Checkpoint。如何去生成Checkpoint，一个状态机能在不停写的情况下生成一个镜像数据么？答案是不确定的，看你要实现的状态机是什么，有的或许可以并很容易，有的可以但很难，有得可能根本无法实现。那这个问题又抛回给paxos库了，我要想办法去给他生成一个镜像数据，并且由我控制。

一个状态机能构建出一份状态数据，那么搞一个镜像状态机就可以同样构建出一份镜像状态数据了。

如上图，用两个状态转移完全一致的状态机，分别管理不同的状态数据，通过灌入相同的paxos log，最终出来的状态数据是完全一致的。

在真正生产级的paxos库里面，这个特性太为重要了。我们实际实现通过一个异步线程来构建这个镜像数据，而当发现其他机器需要获取这份数据的时候，可以很轻易的停止线程的工作，使得这份数据不再写入。最后发送给别的机器使用。

在目前的实现版本，我们真正做到了删paxos log，新机启动获取checkpoint，数据对齐的完全自动化。也就是说，首先程序会根据磁盘使用情况自动删除paxos log，其次，程序自动的通过镜像状态机生成checkpoint，最后，当一个新机器启动的时候，可以自动的获取到checkpoint，然后通过Learner自动的对齐剩下的数据，从而自动的完成无人工介入的机器更换。

### 所以什么时Paxos呢

![holly?](../Resource/Paxos-flow.png)

工程实践上根据具体的业务场景，或保证强一致(safety)，或在节点宕机、网络分化的时候保证可用(liveness)。2PC、3PC是相对简单的解决一致性问题的协议。

2PC(tow phase commit)两阶段提交，顾名思义它分成两个阶段，先由一方进行提议(propose)并收集其他节点的反馈(vote)，再根据反馈决定提交(commit)或中止(abort)事务。我们将提议的节点称为协调者(coordinator)，其他参与决议节点称为参与者(participants, 或cohorts)。

在阶段1中，coordinator发起一个提议，分别问询各participant是否接受。在阶段2中，coordinator根据participant的反馈，提交或中止事务，如果participant全部同意则提交，只要有一个participant不同意就中止。

在异步环境(asynchronous)并且没有节点宕机(fail-stop)的模型下，2PC可以满足全认同、值合法、可结束，是解决一致性问题的一种协议。但如果再加上节点宕机(fail-recover)的考虑，2PC是否还能解决一致性问题呢？coordinator如果在发起提议后宕机，那么participant将进入阻塞(block)状态、一直等待coordinator回应以完成该次决议。这时需要另一角色把系统从不可结束的状态中带出来，我们把新增的这一角色叫协调者备份(coordinator watchdog)。coordinator宕机一定时间后，watchdog接替原coordinator工作，通过问询(query) 各participant的状态，决定阶段2是提交还是中止。这也要求 coordinator/participant 记录(logging)历史状态，以备coordinator宕机后watchdog对participant查询、coordinator宕机恢复后重新找回状态。

从coordinator接收到一次事务请求、发起提议到事务完成，经过2PC协议后增加了2次RTT(propose+commit)，带来的时延(latency)增加相对较少。

3PC(three phase commit)即三阶段提交，既然2PC可以在异步网络+节点宕机恢复的模型下实现一致性，那还需要3PC做什么，3PC是什么鬼？

在2PC中一个participant的状态只有它自己和coordinator知晓，假如coordinator提议后自身宕机，在watchdog启用前一个participant又宕机，其他participant就会进入既不能回滚、又不能强制commit的阻塞状态，直到participant宕机恢复。这引出两个疑问：

>- 能不能去掉阻塞，使系统可以在commit/abort前回滚(rollback)到决议发起前的初始状态。
>- 当次决议中，participant间能不能相互知道对方的状态，又或者participant间根本不依赖对方的状态。

相比2PC，3PC增加了一个准备提交(prepare to commit)阶段来解决以上问题。coordinator接收完participant的反馈(vote)之后，进入阶段2，给各个participant发送准备提交(prepare to commit)指令。participant接到准备提交指令后可以锁资源，但要求相关操作必须可回滚。coordinator接收完确认(ACK)后进入阶段3、进行commit/abort，3PC的阶段3与2PC的阶段2无异。协调者备份(coordinator watchdog)、状态记录(logging)同样应用在3PC。

participant如果在不同阶段宕机，我们来看看3PC如何应对：

>- 阶段1: coordinator或watchdog未收到宕机participant的vote，直接中止事务；宕机的participant恢复后，读取logging发现未发出赞成vote，自行中止该次事务；
>- 阶段2: coordinator未收到宕机participant的precommit ACK，但因为之前已经收到了宕机participant的赞成反馈(不然也不会进入到阶段2)，coordinator进行commit；watchdog可以通过问询其他participant获得这些信息，过程同理；宕机的participant恢复后发现收到precommit或已经发出赞成vote，则自行commit该次事务;
>- 阶段3: 即便coordinator或watchdog未收到宕机participant的commit ACK，也结束该次事务；宕机的participant恢复后发现收到commit或者precommit，也将自行commit该次事务。

因为有了准备提交(prepare to commit)阶段，3PC的事务处理延时也增加了1个RTT，变为3个RTT(propose+precommit+commit)，但是它防止participant宕机后整个系统进入阻塞态，增强了系统的可用性，对一些现实业务场景是非常值得的。

### 再看Paxos证明过程

场景描述: prposer发出一些议案，可能被acceptor Accept，只要被足够多的acceptor Accept则意味着该议案被chosen，同时该议案的alue被chosen。

达成目标：只能有一个value被chosen。

足够多：或者说大多数Q，集合概念。Q是全体成员的子集，任意两个不同的Q的交集不为空。

acceptor的初始Accept

P1. An acceptor must accept the first proposal that it receives. 一个acceptor必须Accept它接收的第一个议案。P1就面临了一个抉择问题，一个acceptor还能不能accept其他议案？即acceptor是否允许accept多个议案？如果不能accept多个议案，则很可能无法形成过半，目前弃用。如果可以accept多个议案，又要保证我们的目标：只能有一个value被chosen。这就引出了如下P2要求来做到我们的目标。

P2-对结果要求

P2：If a proposal with value v is chosen, then every higher-numbered proposal that is chosen has value v. 如果一个value为v的议案被chosen了，则更高的议案必须含有value v才能被chosen。

有了这个约束，我们就能保证在多个议案被chosen的情况下，只有一个value被chosen。P2更像是对chosen一个议案的要求，如果要想实现它，必须把它落实在acceptor的Accept上，那就引出了下面的P2a的要求。

P2a-对acceptor的accept要求

P2a： If a proposal with value v is chosen, then every higher-numbered proposal accepted by any acceptor has value v. 如果一个value为v的议案被chosen，那么每一个acceptor Accept的更高议案是必须含有value v的。acceptor Accept的高议案都是含有value v的，则这些高议案被chosen的时候自然满足P2。（如果更高的议案都是含value v的，那么acceptor 自然可以在不知情的情况下任意Accept其接收的第一个议案）

一个议案可能在一个acceptor没有收到任何议案之前就被大多数Accept，value v被选中。此时一个proposer提出的更高的议案，value不是v，送达这个acceptor之后被Accept，不满足P2a的要求。

P2b-对proposer提出议案的要求（结果上要求）

P2b：If a proposal with value v is chosen, then every higher-numbered proposal issued by any proposer has value v.如果一个value为v的议案被chosen，那么如果一个proposer要提出一个更高的议案，则该议案必须含有value v。

这样的话就杜绝了上述情况，在value v被chosen的情况下，proposer要想提出一个议案，必须采用之前已提交的value v，而不是使用其他的value。同时又保证了P2a。

P2b对proposer提出的议案做出了要求，但是这个议案怎么来（如怎么得到已经被chosen的value v）并没有说明，下面就引出了P2c，来详细描述proposer通过怎样的过程提出的议案满足上面的要求。

P2c-对proposer提出议案的要求（做法上要求）

P2c：For any v and n, if a proposal with value v and number n is issued, then there is a set S consisting of a majority of acceptors such that either

(a) no acceptor in S has accepted any proposal numbered less than n, or (b) v is the value of the highest-numbered proposal among all proposals numbered less than n accepted by the acceptors in S.

要想提出一个议案号为n，value为v的议案，必须满足下面2个条件中的一个

S是一个过半的acceptor集合，满足如下2个条件中的一个。

>- a：S中的acceptor都没有accept任何小于n的议案（n=1，相当于初始时即acceptor还没开始accept议案，此时可以随意提议案）
>- b：S中的acceptor有Accept的小于n的议案，则v是上述议案中编号最大的议案的value（n>1，proposer应当收集acceptor Accept的议案，来保证任意的accpeter没有Accept编号大于n的议案，否则这个议案编号为n，value 为v的议案无效）

比较难以理解的就是P2c给出的提出议案的方案如何能保证P2b。再来看看P2b要证明的是：在一个value为v的议案被chosen的情况下，保证新提出的议案必须含有value v，结合P2c对提出议案的要求，a条件被排除了（因为在P2b的条件下已经有议案被accept了），那就是说目前要在b条件下证明P2b。

即目前的证明题是：

如果一个议案号为m，value为v的议案被chosen了，在满足b的条件下，提出一个议案（n，v1），证明v1=v

这个证明可以用归纳法来证明。

第一步：证明初值成立，如果n=m+1，则至少过半集合C中的acceptor中小于n的最大accept议案是m，m的value是v，根据b条件，S中必然有一个acceptor属于C，即S中必然有一个acceptor accept的最大议案就是m，m已经是最大议案，即S集合中最大accept议案必然是m，所以此时新议案选用的value就是m的value v。第二步：假设m到n-1的议案的value都是v（m之后的议案是否被chosen处于未知状态，至少m议案是被过半accept的），此时过半的集合C中的acceptor accept的最大议案必然落在[m，n-1]中，他们的value全部是v，根据b条件，S中必然有一个或者一些acceptor是属于C集合的，S和C之间公共的acceptor集合为C1，则C1集合具有上述C集合的特点，S中的acceptor accept的最大议案必然落在C1中，由于C1中m之后的议案的value全是v，则此时提出的新议案的value必然采用value v。

总上2步，P2c给出的这个提出议案的要求，必然能够满足P2b。

接下来的问题就是：一个proposer如何知道acceptor accept的最大议案的value呢？这就需要proposer先提前去探测下这个最大议案的value，即这时候才引出运作过程中的prepare过程。前面一直在说的是运作过程的accept过程。

引出prepare过程和P1a

一个proposer向所有的acceptor发送请求，包含要发送的议案号n，来得知他们当前accept的最大议案的value。该请求就被称为prepare请求。

这些acceptor的议案可以分成2大部分

a.小于议案号n的议案，又可以分为：目前已经被accept的议案，从中可以挑选出最大accept的议案的value，作为该proposer要提出的议案的value；还未被accept的议案，这部分议案是还未到达acceptor，proposer要做的就是不再这些议案全部到达被accept了之后再去选择其中最大议案的value，而是直接让acceptor保证：抛弃这一部分的议案，即承诺此后不再accept小于n的议案了。从而简化对这部分议案的处理。

这一部分约束就是对acceptor的accept约束的补充，即

P1a.An acceptor can accept a proposal numbered n if it has not responded to a prepare request having a number greater than n

如果一个acceptor没有对大于n的议案的prepare请求进行响应的前提下，该acceptor才能accept议案n，否则就要拒绝议案n。

b.大于议案号n的议案，如果acceptor accept了大于n的议案，从中选举最大议案的value，作为该proposer要提出的议案的value。

优化prepare

对于上述情况，即proposer一旦发出了一个prepare请求，议案编号为n，如果此时acceptor已经accept了更大的议案，如n+1。

acceptor在收到n+1的议案的prepare的时候，已经做出了承诺，不再accept小于n+1的议案了。即使该proposer提出的编号为n的议案即使在prepare过程中得到了value，该议案在发给acceptor accept的时候，acceptor也会拒绝。因此，acceptor应该在prepare阶段就可以把它拒绝了，即直接拒绝proposer发送的议案号为n的prepare请求。

我们一直在说acceptor对于prepare有2个承诺一个应答，其实就是上述ab的分支：a分支是应答/承诺不再accept小于n的议案，b承诺不再响应小于n的prepare请求。

### Paxos过程

Phase 1

一个proposer选择一个编号为n的议案，向所有的acceptor发送prepare请求。如果acceptor已经响应的prepare请求中议案编号都比n小，则它承诺不再响应prepare请求或者accept请求中议案编号小于n的，并且找出已经accept的最大议案的value返回给该proposer。如果已响应的编号比n大，则直接忽略该prepare请求。

Phase 2

如果proposer收到了过半的acceptors响应，那么将提出一个议案（n，v）,v就是上述所有acceptor响应中最大accept议案的value，或者是proposer自己的value。然后将该议案发送给所有的acceptor。这个请求叫做accept请求，这一步才是所谓发送议案请求，而前面的prepare请求更多的是一个构建出最终议案(n,v)的过程。如果没有收到过半响应，则增大议案编号，重新回到Phase 1阶段。

acceptor接收到编号为n的议案，如果acceptor还没有对大于n的议案的prepare请求响应过，则acceptor就accept该议案，否则拒绝。

## Raft

Raft是一个一致性算法，旨在易于理解。它提供了Paxos的容错和性能。不同之处在于它被分解为相对独立的子问题，它清楚地解决了实际系统所需的所有主要部分。我们希望Raft能够为更广泛的受众提供共识，并且这个更广泛的受众将能够开发出比现在更多的高质量共识系统。

Raft是一个通过管理一个副本日志的一致性算法。它提供了跟(multi-)Paxos一样有效的功能，但是它的架构和Paxos不一样；它比Paxos更加容易理解，并且能用于生产环境中。为了加强理解，raft把一致性的问题分成了三个子问题，leader election, log replication, and safety。

Leader，Follower，candidate

在Raft集群中，有且仅有一个Leader，在Leader运行正常的情况下，一个节点服务器要么就是Leader，要么就是Follower。Follower直到Leader故障了，才有可能变成candidate。Leader负责把client的写请求log复制到follower。它会和follower保持心跳。每个follower都有一个timeout时间（一般为150ms~300ms），在接受到心跳的时候，这个timeout时间会被重置。如果follower没有接收到心跳，这些follower会把他们的状态变为candidate，并且开启新的一轮leader election。

term逻辑时钟

Term相当于paxos中的proposerID，相当于一个国家的朝代。term是一段任意的时间序号。每一任Leader都有一个与之前不同的term。当Leader选举成功之后，一个节点成为了Leader，就会产生一个新的term，并且直到Leader故障，整个集群都会一直在这个term下执行操作。如果leader选举失败了，则会再生成出一个term，再开启一轮leader选举。

Quorums

多数派，意思是超过一半的机器存活，则这个机器可用，这个Quorums指的就是集群可用的指标。例如：集群中的节点数为2N，如果有N+1的机器存活，则代表集群可用，可接受请求，写入log，应用到state machine中去，执行操作。如果少于N+1个机器存活，则代表集群可用，可接受请求，可写入log，但不应用到state machine中去，不执行操作。

Leader Election

只有在下列两种情况下才会进行leader election：在第一次启动raft集群的时候；在一个已存在的Leader故障的时候。选举流程：如果以上两种任何一种发生了，所有的Follower无法再和Leader保持心跳，则它们都会等待一个（选举）timeout，如果其中一个Follower的timeout最先到时，则这个Follower变成candidate开始选举。

candidate先增加term计数器，然后，给自己投票并向所有其他的节点服务器请求投自己一票。如果一个Follower在接受到投票请求时，接受到两个term相同的投票请求时（也就是说，产生了两个candidate），则在多个相同term的投票请求中，这个Follower只能给投给其中一个请求，只能投一票，并且按照先来先服务的原则投票。如果这个candidate收到另外一个节点服务器的消息，并且这个节点服务器的term序号和当前的term序号一样大，甚至更大的话，则这个candidate选举失败，从而它的状态变成Follower，并且接受新的Leader。如果一个candidate获得了Quorums选票N+1(2N为集群中节点的数目)，则它变成新的leader。

如果多个candidate和多个Follower投完票之后，有多个candidate获得了相同的票数，则会产生split vote，则新的term产生，重新选举。Raft用随机选举timeout迅速地解决split vote问题，这个方法就是对于产生spit vote的candidates各自随机生成一个选举timeout，谁先到时，谁当leader，其他candidate都变为Follower。当一个leader被选举出来之后，就在Follower timeout到时变为candidate之前，发心跳信息给所有Followers。

Log Replication

Leader负责把client的请求日志复制给其他Followers。Client发送请求给Leader，其中每个请求都是一条操作指令。Leader接受到client请求之后，把操作指令(Entry)追加到Leader的操作日志中。紧接着对Follower发起AppendEntries请求、尝试让操作指令(Entry)追加到Followers的操作日志中，即落地。如果有Follower不可用，则一直尝试。一旦Leader接受到多数（Quorums）Follower的回应，Leader就会进行commit操作，每一台节点服务器会把操作指令交给状态机处理。这样就保证了各节点的状态的一致性。各服务器状态机处理完成之后，Leader将结果返回给Client。

Saftety

Raft的安全性，体现在如下几个方面：

>- Election safety: 在一个term下，最多只有一个Leader。
>- Leader Append-Only: 一个Leader只能追加新的entries，不能重写和删除entries
>- Log Matching: 集群中各个节点的log都是相同一致的
>- Leader Completeness: 如果一个log entry被committed了，则这个entry一定会出现在Leader的log里。
>- State Machine Safety: 如果一个节点服务器的state machine执行了一个某个log entry命令，则其他节点服务器，也会执行这个log entry命令，不会再执行其他命令

之前四条，在前面都有所提及，而State Machine Safety是在Leader election过程中用到过。

State Machine Safety

一个candidate在选举的时候，它会向其他节点服务器发送包含他的log的消息获取票数，如果它的log是最新的，则会获取选票，如果它的log不是最新的，其他节点服务器还有更加新的log，则会拒绝给这个candidate投票。这就保证了State Machine Safety。所以State Machine Safety保证的就是一个candidate必须拥有最新的log，才能获取票数，才有机会赢得Leader选举，才有机会成为Leader。

Follower crashes

如果一个follower故障了，则不会再接受AppendEntriesandvoterequests，并且Leader会不断尝试与这个节点保持心跳。如果这个节点恢复了，则会接受Leader的最新的log，并且将log应用到state machine中去，执行log中的操作。

Leader crashes

则会进行Leader election。如果碰到Leader故障的情况，集群中所有节点的日志可能不一致。old leader的一些操作日志没有通过集群完全复制。new leader将通过强制Followers复制自己的log来处理不一致的情况，步骤如下：对于每个Follower，new leader将其日志与Followers的日志进行比较，找到他们的达成一致的最后一个log entry。然后删除掉Followers中这个关键entry后面的所有entry，并将其替换为自己的log entry。该机制将恢复日志的一致性。

Raft要求具备唯一Leader，并把一致性问题具体化为保持日志副本的一致性，以此实现相较Paxos而言更容易理解、更容易实现的目标。Raft是state machine system，Zab是primary-backup system。

## ZAB

Zab也是一个强一致性算法，也是(multi-)Paxos的一种，全称是Zookeeper atomic broadcast protocol，是Zookeeper内部用到的一致性协议。相比Paxos，也易于理解。其保证了消息的全局有序和因果有序，拥有强一致性。Zab和Raft也是非常相似的，只是其中有些概念名词不一样。

Role(or Status)

节点状态：

>- Leading：说明当前节点为Leader
>- Following：说明当前节点为Follower
>- Election：说明节点处于选举状态。整个集群都处于选举状态中。

Epoch逻辑时钟

Epoch相当于paxos中的proposerID，Raft中的term，相当于一个国家，朝代纪元。

Quorums

多数派，集群中超过半数的节点集合。

节点中的持久化信息

>- history: a log of transaction proposals accepted; 历史提议日志文件
>- acceptedEpoch:the epoch number of the last NEWEPOCH message accepted; 集群中的最近最新Epoch
>- currentEpoch:the epoch number of the last NEWLEADER message accepted; 集群中的最近最新Leader的Epoch
>- lastZxid:zxid of the last proposal in the history log; 历史提议日志文件的最后一个提议的zxid

在ZAB协议的事务编号Zxid设计中，Zxid是一个64位的数字，低32位是一个简单的单调递增的计数器，针对客户端每一个事务请求，计数器加1；高32位则代表Leader周期epoch的编号，每个当选产生一个新的Leader服务器，就会从这个Leader服务器上取出其本地日志中最大事务的ZXID，并从中读取epoch值，然后加1，以此作为新的epoch，并将低32位从0开始计数。epoch：可以理解为当前集群所处的年代或者周期，每个leader就像皇帝，都有自己的年号，所以每次改朝换代，leader变更之后，都会在前一个年代的基础上加1。这样就算旧的leader崩溃恢复之后，也没有人听他的了，因为follower只听从当前年代的leader的命令。

Zab协议四个阶段

Phase 0: Leader election（选举阶段，Leader不存在）

节点在一开始都处于选举阶段，只要有一个节点得到超半数Quorums节点的票数的支持，它就可以当选prospective leader。只有到达Phase 3 prospective leader才会成为established leader(EL)。这一阶段的目的是就是为了选出一个prospective leader（PL），然后进入下一个阶段。协议并没有规定详细的选举算法，后面我们会提到实现中使用的Fast Leader Election。

Phase 1: Discovery（发现阶段，Leader不存在）

在这个阶段，PL收集Follower发来的acceptedEpoch(或者)，并确定了PL的Epoch和Zxid最大，则会生成一个NEWEPOCH分发给Follower，Follower确认无误后返回ACK给PL。这个一阶段的主要目的是PL生成NEWEPOCH，同时更新Followers的acceptedEpoch，并寻找最新的historylog，赋值给PL的history。这个阶段的本质：发现最新的history log，发现最新的history log，发现最新的history log。

一个follower只会连接一个leader，如果有一个节点f认为另一个follower是leader，f在尝试连接p时会被拒绝，f被拒绝之后，就会进入Phase 0。

Phase 2: Synchronization（同步阶段，Leader不存在）

同步阶段主要是利用leader前一阶段获得的最新提议历史，同步集群中所有的副本。只有当quorum都同步完成，PL才会成为EL。follower只会接收zxid比自己的lastZxid大的提议。这个一阶段的主要目的是同步PL的historylog副本。

Phase 3: Broadcast（广播阶段，Leader存在）

到了这个阶段，Zookeeper 集群才能正式对外提供事务服务，并且 leader 可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步。这个一阶段的主要目的是接受请求，进行消息广播。值得注意的是，ZAB 提交事务并不像 2PC 一样需要全部 follower 都 ACK，只需要得到 quorum （超过半数的节点）的 ACK 就可以了。

zookeeper中zab协议的实现

协议的Java版本实现跟上面的定义有些不同，选举阶段使用的是Fast Leader Election（FLE），它包含了Phase 1的发现职责。因为FLE会选举拥有最新提议历史的节点作为leader，这样就省去了发现最新提议的步骤。实际的实现将Phase 1和Phase 2合并为 Recovery Phase（恢复阶段）。

所以，ZAB的实现只有三个阶段：

Phase 1:Fast Leader Election(快速选举阶段，Leader不存在)

前面提到 FLE 会选举拥有最新提议历史（lastZixd最大）的节点作为 leader，这样就省去了发现最新提议的步骤。这是基于拥有最新提议的节点也有最新提交记录的前提。每个节点会同时向自己和其他节点发出投票请求，互相投票。

选举流程：选epoch最大的；epoch相等，选zxid最大的；epoch和zxid都相等，选择serverID最大的（就是我们配置zoo.cfg中的myid）。节点在选举开始都默认投票给自己，当接收其他节点的选票时，会根据上面的条件更改自己的选票并重新发送选票给其他节点，当有一个节点的得票超过半数，该节点会设置自己的状态为 leading，其他节点会设置自己的状态为 following。

Phase 2:Recovery Phase（恢复阶段，Leader不存在）

这一阶段 follower 发送它们的 lastZixd 给 leader，leader 根据 lastZixd 决定如何同步数据。这里的实现跟前面 Phase 2 有所不同：Follower 收到 TRUNC 指令会中止 L.lastCommittedZxid 之后的提议，收到 DIFF 指令会接收新的提议。

Phase 3:Broadcast Election(广播阶段，Leader存在)

同上

Leader故障

如果是Leader故障，首先进行Phase 1: Fast Leader Election，然后Phase 2: Recovery Phase，恢复阶段保证了如下两个问题，这两个问题同时也和Raft中的Leader故障解决的问题是一样的，总之就是要保证Leader操作日志是最新的：已经被处理的消息不能丢；被丢弃的消息不能再次出现。

Zab和Raft都是强一致性协议，但是Zab和Raft的实质是一样的，都是mutli-paxos衍生出来的强一致性算法。简单而言，他们的算法都都是先通过Leader选举，选出一个Leader，然后Leader接受到客户端的提议时，都是先写入操作日志，然后再与其他Followers同步日志，Leader再commit提议，再与其他Followers同步提议。如果Leader故障，重新走一遍选举流程，选取最新的操作日志，然后同步日志，接着继续接受客户端请求等等。过程都是一样，只不过两个的实现方式不同，描述方式不同。实现Raft的核心是Term，Zab的核心是Zxid，反正Term和Zxid都是逻辑时钟。

## 服务发现

服务发现是大多数分布式系统以及面向服务架构（SOA）的一个核心组成部分。这个难题，简单来说，可以认为是：当一项服务存在于多个主机节点上时，client端如何决策获取相应正确的IP和port。在传统情况下，当出现服务存在于多个主机节点上时，都会使用静态配置的方法来实现服务信息的注册。但是当大型系统中，需要部署更多服务的时候，事情就显得复杂得多。在一个实时的系统中，由于自动或者人工的服务扩展，或者服务的新添加部署，还有主机的宕机或者被替换，服务的location信息可能会很频繁的变化。在这样的场景下，为了避免不必要的服务中断，动态的服务注册和发现就显得尤为重要。

### 问题陈述

在定位服务的时候，其实会有两个方面的问题：服务注册（Service Registration）和服务发现（Service Discovery）。

>- 服务注册: 一个服务将其位置信息在中心注册节点注册的过程。该服务一般会将它的主机IP地址以及端口号进行注册，有时也会有服务访问的认证信息，使用协议，版本号，以及关于环境的一些细节信息。
>- 服务发现: client端的应用实例查询中心注册节点以获知服务位置的过程。

每一个服务的服务注册以及服务发现，都需要考虑一些关于开发以及运营方面的问题：

>- 监控: 当一个已注册完毕的服务失效的时候如何处理。一些情况下，在一个设定的超时定时(timeout)后，该服务立即被一个其他的进程在中心注册节点处注销。这种情况下，服务通常需要执行一个心跳机制，来确保自身的存活状态；而客户端必然需要能够可靠处理失效的服务。
>- 负载均衡: 如果多个相同地位的服务都注册完毕，如何在这些服务之间均衡所有client的请求负载？如果有一个master节点的话，是否可以正确处理client访问的服务的位置。
>- 集成方式: 信息注册节点是否需要提供一些语言绑定的支持，比如说，只支持Java？集成的过程是否需要将注册过程以及发现过程的代码嵌入到你的应用程序中，或者使用一个类似于集成助手的进程？
>- 运行时依赖: 是否需要JVM，ruby或者其他在你的环境中并不兼容的运行时？
>- 可用性考虑: 如果系统失去一个节点的话，是否还能正常工作？系统是否可以实时更新或升级，而不造成任何系统的瘫痪？既然集群的信息注册节点是架构中的中心部分，那该模块是否会存在单点故障问题？

为达到通用的效果，Zookeeper/Doozer/ETCD使用了一致性的数据存储。尽管我们把它们看作服务的注册系统，其实它们还可以用于协调服务来协助leader选举，以及在一个分布式clients的集合中做centralized locking。

Zookeeper是一个集中式的服务，该服务可以维护服务配置信息，命名空间，提供分布式的同步，以及提供组化服务。Zookeeper是由Java语言实现，实现了强一致性（CP），并且是使用Zab协议在ensemble集群之间协调服务信息的变化。Zookeeper在ensemble集群中运行3个，5个或者7个成员。众多client端为了可以访问ensemble，需要使用绑定特定的语言。这种访问形式被显性的嵌入到了client的应用实例以及服务中。

服务注册的实现主要是通过命令空间（namespace）下的ephemeral nodes。ephemeral nodes只有在client建立连接后才存在。当client所在节点启动之后，该client端会使用一个后台进程获取client的位置信息，并完成自身的注册。如果该client失效或者失去连接的时候，该ephemeral node就从树中消息。

服务发现是通过列举以及查看具体服务的命名空间来完成的。Client端收到目前所有注册服务的信息，无论一个服务是否不可用或者系统新添加了一个同类的服务。Client端同时也需要自行处理所有的负载均衡工作，以及服务的失效工作。

ZooKeeper并不只是作为服务发现框架使用的，它非常庞大。如果只是打算将ZooKeeper作为服务发现工具，就需要用到其配置存储和分布式同步的功能。前者可以理解成具有一致性的KV存储，后者提供了ZooKeeper特有的watcher注册于异步通知机制，ZooKeeper能将节点的状态实时异步通知给ZooKeeper客户端。

由于Zookeeper是一个CP强一致性的系统，因此当网络分区（Partition）出故障的时候，部分系统可能将出现不能注册的情况，也可能出现不能找到已存在的注册信息，即使它们可能在Partition出现期间仍然正常工作。特殊的是，在任何一个non-quorum端，任何读写都会返回一个错误信息。

Doozer是一个一致的分布式数据存储系统，Go语言实现，通过Paxos算法来实现共识的强一致性系统。Doozer在集群中运行3，5或者7个节点。和Zookeeper类似，Client端为了访问集群，需要在自身的应用或者服务中使用特殊的语言绑定。

Doozer的服务注册就没有Zookeeper这么直接，因为Doozer没有那些ephemeral node的概念。一个服务可以在一条路径下注册自己，如果该服务不可用的话，它也不会自动地被移除。现有很多种方式来解决这样的问题。一个选择是给注册进程添加一个时间戳和心跳机制，随后在服务发现进程中处理那些超时的路径，也就是注册的服务信息，当然也可以通过另外一个清理进程来实现。

服务发现和Zookeeper很类似，Doozer可以罗列出指定路径下的所有入口，随后可以等待该路径下的任意改动。如果你在注册期间使用一个时间戳和心跳，你就可以在服务发现期间忽略或者删除任何过期的入口，也就是服务信息。和Zookeeper一样，Doozer是一个CP强一致性系统，当发生网络分区故障时，会导致同样的后果。

Etcd是一个高可用的K-V存储系统，主要应用于共享配置、服务发现等场景。Etcd可以说是被Zookeeper和Doozer催生而出。因其易用，简单。很多系统都采用或支持etcd作为服务发现的一部分，比如kubernetes。但正事因为其只是一个存储系统，如果想要提供完整的服务发现功能，必须搭配一些第三方的工具。整个系统使用Go语言实现，使用Raft算法来实现选举一致，同时又具有一个基于HTTP+JSON的API。Etcd，和Doozer和Zookeeper相似，通常在集群中运行3，5或者7个节点。client端可以使用一种特定的语言进行绑定，同时也可以通过使用HTTP客户端自行实现一种。

服务注册环节主要依赖于使用一个key TTL来确保key的可用性，该key TTL会和服务端的心跳捆绑在一起。如果一个服务在更新key的TTL时失败了，那么Etcd会对它进行超时处理。如果一个服务变为不可用状态，client会需要处理这样的连接失效，然后尝试另连接一个服务实例。

服务发现环节设计到罗列在一个目录下的所有key值，随后等待在该目录上的所有变动信息。由于API接口是基于HTTP的，所以client应用会的Etcd集群保持一个long-polling的连接。

Etcd使用Raft一致性协议，是一个强一致性系统。Raft需要一个leader被选举，然后所有的client请求会被该leader所处理。然而，Etcd似乎也支持从non-leaders中进行读取信息，使用的方式是在读情况下提高可用性的未公开的一致性参数。在网络分区故障期间，写操作还是会被leader处理，而且同样会出现失效的情况。

consul相较于etcd、zookeeper的最大特点就是：它整合了用户服务发现普遍的需求，开箱即用，降低了使用的门槛，并不需要任何第三方的工具。代码实现上也足够简单。consul的使用不依赖任何sdk，依靠简单的http请求就能满足服务发现的所有逻辑。不过，服务每次都从consul agent获取其他服务的存活状态，相比于zookeeper的watcher机制，实时性稍差一点，需考虑如何尽可能提高实时性，问题不会很大。
