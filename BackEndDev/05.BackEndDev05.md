# 后台开发读书笔记

## 第六章 TCP协议

TCP/IP分层模型：应用层，传输层，网间层，网络接口层。

应用层对应于OSI七层参考模型中的应用层、会话层和表示层。常见的应用层协议有Finger、Whois，FTP，Gopher，HTTP，Telent，SMTP，IRC，NNTP等。传输层对应于OSI七层参考模型的传输层，它提供两种端到端的通信服务。其中TCP协议提供可靠的数据流运输服务，UDP协议提供不可靠的用户数据报服务。网间层对应于OSI七层参考模型的网络层。本层包含IP协议，RIP协议，负责数据的包装、寻址和路由。同时还包含ICMP用来提供网络诊断信息。网络接口层包括用于协作IP数据在已有网路介质上传输的协议。

### TCP头部的格式

![TCPHead](../Resource/TCPHead.png)

>- 源端口 source port 16bit 用来告知主机该报文段是来自哪里。进行TCP通讯时，客户端通常使用系统自动选择的临时端口号，而服务器则使用知名服务端口号；
>- 目的端口 destination port 16bit 用来告诉远程主机报文段交付给谁；
>- 序号 sequence number TCP 是面向字节流的，在一个TCP连接中传输的字节流中的每个字节都按照顺序编号，用来解决乱序问题。4个字节可以表示的数值范围：[0, 2^32]，一共 2^32 (4294967296) 个序号。序号增加到最大值的时候，下一个序号又回到了0。也就是说TCP协议可对4GB的数据进行编号，在一般情况下可保证当序号重复使用时，旧序号的数据早已经通过网络到达终点或者丢失。主机A和主机B进行通信，A发送给B的第一个TCP报文段中，序号值会被初始化为某个随机的ISN（Initial Sequence Number）。在该传输方向上后续的TCP报文中的序号值将被系统设置成ISN+该报文段所携带的第一个字节在整个字节流中的偏移；
>- 确认号 acknowledgment number 用作对另一方发来的TCP报文段的响应，用来解决丢包问题。其值是收到的TCP报文段序号值+1；
>- 数据偏移 offset 占4比特，表示数据开始的地方离TCP段的起始处有多远。实际上就是TCP段首部的长度。由于首部长度不固定，因此数据偏移字段是必要的。数据偏移以32位为长度单位，也就是4个字节，因此TCP首部的最大长度是60个字节。即偏移最大为15个长度单位=15x32位 = 15x4字节；
>- 保留 reserved；
>- 标志位 tcp flags，包括URG（紧急指针是否有效，告诉系统此报文段中有紧急数据，应尽快传送，而不要按原来的排队顺序来传送。 URG要与首部中的 紧急指针 字段配合使用），ACK（确认号是否有。TCP 规定，在连接建立后所有传送的报文段都必须把ACK设置为 1。），PSH（提示接收端应该尽快推送给接收应用程序，而不用等到整个 TCP 缓存都填满了后再交付），RST（表示对方要求重新建立连接，复位报文段），SYN（表示建立一个连接，同步报文段），FIN（表示通知对方本段要关闭连接，结束报文段）；
>- 窗口大小 window size 用于流量控制，告诉对方本段的TCP接收缓冲区还能容纳多少字节，让对方控制发送数据的速率；
>- 检验和 checksum，接收端对TCP报文执行CRC校验，不仅包括头部，还包括数据部分；
>- 紧急指针 urgent pointer 仅在 URG = 1 时才有意义，它指出本报文段中的紧急数据的字节数。 当URG=1时，发送方 TCP 就把紧急数据插入到本报文段数据的最前面，而在紧急数据后面的数据仍是普通数据。 因此，紧急指针指出了紧急数据的末尾在报文段中的位置；
>- 选项 tcp option。

### TCP报文大小

TCP提供的是一种面向连接的，可靠的字节流服务，TCP提供可靠性的一种重要的方式就是MSS。通过MSS，应用数据被分割成TCP认为最适合发送的数据块，由TCP传递给IP的信息单位称为报文段或段(segment)。代表一个TCP socket的结构体struct tcp_sock中有多个成员用于确定应用数据被分割成最大为多大的数据块较为合适(最大报文段长度MSS)。与最大报文段长度最为相关的一个参数是网络设备接口的MTU，以太网的MTU是1500Byte，基本IP首部长度为20Byte，TCP首部最小是20Byte，所以MSS的值可达1460Byte(MSS不包括协议首部，只包含应用数据)。UDP首部是8Byte，所以UDP的负载最大为1472Byte。

在建立连接的时候，通信双方会相互确认对方的最大报文段。对于一个以太网一般可以到1460Byte，对于非本地的IP，MSS可能只有536Byte。而且中间传输网络的MSS更小的话，这个值会更小。

### 保护消息边界和流

保护消息边界，就是指传输协议把数据当作一条独立的消息在网上传输，接收端只能接收独立的消息。也就是说存在保护消息边界，接收端一次只能接收发送端发出的一个数据包。而面向流则是指无保护消息保护边界的，如果发送端连续发送数据，接收端有可能在一次接收动作中，会接收两个或者更多的数据包。

例如连续发送三个数据包，大小分别是2k，4k ，8k,这三个数据包，都已经到达了接收端的网络堆栈中，如果使用UDP协议，不管使用多大的接收缓冲区去接收数据，必须有三次接收动作，才能够把所有的数据包接收完；而使用TCP协议，只要把接收的缓冲区大小设置在14k以上，就能够一次把所有的数据包接收下来，只需要有一次接收动作。

这就是因为UDP协议的保护消息边界使得每一个消息都是独立的。而流传输却把数据当作一串数据流，它不认为数据是一个一个的消息。所以有很多人在使用tcp协议通讯的时候，并不清楚tcp是基于流的传输，当连续发送数据且使用的缓冲区足够大时，有可能会一次接收到两个甚至更多的数据包，而很多人往往会忽视这一点，只解析检查了第一个数据包，而已经接收的其他数据包却被忽略了。

### TCP 与 UDP 的比较

TCP为了保证可靠传输，尽量减少额外开销（每次发包都要验证），因此采用了流式传输，面向流的传输，相对于面向消息的传输，可以减少发送包的数量，从而减少了额外开销。但对于数据传输频繁的程序来讲，使用TCP可能会容易粘包。当然，对接收端的程序来讲，如果机器负荷很重，也会在接收缓冲里粘包。这样就需要接收端额外拆包，增加了工作量。因此TCP特别适合的是数据要求可靠传输，但是不需要太频繁传输的场合（两次操作间隔100ms，具体是由TCP等待发送间隔决定的，取决于内核中的socket的写法）。

UDP，由于面向的是消息传输，它把所有接收到的消息都挂接到缓冲区的接受队列中，因此它对于数据的提取分离就更加方便，但它没有粘包机制。因此，当发送数据量较小的时候，就会发生数据包有效载荷较小的情况，也会增加多次发送的系统发送开销（系统调用，写硬件等）和接收开销。因此，应该最好设置一个比较合适的数据包的包长，来进行UDP数据的发送。（UDP最大载荷为1472，因此最好能每次传输接近这个数的数据量，这特别适合于视频，音频等大块数据的发送，同时通过减少握手来保证流媒体的实时性）

### 拆包与粘包问题

在socket网络程序中，TCP和UDP分别是面向连接和非面向连接的。因此TCP的socket编程，收发两端（客户端和服务器端）都要有成对的socket。发送端为了将多个发往接收端的包，更有效的发到对方，使用了优化方法（Nagle算法），将多次间隔较小、数据量小的数据合并成一个大的数据块，然后进行封包。这样接收端就难以分辨出来了，必须提供科学的拆包机制。

UDP不会出现粘包问题，因为它有消息边界。UDP不会使用块的合并优化算法，接收端的skbuff(套接字缓冲区）采用了链式结构来记录每一个到达的UDP包，在每个UDP包中就有了消息头（消息来源地址，端口等信息）。这样对于接收端来说就容易进行区分处理。

发生TCP粘包或拆包有很多原因，现列出常见的几点。

>- 要发送的数据大于TCP发送缓冲区剩余空间大小，将会发生拆包。
>- 待发送数据大于MSS（最大报文长度），TCP在传输前将进行拆包。
>- 要发送的数据小于TCP发送缓冲区的大小，TCP将多次写入缓冲区的数据一次发送出去，将会发生粘包。
>- 接收数据端的应用层没有及时读取接收缓冲区中的数据，将发生粘包。

粘包情况有两种，一种是粘在一起的包都是完整的数据包，另一种情况是粘在一起的包有不完整的包。不是所有的粘包现象都需要处理，若传输的数据为不带结构的连续流数据（如文件传输），则不必把粘连的包分开（简称分包）。但在实际工程应用中，传输的数据一般为带结构的数据，这时就需要做分包处理。在处理定长结构数据的粘包问题时，分包算法比较简单；在处理不定长结构数据的粘包问题时，分包算法就比较复杂。特别是粘在一起的包有不完整的包的粘包情况，由于一包数据内容被分在了两个连续的接收包中，处理起来难度较大。实际工程应用中应尽量避免出现粘包现象。

>- 发送端给每个数据包添加包首部，首部中应该至少包含数据包的长度，接收端在接收到数据后，通过读取包首部的长度字段，便知道每一个数据包的实际长度。
>- 发送端将每个数据包封装为固定长度（不够的可以通过补0填充），这样接收端每次从接收缓冲区中读取固定长度的数据就自然而然的把每个数据包拆分开来。
>- 可以在数据包之间设置边界，如添加特殊符号，接收端通过这个边界就可以将不同的数据包拆分开。

### TCP状态流转

![TCPHandShake](../Resource/TCPHandShake.jpg)

三次握手的原因：两个SYN报文段是必须的，通信的双方要互相确认信息，比如MSS的长度。最后一次Client发送给Server的确认是为了防止已失效的连接请求报文突然又传送到了Server，而产生错误。已失效的连接请求报文段是这样产生的：Client发送了一个请求建立连接报文段，然后Client不再需要建立连接。而这个失效的连接请求报文段到达Server后，Server以为建立了连接而消耗了资源等待Client发送数据。而采用三次握手，Server要发送SYN+ACK的报文段，并得到Client的确认后才会进入连接建立的状态。在刚才的情况下，Client不会向Server的确认发出确认。Server由于收不到确认，就知道Client并没有要求建立连接。

![TCPWaveShake](../Resource/TCPWaveShake.jpg)

四次挥手的原因：TCP是全双工的，所以双方都要FIN和ACK。只不过一方是被动的，所以看起来称为所谓的四次挥手。如果两边同时断开连接，那就会进入到CLOSING状态，然后就会到达TIME_WAIT状态。

下面来看看这个看似有点多余的TIME_WAIT状态。主动方最后回应一个ACK，主要作用是保证TCP协议的全双工连接能够可靠关闭和保证这次连接的重复数据段从网络中消失。

先说第一点，如果Client直接CLOSED了，那么由于IP协议的不可靠性或者是其它网络原因，导致Server没有收到Client最后回复的ACK。那么Server就会在超时之后继续发送FIN，此时由于Client已经CLOSED了，就找不到与重发的FIN对应的连接，最后Server就会收到RST而不是ACK，Server就会以为是连接错误把问题报告给高层。这样的情况虽然不会造成数据丢失，但是却导致TCP协议不符合可靠连接的要求。所以，Client不是直接进入CLOSED，而是要保持TIME_WAIT，当再次收到FIN的时候，能够保证对方收到ACK，最后正确的关闭连接。

再说第二点，如果Client直接CLOSED，然后又再向Server发起一个新连接，我们不能保证这个新连接与刚关闭的连接的端口号是不同的。也就是说有可能新连接和老连接的端口号是相同的。一般来说不会发生什么问题，但是还是有特殊情况出现：假设新连接和已经关闭的老连接端口号是一样的，如果前一次连接的某些数据仍然滞留在网络中，这些延迟数据在建立新连接之后才到达Server，由于新连接和老连接的端口号是一样的，又因为TCP协议判断不同连接的依据是socket pair，于是，TCP协议就认为那个延迟的数据是属于新连接的，这样就和真正的新连接的数据包发生混淆了。所以TCP连接还要在TIME_WAIT状态等待2倍MSL，这样可以保证本次连接的所有数据都从网络中消失。

![TCPStatus](../Resource/TCPStatus.jpg)

### TCP超时重传

在发送某一个数据以后就开启一个计时器，在一定时间内如果没有得到发送的数据报的ACK报文，那么就重新发送数据，直到发送成功为止。影响超时重传机制协议效率的一个关键参数是重传超时时间（RTO，Retransmission TimeOut）。RTO的值被设置过大过小都会对协议造成不利影响。RTO设长了，重发就慢，没有效率，性能差。RTO设短了，重发的就快，会增加网络拥塞，导致更多的超时，更多的超时导致更多的重发。RTO的设置和RTT有关。连接往返时间（RTT，Round Trip Time），指发送端从发送 TCP 包开始到接收它的立即响应所消耗的时间。

### TCP滑动窗口

滑动窗口提供TCP的可靠性和流控特性。

![Window](../Resource/TCPSendW.jpg)

![Window](../Resource/TCPReceiveW.jpg)

TCP的滑动窗口的可靠性也是建立在 “确认重传” 基础上的。发送窗口只有收到对端对于本段发送窗口内字节的ACK确认，才会移动发送窗口的左边界。接收端可以根据自己的状况通告窗口大小，从而控制发送端的接收，进行流量控制。

发送方窗口的上限值 = Min [rwnd, cwnd]
当rwnd < cwnd 时，是接收方的接收能力限制发送方窗口的最大值。
当cwnd < rwnd 时，则是网络的拥塞限制发送方窗口的最大值。

### TCP拥塞控制

拥塞控制是一个全局性的过程； 流量控制是点对点通信量的控制。TCP拥塞控制4个核心算法：慢开始（slow start）、拥塞避免（Congestion Avoidance）、快速重传（fast retransmit）、快速恢复（fast recovery）。拥塞窗口（cwnd，congestion window），其大小取决于网络的拥塞程度，并且动态地在变化。

慢开始算法的思路就是，不要一开始就发送大量的数据，先探测一下网络的拥塞程度，也就是说由小到大逐渐增加拥塞窗口的大小。为了防止cwnd增长过大引起网络拥塞，还需设置一个慢开始门限ssthresh状态变量。

>- 当 cwnd < ssthresh 时，使用慢开始算法。
>- 当 cwnd > ssthresh 时，改用拥塞避免算法。
>- 当 cwnd = ssthresh 时，慢开始与拥塞避免算法任意。

拥塞避免算法让拥塞窗口缓慢增长，即每经过一个往返时间RTT就把发送发的拥塞窗口cwnd加1，而不是加倍。无论是在慢开始阶段还是在拥塞避免阶段，只要发送方判断网络出现拥塞，就把慢开始门限设置为出现拥塞时的发送窗口大小的一半。然后把拥塞窗口设置为 1，执行慢开始算法。

![Window](../Resource/TCPCC.png)

>- TCP 连接初始化，将拥塞窗口设置为1；
>- 执行慢开始算法，cwnd按指数规律增长，直到cwnd=ssthresh时，开始执行拥塞避免算法，cwnd按线性规律增长；
>- 当网络发生拥塞，把ssthresh值更新为拥塞前ssthresh值的一半，cwnd重新设置为1，重新开始慢开始算法。

### 快重传和快恢复

快速重传 (Fast retransmit) 要求接收方在收到一个失序的报文段后就立即发出重复确认（为的是使发送方及早知道有报文段没有到达对方），而不要等到自己发送数据时捎带确认。快重传算法规定，发送方只要一连收到 3 个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待设置的重传计数器时间到期。

在当发送方连续收到三个重复确认，就执行 “乘法减小” 算法，把慢开始门限ssthresh减半。这是为了预防网络发生拥塞。快速恢复 (Fast Recovery)则不执行慢开始算法。由于发送方现在认为网络很可能没有发生拥塞，因此与慢开始不同之处是现在不执行慢开始算法（即拥塞窗口cwnd现在不设置为1），而是把cwnd值设置为慢开始门限ssthresh减半后的数值，然后开始执行拥塞避免算法（“加法增大”），使拥塞窗口缓慢地线性增大。

### TCP网络编程API

网络中的进程之间如何通信？首先解决的问题是如何标识一个进程。网络层的IP地址可以标识网络中的主机，传输层的“协议+端口号”可以唯一标识主机中的应用程序（进程）。这样利用三元组（IP地址，协议，PORT端口号）就可以标识网络进程。

Socket起源于UNIX。UNIX/Linux基本哲学之一就是一切皆文件，都可以用“open-write/read-close”模式来操作。Socket就是一种特殊的文件，一些Socket函数就是对其进行读写，打开，关闭的操作。

![TCPSocket](../Resource/TCPSocket.png)

服务器Socket接收到客户端Socket请求后被动打开，开始接收客户端请求，直到客户端返回连接信息。这时候Socket进入阻塞状态，即accept()方法一直到客户端返回连接信息后才返回，开始接收下一客户端的连接请求。

#### Socket函数

```cpp
int socket(int domain, int type, int protocal);
```

domain指定要创建的套接字的协议簇地址类型：AF_INET（使用ipv4地址与端口号作为地址），AF_INET6，AF_LOCAL（又称AF_UNIX，Unix的Socket，使用绝对路径作为地址），AF_ROUTE。
type指定套接字类型：SOCK_STREAM（提供面向连接的稳定数据传输），SOCK_DGRAM（使用不连续不可靠的数据报连接），SOCK_RAW，SOCK_PACKET，SOCK_SEQPACKET。
protocol指定协议类型：IPPROTO_TCP，IPPROTO_UDP，IPPROTO_SCTP，IPPROTO_TIPC。通常设置为0，表示使用默认协议。

当调用socket创建一个Socket时，返回的socket描述符存在于协议族（address family，AF_XXX）空间中，但没有一个具体的地址。如果想要赋予一个地址必须调用bind()函数，否则系统在调用connect()，listen()会自动随机分配一个端口。

如果函数执行发生异常，将返回值为INVALID_SOCKET（linux下返回-1）的错误码，程序需要对这个返回值进行检查以保证程序的正常运行。返回的套接字描述符是一个整型类型的值。每个进程的进程空间里都有一个套接字描述符表，该表中存放着套接字描述符和套接字数据结构的对应关系。该表中有一个字段存放新创建的套接字的描述符，另一个字段存放套接字数据结构的地址。每个进程在自己的进程空间中都有一个套接字描述符表，但是套接字数据结构存放在操作系统的内核缓冲里。

```cpp
int bind(int sockfd, const struct sockaddr* addr, socklen_t addrlen);
```

bind()函数把一个地址族的特定地址赋给socket。例如对应的AF_INET，AF_INET6就是把一个ipv4或者ipv6地址和端口号组合赋给socket。函数执行成功返回0，否则返回SOCKET_ERROR。

sockfd为socket描述符，是通过socket函数创建来唯一标识一个Socket的。bind函数给这个描述符绑定一个名字。

addr是一个指针，指向要绑定的sockfd的协议地址。这个地址结构根据创建socket时的地址协议族的不同而不同。

```cpp
// ipv4
struct sockaddr_in {
    sa_family_t    sin_family; // address family: AF_INET
    in_port_t      sin_port;   // port in network byte order
    struct in_addr sinn_addr;  // internet address
}

struct in_addr {
    uint32_t s_addr;           // address in network byte order
}
```

addrlen对应的是地址的长度。

服务器启动时一般会绑定一个众所周知的地址，用于提供服务，所以一般会在调用listen前调用bind()；客户端则不用指定，在connect()时随机生成一个。

```cpp
int listen(int sockfd, int backlog);
int connect(int sockfd, const struct sockaddr* addr, socklen_t addrlen);
```

listen函数的第一个参数即为要监听的socket描述符，第二个参数为相应的socket可以排队的最大连接个数。socket()函数创建的Socket默认是一个主动类型的，listen函数将其设置为被动类型的。

connect函数的第一个参数为客户端的socket描述符，第二个参数为服务器的socket地址，第三个参数为socket地址的长度。客户端调用connect函数和TCP服务器建立连接。

```cpp
int accept(int sockfd, struct sockaddr* addr, socklen_t* addrlen);
```

TCP服务器监听到请求后会调用accept函数接收请求，建立连接后就可以开始网络I/O操作。accept函数的第一个参数为服务器的socket描述符，第二个参数是指针，用于返回客户端的协议地址，第三个参数是指针，用于返回协议地址的长度。如果accept成功，那么其返回值是内核自动生成的一个全新的描述符，代表于客户端的TCP连接。一个服务器通常只创建一个监听socket描述符，它在服务器的生命周期一直存在。内核为每个由服务器进程接收的客户端创建一个已连接的socket描述符，当服务器完成了对某个客户端的服务，相应的socket描述符就被关闭。

```cpp
ssize_t read(int fd, void* buf, size_t count);
```

read函数负责从fd中读取内容，读取成功返回实际所读的字节数，如果返回值是0表示已经读到文件结束，小于0表示出现了错误。如果错误为EINTR说明是由中断引起的，如果是ECONNREST表示网络连接除了问题。三个参数分别是socket描述符，缓冲区和缓冲区长度。

```cpp
ssize_t write(int fd, const void* buf, size_t count);
```

write函数负责向buf中写入内容。写入成功返回写入的字节数。失败返回-1，并设置errno变量。返回值大于0表示写了部分或者全部数据，返回值小于0表示出现了错误。如果错误为EINTR说明是由中断引起的，如果为EPIPE表示网络连接出现了问题（对方关闭了连接）。三个参数分别是socket描述符，缓冲区和缓冲区长度。

其他的网络I/O操作还有以下几组：recv()/send() readv()/writev() recvmsg()/sendmsg() recvfrom()/sendto()。

```cpp
int close(int fd);
```

close一个TCP socket的默认行为是，会把该socket标记为已关闭。然后立即返回到回调进程。close只是使相应的socket描述符的引用计数器-1，只有当引用计数为0时才会出发TCP客服端向服务器发送终止连接的请求。

### TCP协议选项

TCP头部的固定长度为20Byte，选项部分最长为60Byte（数据偏移4bit，取最大1111，偏移量的单位是32bit，即4Byte）。TCP选项部分实际应用有以下几种。

>- SO_REUSEADDR 一般来说一个端口被释放后需要两分钟左右的等待（TIME_WAIT）才能再次使用，使用该选项可以是端口释放后立即被使用。server程序应该在调用bind()之前设置SO_REUSEADDR套接字选项。
>- TCP_NODELAY/TCP_CHORK 为了解决糊涂窗口症候群（有效荷载利用率低）的问题提出了Nagle算法。如果发送端要多次发送包含少量字符的数据包，则发送端会先将第一个小包发出去，而将后面到达的少量字符数据都缓存起来而不立即发送，直到接收端对前一个数据包报文段的ACK确认位置，或者当前字符属于紧急数据，或者积攒到一定数量的数据等多种情况下才将其组成一个较大的数据包发送出去。Nagle是默认开启的，但是并不适用于所有场景。TCP_NODELAY/TCP_CHORK都禁用了该算法。TCP_NODELAY不会讲小包拼接成大包发送而是直接发送小包；在传送大量数据的时候可以设置TCP_CHORK，这样会尽量在每次发送最大的数据量。设置TCP_CHORK后会有200ms的阻塞，当阻塞时间过后数据会自动发送。
>- SO_LINGER 默认close立即返回，但是当发送缓冲区中还有一部分数据的时候系统会尝试将数据发送给对端。通过设置SO_LINGER可以改变close的行为，可以采用默认情形；或者close不被阻塞立即执行，丢弃socket发送缓冲区的数据并向对端发送RST报文来强制关闭（这种非正常的四次握手方式结束TCP连接，TCP连接不会进入TIME_WAIT状态）；或者close调用阻塞进程，直到所有所有数据发送完毕或超时。
>- TCP_DEFFER_ACCEPT 当接收到第一个数据后才创建连接，对于像HTTP等非交互式服务器很有意义，可以防御空链接攻击。使用TCP_DEFER_ACCEPT可以减少用户程序hold的连接数，也可以减少用户调用epoll_ctl和epoll_wait的次数，从而提高了程序的性能。设置listen套接字的TCP_DEFER_ACCEPT选项后， 只当一个链接有数据时是才会从accpet中返回（而不是三次握手完成)。
>- SO_KEEPALIVE 用于保持连接，检测对方主机是否崩溃，避免永远阻塞于TCP连接的输入。
>- SO_SNDTIMEO/SO_RCVTIMEO 分别设置socket的发送和接收超时时间。接收超时会影响read，readv，recv，revcfrom，recvmsg的状态，发送超时会影响write，writev，send，sendto，sendmsg的状态。
>- SO_RECVBUF/SO_SNDBUF 分别设置发送和接收缓冲区的大小。TCP对发送但未收到确认的数据保留一个副本，直到被确认为止。

### HTTP协议

#### web始祖HTTP

全称：超文本传输协议 (HyperText Transfer Protocol) 伴随着计算机网络和浏览器的诞生，HTTP1.0 也随之而来，处于计算机网络中的应用层，HTTP是建立在TCP协议之上，所以HTTP协议的瓶颈及其优化技巧都是基于TCP协议本身的特性，例如tcp建立连接的3次握手和断开连接的4次挥手以及每次建立连接带来的RTT延迟时间。

HTTP的请求和回应报文都分为报文头，0和或者多个请求头，空行和可选的消息体四个部分。HTTP协议是基于行的协议，每一行都以\r\n作为分隔符。HTTP/1.1协议定义了9中方法（或者称动作）来表示Request-URI指定资源的不同操作方式：OPTIONS，HEAD，GET，POST，PUT，DELETE，TRACE，CONNECT，PATHC。

#### HTTP的基本优化

影响一个HTTP网络请求的因素主要有两个：带宽和延迟。如果说还停留在拨号上网的阶段，带宽可能会成为一个比较严重影响请求的问题，但是现在网络基础建设已经使得带宽得到极大的提升，不再会担心由带宽而影响网速，那么就只剩下延迟了。

>- 浏览器阻塞（HOL blocking）：浏览器会因为一些原因阻塞请求。浏览器对于同一个域名，同时只能有4个连接（这个根据浏览器内核不同可能会有所差异），超过浏览器最大连接数限制，后续请求就会被阻塞。
>- DNS查询（DNS Lookup）：浏览器需要知道目标服务器的IP才能建立连接。将域名解析为IP的这个系统就是 DNS。这个通常可以利用DNS缓存结果来达到减少这个时间的目的。
>- 建立连接（Initial connection）：HTTP 是基于 TCP 协议的，浏览器最快也要在第三次握手时才能捎带HTTP请求报文，达到真正的建立连接，但是这些连接无法复用会导致每次请求都经历三次握手和慢启动。三次握手在高延迟的场景下影响较明显，慢启动则对文件类大请求影响较大。

#### HTTP1.0 和 HTTP1.1

HTTP1.0最早在网页中使用是在1996年，那个时候只是使用一些较为简单的网页上和网络请求上，而HTTP1.1则在1999年才开始广泛应用于现在的各大浏览器网络请求中，同时HTTP1.1也是当前使用最为广泛的 HTTP 协议。 主要区别主要体现在：

>- 缓存处理，在HTTP1.0中主要使用header里的`If-Modified-Since` `Expires`来做为缓存判断的标准，HTTP1.1则引入了更多的缓存控制策略例如`Entity tag` `If-Unmodified-Since` `If-Match` `If-None-Match`等更多可供选择的缓存头来控制缓存策略。
>- 带宽优化及网络连接的使用，HTTP1.0 中存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP1.1则在请求头引入了range头域，它允许只请求资源的某个部分，即返回码是206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。
>- 错误通知的管理，在 HTTP1.1 中新增了24个错误状态响应码，如409（Conflict）表示请求的资源与资源的当前状态发生冲突；410（Gone）表示服务器上的某个资源被永久性的删除。
>- Host头处理，在HTTP1.0中认为每台服务器都绑定一个唯一的IP地址，因此请求消息中的URL并没有传递主机名（hostname）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个 IP 地址。HTTP1.1的请求消息和响应消息都应支持Host头域，且请求消息中如果没有Host头域会报告一个错误（400 Bad Request）。
>- 长连接，HTTP 1.1支持长连接（PersistentConnection）和请求的流水线（Pipelining）处理，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟，在HTTP1.1中默认开启Connection：keep-alive，一定程度上弥补了HTTP1.0每次请求都要创建连接的缺点。以下是常见的 HTTP1.0：

![HTTP1&1.0](../Resource/HTTP1&1.0.png)

上面提到过的，HTTP1.x在传输数据时，每次都需要重新建立连接，无疑增加了大量的延迟时间，特别是在移动端更为突出。HTTP1.x在传输数据时，所有传输的内容都是明文，客户端和服务器端都无法验证对方的身份，这在一定程度上无法保证数据的安全性。HTTP1.x 在使用时，header里携带的内容过大，在一定程度上增加了传输的成本，并且每次请求header基本不怎么变化，尤其在移动端增加用户流量。虽然 HTTP1.x 支持了keep-alive，来弥补多次创建连接产生的延迟，但是keep-alive使用多了同样会给服务端带来大量的性能压力，并且对于单个文件被不断请求的服务 (例如图片存放网站)，keep-alive可能会极大的影响性能，因为它在文件被请求之后还保持了不必要的连接很长时间。

#### HTTPS - HTTP over SSL/TLS

为了解决安全问题，网景在1994年创建了 HTTPS，并应用在网景导航者浏览器中。 最初，HTTPS是与SSL一起使用的；在SSL逐渐演变到TLS时（其实两个是一个东西，只是名字不同而已），最新的HTTPS也由在2000年五月公布的RFC 2818正式确定下来。HTTPS是安全版的HTTP，并且由于当今时代对安全性要求更高，chrome和firefox都大力支持网站使用HTTPS，Apple也在ios 10系统中强制app使用HTTPS来传输数据。  

HTTPS协议需要到CA申请证书，一般免费证书很少，需要交费。HTTP协议运行在 TCP 之上，所有传输的内容都是明文，HTTPS运行在SSL/TLS之上，SSL/TLS运行在 TCP 之上，所有传输的内容都经过加密的。HTTP和HTTPS使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。HTTPS可以有效的防止运营商劫持，解决了防劫持的一个大问题。

HTTPS的安全性由SSL/TLS来保证。

按照密钥的使用方式，加密可以分为两大类：对称加密（如AES ChaCha20）和非对称加密（如RSA ECC）。“对称加密”很好理解，就是指加密和解密时使用的密钥都是同一个，是“对称”的。只要保证了密钥的安全，那整个通信过程就可以说具有了机密性。对称加密看上去好像完美地实现了机密性，但其中有一个很大的问题：如何把密钥安全地传递给对方，术语叫“密钥交换”。所以，就出现了非对称加密（也叫公钥加密算法）。它有两个密钥，一个叫“公钥”（public key），一个叫“私钥”（private key）。两个密钥是不同的，“不对称”，公钥可以公开给任何人使用，而私钥必须严格保密。公钥和私钥有个特别的“单向”性，虽然都可以用来加密解密，但公钥加密后只能用私钥解密，反过来，私钥加密后也只能用公钥解密。你或许会说：“把密钥再加密一下发过去就好了”，但传输“加密密钥的密钥”又成了新问题。这就像是“鸡生蛋、蛋生鸡”，可以无限递归下去。只用对称加密算法，是绝对无法解决密钥交换的问题的。非对称加密可以解决“密钥交换”的问题。网站秘密保管私钥，在网上任意分发公钥，你想要登录网站只要用公钥加密就行了，密文只能由私钥持有者才能解密。而黑客因为没有私钥，所以就无法破解密文。

对称加密和非对称加密两者结合起来的混合加密，实现了机密性。

![HTTPS_KEY](../Resource/HTTPS-KeyExchange.png)

实现完整性的手段主要是摘要算法（Digest Algorithm），也就是常说的散列函数、哈希函数（Hash Function）。

数字签名则实现了身份认证和不可否认。数字签名的原理其实很简单，就是把公钥私钥的用法反过来，之前是公钥加密、私钥解密，现在是私钥加密、公钥解密。但又因为非对称加密效率太低，所以私钥只加密原文的摘要，这样运算量就小的多，而且得到的数字签名也很小，方便保管和传输。签名和公钥一样完全公开，任何人都可以获取。但这个签名只有用私钥对应的公钥才能解开，拿到摘要后，再比对原文验证完整性，就可以像签署文件一样证明消息确实是你发的。

只要你和网站互相交换公钥，就可以用“签名”和“验签”来确认消息的真实性，因为私钥保密，黑客不能伪造签名，就能够保证通信双方的身份。比如，你用自己的私钥签名一个消息“我是小明”。网站收到后用你的公钥验签，确认身份没问题，于是也用它的私钥签名消息“我是某宝”。你收到后再用它的公钥验一下，也没问题，这样你和网站就都知道对方不是假冒的，后面就可以用混合加密进行安全通信了。

到现在，综合使用对称加密、非对称加密和摘要算法，我们已经实现了安全的四大特性，是不是已经完美了呢？不是的，这里还有一个“公钥的信任”问题。解决办法：CA的数字证书。公钥的分发需要使用数字证书，必须由 CA 的信任链来验证，否则就是不可信的。作为信任链的源头 CA 有时也会不可信，解决办法有CRL、OCSP，还有终止信任。

总结：保密性：靠混合加密解决，非对称加密实现对称加密秘钥传递，对称加密实现内容加密。完整性：靠摘要算法解决。身份认证：靠数字证书解决，数字证书因为CA机构的信任变成一个完整信任链条，从而实现通过数字证书证明了对方真实身份，但注意身份真实也可能是挂羊头卖狗肉，是一个坏人，所以，有了CRL、OCSP，还有终止信任。不可否认：靠数字签名解决，内容摘要算法得到摘要，私钥加密摘要，对方使用对应公钥解密，得到摘要，再和自己得到的服务器提供的原文摘要对比，一致说明这个内容就是原服务器提供的，由证书说明了服务器的身份。

![HTTP&HTTPS](../Resource/HTTP&HTTPS.png)

如果一个网站要全站由HTTP替换成HTTPS，可能需要关注以下几点：安装CA证书，一般的证书都是需要收费的。在购买证书之后，在证书提供的网站上配置自己的域名，将证书下载下来之后，配置自己的web服务器，同时进行代码改造。HTTPS降低用户访问速度。由于SSL握手，HTTPS对速度会有一定程度的降低，但是只要经过合理优化和部署，HTTPS对速度的影响完全可以接受。在很多场景下，HTTPS速度完全不逊于HTTP，如果使用SPDY，HTTPS的速度甚至还要比HTTP快。相对于HTTPS降低访问速度，其实更需要关心的是服务器端的CPU压力，HTTPS中大量的密钥算法计算，会消耗大量的CPU资源，只有足够的优化，HTTPS的机器成本才不会明显增加。

2012年google提出了SPDY的方案，大家才开始从正面看待和解决老版本HTTP协议本身的问题，SPDY可以说是综合了HTTPS和HTTP两者有点于一体的传输协议，主要解决：

>- 降低延迟，针对HTTP高延迟的问题，SPDY采取了多路复用（multiplexing）。多路复用通过多个请求stream共享一个tcp连接的方式，解决了HOL blocking的问题，降低了延迟同时提高了带宽的利用率。
>- 请求优先级（request prioritization）。多路复用带来一个新的问题是，在连接共享的基础之上有可能会导致关键请求被阻塞。SPDY允许给每个request设置优先级，这样重要的请求就会优先得到响应。比如浏览器加载首页，首页的html内容应该优先展示，之后才是各种静态资源文件，脚本文件等加载，这样可以保证用户能第一时间看到网页内容。
>- header压缩。前面提到HTTP1.x的header很多时候都是重复多余的。选择合适的压缩算法可以减小包的大小和数量。
>- 基于HTTPS的加密协议传输，大大提高了传输数据的可靠性。
>- 服务端推送（server push），采用了SPDY的网页，例如网页有一个sytle.css的请求，在客户端收到sytle.css数据的同时，服务端会将sytle.js的文件推送给客户端，当客户端再次尝试获取sytle.js时就可以直接从缓存中获取到，不用再发请求了。SPDY 构成图：

![HTTP_SPDY](../Resource/HTTPSPDY.png)

#### HTTP/2的前世今生

HTTP/2可以说是SPDY的升级版（其实原本也是基于SPDY设计的），但是HTTP/2跟SPDY仍有不同的地方，主要是以下两点：

>- HTTP/2支持明文HTTP传输，而SPDY强制使用HTTPS
>- HTTP/2消息头的压缩算法采用HPACK，而非SPDY采用的DEFLATE

#### HTTP/2的新特性

HTTP/2把 HTTP 分解成了“语义”和“语法”两个部分，“语义”层不做改动，与HTTP/1 完全一致（即RFC7231）。比如请求方法、URI、状态码、头字段等概念都保留不变，这样就消除了再学习的成本，基于 HTTP 的上层应用也不需要做任何修改，可以无缝转换到 HTTP/2。特别要说的是，与 HTTPS 不同，HTTP/2没有在URI 里引入新的协议名，仍然用“http”表示明文协议，用“https”表示加密协议。

>- 新的二进制格式（Binary Format），HTTP1.x的解析是基于文本。基于文本协议的格式解析存在天然缺陷，文本的表现形式有多样性，要做到健壮性考虑的场景必然很多，二进制则不同，只认0和1的组合。基于这种考虑HTTP/2的协议解析决定采用二进制格式，实现方便且健壮。
>- 多路复用（MultiPlexing），即连接共享，即每一个request都是是用作连接共享机制的。一个request对应一个id，这样一个连接上可以有多个request，每个连接的request可以随机的混杂在一起，接收方可以根据request的id将request再归属到各自不同的服务端请求里面。它把 TCP 协议的部分特性挪到了应用层，把原来的“Header+Body”的消息“打散”为数个小片的二进制“帧”（Frame），用“HEADERS”帧存放头数据、“DATA”帧存放实体数据。

![HTTP2MP](../Resource/HTTP2MP.png)

header压缩，如上文中所言，对前面提到过HTTP1.x的header带有大量信息，而且每次都要重复发送，HTTP/2使用encoder来减少需要传输的header大小，通讯双方各自cache一份header fields表，既避免了重复header的传输，又减小了需要传输的大小。服务端推送（server push），同SPDY一样，HTTP/2也具有server push功能。

HTTP/2定义了一个“流”（Stream）的概念，它是二进制帧的双向传输序列，同一个消息往返的帧会分配一个唯一的流 ID。你可以想象把它成是一个虚拟的“数据流”，在里面流动的是一串有先后顺序的数据帧，这些数据帧按照次序组装起来就是 HTTP/1里的请求报文和响应报文。因为“流”是虚拟的，实际上并不存在，所以 HTTP/2就可以在一个 TCP 连接上用“流”同时发送多个“碎片化”的消息，这就是常说的“多路复用”（  Multiplexing）——多个往返通信都复用一个连接来处理。在“流”的层面上看，消息是一些有序的“帧”序列，而在“连接”的层面上看，消息却是乱序收发的“帧”。多个请求/ 响应之间没有了顺序关系，不需要排队等待，也就不会再出现“队头阻塞”问题，降低了延迟，大幅度提高了连接的利用率。

关于 HTTP2 和 HTTP1.x 的区别大致可以看下图：

![HTTP1.1&2.0](../Resource/HTTP1.1&2.0.png)

![HTTP&HTTPS&HTTP2](../Resource/HTTP&HTTPS&HTTP2.png)

HTTP/2虽然使用“帧”“流”“多路复用”，没有了“队头阻塞”，但这些手段都是在应用层里，而在下层，也就是 TCP 协议里，还是会发生“队头阻塞”。在HTTP/2把多个“请求- 响应”分解成流，交给TCP 后，TCP 会再拆成更小的包依次发送（其实在 TCP 里应该叫segment，也就是“段”）。

在网络良好的情况下，包可以很快送达目的地。但如果网络质量比较差，像手机上网的时候，就有可能会丢包。而 TCP 为了保证可靠传输，有个特别的“丢包重传”机制，丢失的包必须要等待重新传输确认，其他的包即使已经收到了，也只能放在缓冲区里，上层的应用拿不出来，只能“干着急”。

Google 在推 SPDY 的时候就已经意识到了这个问题，于是就又发明了一个新的“QUIC”协议，让HTTP跑在    QUIC上而不是TCP上。而这个“HTTP over QUIC”就是HTTP协议的下一个大版本，HTTP/3。它在HTTP/2的基础上又实现了质的⻜跃，真正“完美”地解决了“队头阻塞”问题。

 HTTP/3有一个关键的改变，那就是它把下层的TCP“抽掉”了，换成了UDP。因为UDP是无序的，包之间没有依赖关系，所以就从根本上解决了“队头阻塞”。并且可以用户态定义流量控制、拥塞避免等算法，优化慢启动、弱网、重建连接等问题。

#### HTTP/2的升级改造

HTTP/2其实可以支持非HTTPS的，但是现在主流的浏览器像chrome，firefox表示还是只支持基于TLS部署的HTTP/2协议，所以要想升级成HTTP/2还是先升级HTTPS为好。当网站已经升级HTTPS之后，那么升级HTTP/2就简单很多，如果你使用NGINX，只要在配置文件中启动相应的协议就可以了，可以参考NGINX白皮书。使用了HTTP/2那么，原本的 HTTP1.x 怎么办，这个问题其实不用担心，HTTP/2完全兼容HTTP1.x的语义，对于不支持HTTP/2的浏览器，NGINX会自动向下兼容的。

#### HTTP状态码

>- 1XX系列：指定客户端应相应的某些动作，代表请求已被接受，需要继续处理。
>- 2XX系列：成功。代表请求已成功被服务器接收、理解、并接受。
>- 3XX系列：重定向。代表需要客户端采取进一步的操作才能完成请求，这些状态码用来重定向，后续的请求地址（重定向目标）在本次响应的 Location 域中指明。
>- 4XX系列：客户端错误。代表了客户端看起来可能发生了错误，妨碍了服务器的处理。
>- 5xx系列：服务器端错误。服务器在处理请求的过程中有错误或者异常状态发生，也有可能是服务器意识到以当前的软硬件资源无法完成对请求的处理。

#### Websocket

Web就是HTTP的意思，Socket就是⽹络编程⾥的套接字，也就是HTTP协议上的⽹络套接字，可以任意双向通信。HTTP是基于TCP的，通过TCP收发的消息⽤HTTP的应⽤层协议解析。WebSocket是⾸先通过HTTP协议把TCP链接建好，然后通过Upgrade字段进⾏协议转换，在收到服务器的101 Switching Protocols应答之后，后续的TCP消息就通过WebSocket协议解析。

WebSocket 和 HTTP/2都是⽤来弥补HTTP协议的⼀些缺陷和不⾜，WebSocket 主要解决双向通信、全双⼯问题，HTTP/2主要解决传输效率的问题，两者在⼆进制帧的格式上也不太⼀样，HTTP/2有多路复⽤、优先级和流的概念。

#### HTTP性能优化

性能优化是⼀个复杂的概念，在HTTP⾥可以分解为服务器性能优化、客户端性能优化和传输链路优化；服务器有三个主要的性能指标：吞吐量、并发数和响应时间，此外还需要考虑资源利⽤率； 客户端的基本性能指标是延迟，影响因素有地理距离、带宽、DNS 查询、TCP 握⼿等；从服务器到客户端的传输链路可以分为三个部分，我们能够优化的是前两个部分，也就是“第⼀公⾥”和“中间⼀公⾥”。

花钱购买硬件、软件或者服务可以直接提升⽹站的服务能⼒，其中最有价值的是 CDN；不花钱也可以优化 HTTP，三个关键词是“开源”“节流”和“缓存”：后端应该选⽤⾼性能的 Web 服务器，开启⻓连接，提升 TCP 的传输效率，前端应该启⽤ gzip、br 压缩，减⼩⽂本、图⽚的体积，尽量少传不必要的头字段，缓存是⽆论何时都不能忘记的性能优化利器，应该总使⽤ Etag 或 Last-modified 字段标记资源；升级到HTTP/2能够直接获得许多⽅⾯的性能提升，但要留意⼀些 HTTP/1 的“反模式”。

### 网络字节序与主机序

不同的CPU有不同的字节序类型，字节序类型是指整数在内存中保存的顺序，称为主机序。Little Endian将低序字节存储在起始位置，Big Endian将高序字节存储在起始位置。比如数字0x12345678，采用Big Endian为 （低地址）12|34|56|78（高地址），采用Little Endian为（低地址）78|56|34|12（高地址）。

C/C++语言编写的程序里数据存储顺序是跟编译平台所在的CPU相关的，而JAVA编写的程序则唯一采用big endian方式来存储数据。试想，如果你用C/C++语言在x86平台下编写的程序跟别人的JAVA程序互通时会产生什么结果？就拿上面的0x12345678来说，你的程序传递给别人的一个数据，将指向0x12345678的指针传给了JAVA程序，由于JAVA采取big endian方式存储数据，很自然的它会将你的数据翻译为0x78563412。因此，在你的C程序传给JAVA程序之前有必要进行字节序的转换工作。

![BigEndian](../Resource/Big-Endian.png)

![LittleEndian](../Resource/Little-Endian.png)

所有网络协议也都是采用big endian的方式来传输数据的。所以有时我们也会把big endian方式称之为网络字节序。当两台采用不同字节序的主机通信时，在发送数据之前都必须经过字节序的转换成为网络字节序后再进行传输。
