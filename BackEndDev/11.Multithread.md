# C++11多线程编程

C++11 新标准中引入了四个头文件来支持多线程编程，他们分别是atomic thread mutex condition_variable future。

>- atomic：声明了两个类, std::atomic和std::atomic_flag，另外还声明了一套C风格的原子类型和与C兼容的原子操作的函数。
>- thread：声明了std::thread类，另外std::this_thread命名空间也在该头文件中。
>- mutex：声明了与互斥量(mutex)相关的类，包括std::mutex系列类，std::lock_guard std::unique_lock以及其他的类型和函数。
>- condition_variable：声明了与条件变量相关的类，包括std::condition_variable和std::condition_variable_any。
>- future：该头文件主要声明了std::promise,std::package_task两个Provider类，以及std::future和std::shared_future两个future类，另外还有一些与之相关的类型和函数，std::async()函数就声明在此头文件中。

## 线程 thread

>- std::thread()，创建线程一般会绑定一个底层的线程。若该thread还绑定好函数对象，则即刻将该函数运行于thread的底层线程。
>- joinable()：是否可以阻塞至该thread绑定的底层线程运行完毕。
>- join()：本线程阻塞直至该thread的底层线程运行完毕。
>- detach()：该thread绑定的底层线程分离出来，任该底层线程继续运行，thread失去对该底层线程的控制。

## 互斥变量 mutex

为了避免多线程对共享变量的一段操作会发生冲突，引入了互斥体和锁。

>- std::mutex，互斥体，一般搭配锁使用，也可自己锁住自己（lock(),unlock()）。若互斥体被第二个锁请求锁住，则第二个锁所在线程被阻塞直至第一个锁解锁。
>- std::lock_guard，构造时请求上锁，释放时解锁，性能耗费较低。适用区域的多线程互斥操作。
>- std::unique_lock，更多功能、更灵活的锁，随时可解锁或重新锁上（减少锁的粒度），性能耗费比前者高一点点。适用灵活的区域的多线程互斥操作。一般来说使用unique_lock例子有：延迟锁定，锁的所有权需要在不同的作用域之间转移。

```cpp
// 为了避免死锁，常见的建议是始终使用相同的顺序锁定多个互斥变量。
// 锁定两个互斥锁的使用示例

struct bank_account {
    explicit bank_account(int balance) : balance(balance) {}
    int balance;
    std::mutex m;
};

void transfer(bank_account &from, bank_account &to, int amount) {
    // 锁定两个互斥而不死锁，提供了关于锁定给定互斥元的全或者无语义。
    std::lock(from.m, to.m);

    // 保证二个已锁定互斥在作用域结尾解锁
    // adopt_lock告知lock_gurad互斥变量已经锁定，不要在构造函数中尝试锁定互斥变量
    std::lock_guard<std::mutex> lock1(from.m, std::adopt_lock);
    std::lock_guard<std::mutex> lock2(to.m, std::adopt_lock);

// 等价方法：

// defer_lock用于表示在构造函数中，互斥变量应保持未锁定。
//    std::unique_lock<std::mutex> lock1(from.m, std::defer_lock);
//    std::unique_lock<std::mutex> lock2(to.m, std::defer_lock);

// 注意：在unique_lock对象上（而非互斥变量）上调用std::lock()
//    std::lock(lock1, lock2);

    from.balance -= amount;
    to.balance += amount;
}

int main() {
    bank_account my_account(100);
    bank_account your_account(50);

    // thread的参数默认都是值传递，使用引用语义则需要调用std::ref()，也可能调用std::move()提高性能
    std::thread t1(transfer, std::ref(my_account), std::ref(your_account), 10);
    std::thread t2(transfer, std::ref(your_account), std::ref(my_account), 5);

    t1.join();
    t2.join();
}
```

## once_flag call_once

std::once_flag和std::call_once()用于处理once操作。call_once()比显示使用互斥体通常会有更低的开销，特别是初始化已经完成的时候。

```cpp
std::shared_ptr<resource> res;
std::once_flag flag;

void init() {
    res.reset(new resource);
}

void foo(){
    std::call_once(flag, init);
    res->do_something();
}
```

进一步的，c++11中的局部静态变量是thread safe，对于需要单一全局实例的场合，可以用作call_once()的替代品。

```cpp
class foo;

foo& get_foo_instance(){
    static foo instance;  // 初始化保证是线程安全的
    return instance;
}
```

## 条件变量 condition_variable

条件变量一般是用来实现多个线程的等待队列，即主线程通知（notify）有活干了，则等待队列中的其它线程就会被唤醒。条件变量在使用时涉及到重复的加锁解锁，所以传入的互斥对像是unique_lock，而非lock_guard。

>- cond.wait(std::unique_lockstd::mutex& lock, Predicate pred = \[\](){return true;}); pred()为true时直接返回，pred()为false时，lock必须满足已被当前线程锁定的前提。执行原子地释放锁定，阻塞当前线程，并将其添加到等待*this的线程列表中。
>- notify_one()/notify_all()：激活某个或者所有等待的线程，被激活的线程重新获得锁。

虚假唤醒：处于等待的添加变量可以通过notify_one()/notify_all()进行唤醒，调用函数进行信号的唤醒时，处于等待的条件变量会重新进行互斥锁的竞争。没有得到互斥锁的线程就会发生等待转移（wait morphing），从等待信号量的队列中转移到等待互斥锁的队列中，一旦获取到互斥锁的所有权就会接着向下执行，但是此时其他线程已经执行并重置了执行条件(例如一个活只需要两个线程来干，通知完两个线程后重置执行条件)，这可能导致该线程执行引发未定义的错误。

```cpp
// 不能应对虚假唤醒
if(pred()){
   cv.wait(lock);
}
// 利用while重复判断执行条件，可以应对虚假唤醒
while(!pred()){
   cv.wait(lock);
}
// C++11提供了更方便的语法，将判断条件作为一个参数，实际上等价于前者
cv.wait(lock, pred);
```

## 获取者 future

用于访问共享状态（即获取值）。当future的状态还不是ready时就调用一个绑定的promise, packaged_task等的析构函数，会在期望里存储一个异常。std::future有局限性，在很多线程等待时，只有一个线程能获取等待结果。

```cpp
std::future<int> f = std::async(func);
int res = f.get();
```

>- share()：分享同一个共享状态给另一个future。
>- wait()：若共享状态不是ready，则阻塞直至ready。
>- get()：获得共享状态的值，若共享状态不是ready，则阻塞直至ready。

std::shared_future

当需要多个线程等待相同的事件的结果，即多处访问同一个共享状态，需要用std::shared_future来替代std::future。shared_future与future类似，但shared_future可以拷贝、多个shared_future可以共享某个共享状态的最终结果，即共享状态的某个值或者异常。shared_future可通过某个future对象隐式转换，或通过future::share()显示转换，无论哪种转换，被转换的那个future对象都会变为not-valid。

## 提供者promise -> future

构造时产生一个未就绪的共享状态（包含存储的T值和是否就绪的状态）。可设置T值，并让状态变为ready。std::promise允许move语义（右值构造，右值赋值），但不允许拷贝（拷贝构造、赋值），std::future亦然。std::promise和std::future合作共同实现了多线程间通信。

```cpp
void read(std::future<std::string> *future) {
    // future会一直阻塞，直到有值到来
    std::cout << future->get() << std::endl;
}

int main() {
    // promise 相当于生产者
    std::promise<std::string> promise;
    // future 相当于消费者, 右值构造
    std::future<std::string> future = promise.get_future();
    // 另一线程中通过future来读取promise的值
    std::thread thread(read, &future);
    // 让read等一会儿:)
    std::this_thread::sleep_for(seconds(1));
    // 设置value
    promise.set_value("hello future");
    // 等待线程执行完成
    thread.join();

    return 0;
}
```

>- get_future()：共享状态绑定到future对象。一个std::promise实例只能与一个std::future关联共享状态，当在同一个std::promise上反复调用get_future会抛出future_error异常。
>- set_value()：设置共享状态的T值，并让状态变为ready，则绑定的future对象可get()。set_value只能被调用一次，多次调用会抛出std::future_error异常。
>- set_exception()：为promise设置异常，此后promise的共享状态变标志变为ready。
>- set_value_at_thread_exit()：设置共享状态的值，但是不将共享状态的标志设置为ready，当线程退出时该promise对象会自动设置为ready。调用future::get的线程会被阻塞；当持有promise的线程退出时，调用future::get的线程解除阻塞，同时get返回set_value_at_thread_exit所设置的值。该函数已经设置了 promise 共享状态的值，如果在线程结束之前有其他设置或者修改共享状态的值的操作，则会抛出 future_error( promise_already_satisfied)。

如果promise直到销毁时，都未设置过任何值，则promise会在析构时自动设置为std::future_error，这会造成std::future.get()抛出std::future_error异常。

## 可异步执行的打包任务 packaged_task -> future

构造时绑定一个函数对象，也产生一个未就绪的共享状态。通过thread启动或者仿函数形式启动该函数对象。但是相比promise，没有提供set_value()公用接口，而是当执行完绑定的函数对象，其执行结果返回值或所抛异常被存储于能通过std::future对象访问的共享状态中。packaged_task对象需要交付给thread运行或者显示调用。

>- get_future()：共享状态绑定到future对象。
>- valid()：检查当前packaged_task是否和一个有效的共享状态相关联，默认构造函数生成的packaged_task对象返回false，除非中间进行了move赋值操作或者swap操作。
>- make_ready_at_thread_exit()：该函数会调用被包装的任务，并向任务传递参数，类似std::packaged_task的operator()成员函数。但是与operator()函数不同的是，make_ready_at_thread_exit并不会立即设置共享状态的标志为ready，而是在线程退出时设置共享状态的标志。该函数已经设置了promise共享状态的值，如果在线程结束之前有其他设置或者修改共享状态的值的操作，则会抛出std::future_error(promise_already_satisfied)。
>- reset()：packaged_task相比与promice是可以重复使用的。调用reset后，需要重新get_future，以便获取下次执行的结果。由于是重新构造了promise，因此reset操作并不会影响之前调用的make_ready_at_thread_exit结果，也即之前的定制的行为在线程退出时仍会发生。
>- operator()(Args... args)：调用该 packaged_task 对象所包装的对象(通常为函数指针，函数对象，lambda表达式等)，传入的参数为args。如果成功调用packaged_task所包装的对象，则返回值被保存在future中；如果调用packaged_task所包装的对象失败且抛出了异常，则异常也会被保存在future中。以上两种情况都使共享状态的标志变为ready。

```cpp
int sum(int a, int b) {
    return a + b;
}

int main() {
    std::packaged_task<int(int,int)> task(sum);
    std::future<int> future = task.get_future();

    // std::promise一样，std::packaged_task支持move，但不支持拷贝
    // std::thread的第一个参数不止是函数，还可以是一个可调用对象，即支持operator()(Args...)操作
    std::thread t(std::move(task), 1, 2);
    // 等待异步计算结果
    std::cout << "1 + 2 => " << future.get() << std::endl;

    t.join();
    return 0;
}
```

## 自动异步执行的任务 async -> future

std::async(std::launch::async | std::launch::deferred, Func, Args...)

异步执行一个函数，其函数执行完后的返还值绑定给使用std::async的futrue（其实是封装了thread, packged_task的功能，使异步执行一个任务更为方便）。若用创建std::thread执行异步行为，硬件底层线程可能不足，产生错误。而std::async将这些底层细节掩盖住，如果使用默认参数则与标准库的线程管理组件一起承担线程创建和销毁、避免过载、负责均衡的责任。所以尽量使用以任务为驱动的async操作设计，而不是以线程为驱动的thread设计。

std::async中的第一个参数是启动策略，它控制std::async的异步行为，可以用三种不同的启动策略来创建。

>- std::launch::async参数 保证异步行为，即传递函数将在单独的线程中执行。
>- std::launch::deferred参数 当其他线程调用get()/wait()来访问共享状态时，将调用非异步行为。
>- std::launch::async | std::launch::deferred参数 是默认行为。有了这个启动策略，它可以异步运行或不运行，这取决于系统的负载。

```cpp
std::future<std::string> resultFromDB = std::async(std::launch::async, fetchDataFromDB, "Data1");
std::string data1 = resultDromDB.get();
```

## 时间库 chrono

多线程中的阻塞调用，比如wait()系列，基本上都提供了有时间限制的重载版本：wait_for() 等待一段时间，wait_until()等待至指定的时间点。休眠线程使用std::this_thread::sleep_for()或者std::this_thread::sleep_until()。基于时间段的等待使用类库内部的匀速时钟来衡量时间。

时间段和时间都有相应的转换函数duration_cat<>()/time_point_cast<>()。时间段和时间点都不提供流式输出，时间段提供count()方法，时间点提供since_time_since_epoch()方法。

```cpp
// 时间段
std::chrono::duration<rep, period> d(n);
// period表示一个单位时间周期，类型std::ratio<x, y>表示 x/y * 1s -> std::ratio<1, 25> 即 1/25 s ，ratio<1, -2>表示单位时间是-0.5秒。
// rep是一个精度类型，比如int float等，表示rep个period。上述声明的d(n)即表示n个period的时间长度。

// 预定义的时间长度单位
typedef duration <Rep, ratio<3600,1>> hours;
typedef duration <Rep, ratio<60,1>> minutes;
typedef duration <Rep, ratio<1,1>> seconds;
typedef duration <Rep, ratio<1,1000>> milliseconds;
typedef duration <Rep, ratio<1,1000000>> microseconds;
typedef duration <Rep, ratio<1,1000000000>> nanoseconds;

// 举个例子
timespec t1, t2;
clock_gettime(CLOCK_MONOTONIC, &t1);
std::chrono::duration<float, std::ratio<60, 1>> s60(0.1); // 0.1 * 1/60 s -> 6s
std::this_thread::sleep_for(s60 + std::chrono::seconds(1)); // 1s
std::chrono::milliseconds  ms = std::chrono::duration_cast<std::chrono::milliseconds > (s60); // duration_cast
clock_gettime(CLOCK_MONOTONIC, &t2);
cout << t2.tv_sec - t1.tv_sec << endl; // 7s
```

```cpp
// 时间点

// 始终的当前时间可以通过该时钟类型的静态成员函数now()来获取
std::chrono::system_clock::time_point tp = std::chrono::system_clock::now();
cout << tp.time_since_epoch().count() << endl; // 输出该时间点自1970年
// 时钟类型有
// system_clock：当前系统范围(即对各进程都一致)的一个实时的日历时钟，可以和time_t相互转换
// steady_clock：当前系统实现的一个维定时钟，该时钟的每个时间嘀嗒单位是均匀的
// high_resolution_clock：当前系统实现的一个高分辨率时钟
```

## 内存模型

原子类型的大多数API都需要程序员提供一个std::memory_order（可译为内存序，访存顺序）的枚举类型值作为参数，比如：atomic_store，atomic_load，atomic_exchange，atomic_compare_exchange等API的最后一个形参为std::memory_order order，默认值是std::memory_order_seq_cst（顺序一致性）。一般来讲，内存模型可分为静态内存模型和动态内存模型，静态内存模型主要涉及类的对象在内存中是如何存放的。动态内存模型可理解为存储一致性模型，主要是从行为（behavioral）方面来看多个线程对同一个对象同时（读写）操作时（concurrency）所做的约束，动态内存模型理解起来稍微复杂一些，涉及了内存，Cache，CPU 各个层次的交互，尤其是在共享存储系统中，为了保证程序执行的正确性，就需要对访存事件施加严格的限制。

文献中常见的存储一致性模型包括顺序一致性模型，处理器一致性模型，弱一致性模型，释放一致性模型，急切更新释放一致性模型、懒惰更新释放一致性模型，域一致性模型以及单项一致性模型。不同的存储一致性模型对访存事件次序的限制不同，因而对程序员的要求和所得到的的性能也不一样。存储一致性模型对访存事件次序施加的限制越弱，我们就越有利于提高程序的性能，但编程实现上更困难。

顺序一致性模型由 Lamport 于 1979 年提出。顺序一致性模型最好理解但代价太大。如果在共享存储系统中多机并行执行的结果等于把每一个处理器所执行的指令流按照某种方式顺序地交织在一起在单机上执行的结果，则该共享存储系统是顺序一致性的。

顺序一致性不仅在共享存储系统上适用，在多处理器和多线程环境下也同样适用。而在多处理器和多线程环境下理解顺序一致性包括两个方面，(1). 从多个线程平行角度来看，程序最终的执行结果相当于多个线程某种交织执行的结果，(2)从单个线程内部执行顺序来看，该线程中的指令是按照程序事先已规定的顺序执行的(即不考虑运行时CPU乱序执行和Memory Reorder)。

现代的CPU大都支持多发射和乱序执行，在乱序执行时，指令被执行的逻辑可能和程序汇编指令的逻辑不一致，在单线程条件下，CPU的乱序执行不会带来大问题，但是在多核多线程时代，当多线程共享某一变量时，不同线程对共享变量的读写就应该格外小心，不适当的乱序执行可能导致程序运行错误。因此，CPU的乱序执行也需要作出适当的约束。

那么这个约束是什么呢？答曰：内存模型。C++程序员要想写出高性能的多线程程序必须理解内存模型，编译器会给你的程序做优化(静态)，CPU为了提升性能也有乱序执行(动态)，总之，程序在最终执行时并不会按照你之前的原始代码顺序来执行，因此内存模型是程序员、编译器，CPU之间的契约，遵守契约后大家就各自做优化，从而尽可能提高程序的性能。

```cpp
enum memory_order {
    memory_order_relaxed, // 强烈建议不要使用
    memory_order_consume,
    memory_order_acquire,
    memory_order_release,
    memory_order_acq_rel, // 如果你是大佬 可以试试
    memory_order_seq_cst  // 默认
};
```

std::memory_order规定了普通访存操作和相邻的原子访存操作之间的次序是如何安排的，在多核系统中，当多个线程同时读写多个变量时，其中的某个线程所看到的变量值的改变顺序可能和其他线程写入变量值的次序不相同。同时不同的线程所观察到的某变量被修改次序也可能不相同。然而，如果保证所有对原子变量的操作都是顺序的话，可能对程序的性能影响很大，因此可以通过std::memory_order来指定编译器对访存次序所做的限制。在原子类型的API中可以通过额外的参数指定该原子操作的访存次序(内存序)，默认的内存序是std::memory_order_seq_cst。

上述访存次序可以分为3类，顺序一致性模型(std::memory_order_seq_cst)，Acquire-Release模型(std::memory_order_consume, std::memory_order_acquire, std::memory_order_release, std::memory_order_acq_rel) 和Relax模型(std::memory_order_relaxed)。

>- memory_order_seq_cst 顺序一致性模型，这个是默认提供的最强的一致性模型。
>- memory_order_release/acquire/consume 提供release、acquire或者consume, release语意的一致性保障。在这种序列模型下，原子load操作是acquire操作(memory_order_acquire), 原子store操作是release操作(memory_order_release), 原子read_modify_write操作(如fetch_add() exchange())可以是 acquire, release 或两者皆是(memory_order_acq_rel)。同步是成对出现的，它出现在一个进行release操作和一个进行acquire操作的线程间。一个release操作syncrhonized-with一个想要读取刚才被写的值的acquire操作。这意味着不同线程仍然看到了不一样的次序，但这次序是被限制过了的。当在此再引入happens-before关系时，就可以实现更大规模的顺序关系。memory_order_consume没搞懂。
>- memory_order_relaxed 提供松散一致性模型保障，不提供operation order保证。

三种不同的内存模型在不同类型的CPU上(如 X86，ARM，PowerPC等)所带来的代价也不一样。例如，在X86或者X86-64平台下，Acquire-Release类型的访存序不需要额外的指令来保证原子性，即使顺序一致性类型操作也只需要在写操作（Store）时施加少量的限制，而在读操作（Load）则不需要花费额外的代价来保证原子性。

顺序一致性符合直觉，非顺序一致性下，事件不再有单一的全局顺序，线程不必和事件的顺序一致。在没有其他的顺序约束时，唯一的要求时所有的线程对于每个独立变量的修改顺序达成一致。

### 理解C++的Memoy order

[link](http://senlinzhan.github.io/2017/12/04/cpp-memory-order/ "原文")

如果不使用任何同步机制，在多线程中读写同一个变量，那么程序的结果是难以预料的。简单来说，编译器以及CPU的一些行为，会影响到程序的执行结果：即使是简单的语句也不保证是原子操作；CPU可能会调整指令的执行顺序；在CPU cache的影响下，一个CPU执行了某个指令，不会立即被其它CPU看见。原子操作说的是，一个操作的状态要么就是未执行，要么就是已完成，不会看见中间状态。

多线程读写同一变量需要使用同步机制，最常见的同步机制就是std::mutex和std::atomic。然而，从性能角度看，通常使用std::atomic会获得更好的性能。默认情况下，std::atomic使用的是 Sequentially-consistent ordering（memory_order_seq_cst）。但在某些场景下，合理使用其它ordering，可以让编译器优化生成的代码，从而提高性能。

#### Relaxed ordering

在这种模型下，std::atomic的load()和store()都要带上memory_order_relaxed参数。Relaxed ordering 仅仅保证load()和store()是原子操作，除此之外，不提供任何跨线程的同步。

```cpp
std::atomic<int> x = 0;     // global variable
std::atomic<int> y = 0;     // global variable

// Thread-1:
r1 = y.load(memory_order_relaxed); // A
x.store(r1, memory_order_relaxed); // B

// Thread-2:
r2 = x.load(memory_order_relaxed); // C
y.store(42, memory_order_relaxed); // D
```

执行完上面的程序，可能出现r1 == r2 == 42。理解这一点并不难，因为编译器允许调整 C 和 D 的执行顺序。如果程序的执行顺序是 D -> A -> B -> C，那么就会出现r1 == r2 == 42。

如果某个操作只要求是原子操作，除此之外，不需要其它同步的保障，就可以使用 Relaxed ordering。程序计数器是一种典型的应用场景：

```cpp
#include <cassert>
#include <vector>
#include <iostream>
#include <thread>
#include <atomic>

std::atomic<int> cnt = {0};

void f() {
    for (int n = 0; n < 1000; ++n) {
        cnt.fetch_add(1, std::memory_order_relaxed);
    }
}

int main() {
    std::vector<std::thread> v;
    for (int n = 0; n < 10; ++n) {
        v.emplace_back(f);
    }
    for (auto& t : v) {
        t.join();
    }
    assert(cnt == 10000);
    return 0;
}
```

#### Release-Acquire ordering

在这种模型下，store()使用memory_order_release，而load()使用memory_order_acquire。这种模型有两种效果，第一种是可以限制CPU指令的重排：在store()之前的所有读写操作，不允许被移动到这个store()的后面。在load()之后的所有读写操作，不允许被移动到这个load()的前面。

除此之外，还有另一种效果：假设Thread-1 store()的那个值，成功被Thread-2 load()到了，那么Thread-1 在store()之前对内存的所有写入操作，此时对Thread-2 来说，都是可见的。

```cpp
std::atomic<int> x = 0;     // global variable
std::atomic<int> y = 0;     // global variable

// Thread-1:
r1 = y.load(memory_order_relaxed); // A
x.store(r1, memory_order_release); // B

// Thread-2:
r2 = x.load(memory_order_acquire); // C
y.store(42, memory_order_relaxed); // D
```

首先A不允许被移动到B的后面。同样D也不允许被移动到C的前面。当C从while循环中退出了，说明C读取到了B store()的那个值，此时，Thread-2保证能够看见Thread-1执行B之前的所有写入操作（也即是 A）。

```cpp
#include <thread>
#include <atomic>
#include <cassert>
#include <string>

std::atomic<bool> ready{false};
int data = 0;

void producer() {
    data = 100;
    ready.store(true, std::memory_order_release);
}

void consumer() {
    while (!ready.load(std::memory_order_acquire));
    assert(data == 100);
}

int main() {
    std::thread t1(producer);
    std::thread t2(consumer);
    t1.join();
    t2.join();
    return 0;
}
```

## 原子类型 atomic

最简单的原子类型: atomic_flag。atomic_flag 一种简单的原子布尔类型，只支持两种操作，test_and_set()和clear()。test_and_set()函数检查std::atomic_flag标志，如果std::atomic_flag之前没有被设置过，则设置std::atomic_flag的标志，并返回先前该std::atomic_flag对象是否被设置过，如果之前std::atomic_flag对象已被设置，则返回true，否则返回false。clear()恢复其为clear状态。

结合 std::atomic_flag::test_and_set() 和 std::atomic_flag::clear()，std::atomic_flag 对象可以当作一个简单的自旋锁使用。

```cpp
class spinlock_mutex {
public:
    spinlock_mutex():flag(ATOMIC_FLAG_INIT){}

public:
    void lock(){
        // set失败返回原来的值 -> true
        // 则继续循环直到操作成功过返回原来的值 -> false
        // 真的有点反人类呢
        while(flag.test_and_set()){}
    }

    void unlock(){
        flag.clear();
    }
private:
    std::atomic_flag flag;

};
```

std::atomic是模板类，一个模板类型为T的原子对象中封装了一个类型为T的值。原子类型对象的主要特点就是从不同线程访问不会导致数据竞争。

```cpp
template <class T> struct atomic;

// 基本成员函数

// 判断该对象是否具备lock-free特性。
bool is_lock_free();

// 修改被封装的值。
void store (T val, memory_order sync = memory_order_seq_cst);
// 读取被封装的值。
T load (memory_order sync = memory_order_seq_cst);

// 与load功能类似，也是读取被封装的值，operator T()是类型转换(type-cast)操作。
operator T(); // 比如说一个 std::atomic<int> foo, foo == 0 这条语句就是取值和隐式类型转换。

// 读取并修改被封装的值，exchange会将val指定的值替换掉之前该原子对象封装的值，并返回之前该原子对象封装的值。
T exchange (T val, memory_order sync = memory_order_seq_cst);

// 比较并交换被封装的值(weak)与参数expected所指定的值是否相等。
// 相等则用val替换原子对象的旧值，返回true；
// 不相等，则用原子对象的旧值替换expected，返回false； -> 值返回的骚操作
bool compare_exchange_weak (T& expected, T val, memory_order sync = memory_order_seq_cst);
bool compare_exchange_strong (T& expected, T val, memory_order sync = memory_order_seq_cst);
// 这个重载版本，memory_order的选择取决于比较的结果。
// 该函数直接比较原子对象所封装的值与参数expected的物理内容，所以某些情况下，对象的比较操作在使用 operator==() 判断时相等，但 compare_exchange_weak 判断时却可能失败，因为对象底层的物理内容中可能存在位对齐或其他逻辑表示相同但是物理表示不同的值(比如 true 和 2 或 3，它们在逻辑上都表示"真"，但在物理上两者的表示并不相同)。
bool compare_exchange_weak (T& expected, T val, memory_order success, memory_order failure);
bool compare_exchange_strong (T& expected, T val, memory_order success, memory_order failure);

// weak版本的CAS允许偶然出乎意料的返回（比如在字段值和期待值一样的时候却返回了false），不过在一些循环算法中，这是可以接受的。通常它比起strong有更高的性能。

/*

想要性能，使用compare_exchange_weak+循环来处理。
想要简单，使用compare_exchange_strong。
如果是x86平台，两者没区别
如果想在移值的时候，拿到高性能，用compare_exchange_weak。

*/
```

C++11 标准库中的std::atomic针对整形(integral)和指针类型的特化版本新增了一些算术运算和逻辑运算操作。

```cpp
// 将原子对象的封装值加val，并返回原子对象的旧值。如果第二个参数不指定，等价于 +=
integral fetch_add(integral, memory_order = memory_order_seq_cst);
// 将原子对象的封装值减val，并返回原子对象的旧值。如果第二个参数不指定，等价于 -=
integral fetch_sub(integral, memory_order = memory_order_seq_cst);
// 只适用于整型
integral fetch_and(integral, memory_order = memory_order_seq_cst);
// 只适用于整型
integral fetch_or(integral, memory_order = memory_order_seq_cst);
// 只适用于整型
integral fetch_xor(integral, memory_order = memory_order_seq_cst);

integral operator++(int);
integral operator--(int);
integral operator++();
integral operator--();
integral operator+=(integral);
integral operator-=(integral);
integral operator&=(integral);
integral operator|=(integral);
integral operator^=(integral);
```

## thread_local

thread_local变量在线程创建时生成(不同编译器实现略有差异，但在线程内变量第一次使用前必然已构造完毕)。线程结束时被销毁(析构，利用析构特性，thread_local变量可以感知线程销毁事件)。每个线程都拥有其自己的变量副本。thread_local可以和static或extern联合使用，这将会影响变量的链接属性。

```cpp
class A {
public:
  A() {
    std::cout << std::this_thread::get_id()
              << " " << __FUNCTION__
              << "(" << (void *)this << ")"
              << std::endl;
  }
  ~A() {
    std::cout << std::this_thread::get_id()
              << " " << __FUNCTION__
              << "(" << (void *)this << ")"
              << std::endl;
  }
  // 线程中，第一次使用前初始化
  void doSth() {}
};

thread_local A a;

int main() {
  a.doSth();
  std::thread t([]() {
    std::cout << "Thread: "
              << std::this_thread::get_id()
              << " entered" << std::endl;
    a.doSth();
  });
  t.join();
  return 0;
}

...
$> g++ -std=c++11 -o debug/tls.out ./thread_local.cpp
$> ./debug/tls.out
01 A(0xc00720)
Thread: 02 entered
02 A(0xc02ee0)
02 ~A(0xc02ee0)
01 ~A(0xc00720)
$>
```

## 为并发设计数据结构的准则：保证存取是安全的以及允许真正的并发存取

1. 保证当数据结构不变性被别的线程破坏时的状态不会被被别的线程看到。
2. 注意避免数据结构借口所固有的竞争，通过为完整操作提供函数，而不是提供操作步骤。
3. 注意当出现例外时，数据结构是怎么样来保证不变性不被破坏的。
4. 当使用数据结构时，通过限制锁的范围和避免使用嵌套锁，来降低死锁的机会。

## 编写无锁数据结构的准则

1. 使用std::memory_order_seq_cst作为原型，避免过早优化。
2. 使用无锁内存回收模式。无锁代码的最大问题之一就是管理内存。
3. 当心ABA问题。线程1读取原子变量x并在后续的调用中阻塞，中途变量发生变化但又改回去了，线程1重新获得x并执行了比较/交换操作，但这个值可能不是原来的值了。避免这种问题的方法就是在变量x上加一个ABA计数器。
4. 识别忙于等待的循环以及辅助其他线程。

## 线程池

最简单的线程池

线程池最简单的形式是含有一个固定数量的工作线程（典型的数量是std::thread::hardware_concurrency()的返回值）来处理任务。当有任务需要处理时，调用一个函数将任务放到等待队列中；每个工作线程都是从该队列中取出任务，执行完之后继续取出更多的任务来处理。

如果所有的任务是完全独立的，工作线程之间互不依赖，线程池可以设计的非常简单。但存在许多简单线程池无法满足需求的情况，在这些情况下可能会引发一些问题，比如死锁。

等待提交给线程池的任务

在提交任务时，可以让提交函数返回一个任务句柄，利用这个句柄等待任务结束。这个任务句柄包装了条件变量或者其他简化使用线程池的代码。比如使用std::packaged_task提交任务，并返回一个std::future来等保存任务的返回值和允许调用者等待任务结束。

等待其他任务的任务

线程池的线程数量有限，如果线程上的任务都在等待一个未被分配到线程的任务，则会发生死锁。为了解决这种情况，可以让线程在等待某个未完成任务时，重新取一个待处理的任务来解决这个问题。这个工作可以交给线程是来实现，即提供类似于run_pending_task这样的接口。

避免工作队列上的竞争

当所有工作线程共享同一个工作队列时，高并发情况下工作队列的竞争会越来越多。即使使用无锁队列，虽然没有显示的等待，但是乒乓缓存会非常耗时。避免乒乓缓存的一个方法是在每一个线程都是用一个单独的工作队列，每个线程将新的任务添加到它自己的队列中，只有当自己的队列为空时，才从全局的工作队列取任务。可以使用thread_local变量来保证每个线程有一个自己的工作队列再加上一个全局的工作队列。这样能够很好的降低对全局工作队列的竞争，但是当任务分布不均匀时，可能导致一些线程的私有队列中有大量的任务，另外一些则没有任务处理。解决的办法是允许线程在自己的私有队列以及全局队列中都没有任务时，从其他线程的队列中窃取工作。

中断线程

没看明白，boost::thread提供了相关的api。

## 并行算法

三个标注执行策略：

>- std::execution::sequenced_policy
>- std::execution::parallel_policy
>- std::execution::parallel_unsequenced_policy

这些类都定义在execution头文件中。这个头文件中也定义了三个相关的策略对象可以传递到算法中：std::execution::seq std::execution::par std::execution::par_unseq。除了复制这三种类型的对象外，不能以自己的方式对执行策略对象进行构造，因为它们可能有一些特殊的初始化要求。实现还会定义一些其他的执行策略，但开发者不能自定义执行策略。

将执行策略传递给标准算法库中的算法，那么算法的行为就由执行策略控制。这会有几方面的影响：

>- 算法复杂化 除了对并行的管理调度开销外，并行算法的核心操作将会被多次执行。复杂度的变化根据每个算法不同会有所变化，取决于库实现和平台，而不是传递给算法的数据。
>- 抛出异常时的行为 具有执行策略的算法在执行期间触发异常，则结果由执行策略确定。如果有异常未被捕获，那么标准执行策略都会调用std::terminate。如果标准库无法提供给内部操作足够的资源，则在无执行策略算法执行时，会触发std::bad_alloc异常。
>- 算法执行的位置、方式和时间 相应执行策略指定使用那些代理来执行算法，无论这些代理是“普通”线程、向量流、GPU线程，还是其他的什么。执行策略还将对算法步骤进行执行时的约束和安排：是否以特定的顺序运行，算法步骤之间是否可以交错，或彼此并行运行等。

有执行策略和没有执行策略的函数列表间有一个重要的区别，这只会影响到一些算法：如果“普通”算法允许输入迭代器或输出迭代器，那执行策略的重载则需要前向迭代器。因为输入迭代器是单向迭代的：只能访问当前元素，并且不能将迭代器存储到以前的元素。输出迭代器只允许写入当前元素：不能在写入后面的元素后，后退再写入前面的元素。

C++标准库定义了五类迭代器：输入迭代器、输出迭代器、正向迭代器、双向迭代器和随机访问迭代器。

>- 输入迭代器是用于检索值的单向迭代器。通常用于控制台或网络的输入，或生成序列。该迭代器的任何副本都是无效的。
>- 输出迭代器是用于向单向迭代器写入值。通常输出到文件或向容器添加值。该迭代器会使该迭代器的任何副本失效。
>- 前向迭代器是通过数据不变进行单向迭代的多路径迭代器。虽然迭代器不能返回到前一个元素，但是可以存储前面元素的副本，并使用它们引用。前向迭代器返回对元素的实际引用，因此可以用于读写（如果目标是不是常量）。
>- 双向迭代器是像前向迭代器一样的多路径迭代器，但是它也可以后向访问之前的元素。
>- 随机访问迭代器是可以像双向迭代器一样前进和后退的多路径迭代器，是它们比单个元素大的跨距前进和后退，并且可以使用数组索引运算符，在偏移位置直接访问元素。

```cpp
// 输入迭代器 只能前向读取的迭代器，支持++，不能--
#include <iostream>
#include <iterator>

using namespace std;

int main() {
    istream_iterator<int> it(cin);
    istream_iterator<int> eof;
    while (it != eof) {
        cout << *it << endl;
        ++it;
    }
    cout << "over" << endl;
    return 0;
}

// 输出迭代器 只能前向写入的迭代器，支持++，不能--
#include <iostream>
#include <iterator>

using namespace std;

int main() {
    ostream_iterator<int> it(cout);
    for (int i = 0; i < 5; i++) {
        *it = i;
        it++;
    }
    cout << endl;
}
```

### std::execution::sequenced_policy

顺序策略并不是并行策略：它使用强制的方式实现，在执行线程上函数的所有操作。但它仍然是一个执行策略，因此对算法的复杂性和异常影响与其他标准执行策略相同。以该执行策略类型为一种独有类型，对并行算法重载消歧义，并要求并行算法的执行可以不并行化。以此策略调用（通常以 std::execution::seq 指定）的并行算法中，元素访问函数的调用在调用方线程中是非确定顺序的。

```cpp
// not work
std::vector<int> v(1000);
int count=0;
std::for_each(std::execution::seq,v.begin(),v.end(),
  [&](int& x){ x=++count; });
```

### std::execution::parallel_policy

并行策略提供了在多个线程下运行的算法版本。以该执行策略类型为一种独有类型，对并行算法重载消歧义，并指示并行算法的执行可以并行化。以此策略调用（通常以 std::execution::par 指定）的并行算法中，元素访问函数的调用允许在调用方线程，或由库隐式创建的线程中执行，以支持并行算法执行。任何执行于同一线程中的这种调用彼此间是非确定顺序的。

这就对算法所使用的迭代器、相关值和可调用对象有了额外的要求：想要并行调用，他们间就不能有数据竞争，也不能依赖于线程上运行的其他操作，或者依赖的操作不能在同一线程上。

```cpp
// not work
std::vector<int> v(1000);
std::atomic<int> count=0;
std::for_each(std::execution::seq,v.begin(),v.end(),
  [&](int& x){ x=++count; });
```

### std::execution::parallel_unsequenced_policy

并行不排序策略提供了最大程度的并行化算法，用以得到对算法使用的迭代器、相关值和可调用对象的严格要求。使用并行不排序策略调用的算法，可以在任意线程上执行，这些线程彼此间没有顺序。以该执行策略类型为一种独有类型，对并行算法重载消歧义，并指示并行算法的执行可以并行化、向量化，或在线程间迁移（例如用亲窃取的调度器）。容许以此策略调用的并行算法中的元素访问函数调用在未指定线程中以无序方式执行，并相对于每个线程中的另一调用无顺序。

使用并行不排序策略时，算法使用的迭代器、相关值和可调用对象不能使用任何形式的同步，也不能调用任何需要同步的函数。也就是，必须对相关的元素或基于该元素可以访问的数据进行操作，并且不能修改线程之间或元素之前共享的状态。

这意味着不能使用互斥量或原子变量，或前面章节中描述的任何其他同步机制，以确保多线程的访问是安全的；相反，必须依赖于算法本身，而不是使用多个线程访问同一个元素，并在调用并行算法之外使用外部同步，从而避免其他线程访问数据。

### others

#### detach()

std::thread::detach just allows the wrapper to be destroyed without throwing an exception; the actual thread's stack and OS resource reclamation happens in the thread's execution context after your thread's main function exits, and that should be 100% automatic.

detach()一个线程后，改线程抛出异常不会导致主线程退出；但是主线程退出依然后使得detach()后的线程自动销毁回收。所以应当尽量使用join()来保证程序运行始终在预期的状态。

### tips

线程是宝贵的，一个程序可以使用几个或者十几个线程，但是一台机器上不应同时运行几百个、几千个用户线程，这会增加内核调度器的负担，降低整体性能。

线程的创建和销毁是有代价的，一个程序最好在一开始创建所需的线程，并一直反复使用。不要在运行期间反复创建和销毁线程，如果有必要这么做，也要尽量降低频率（一分钟一次或者更低）。

每个线程应该有明确的职责，例如IO线程、计算线程等。

线程之间的交互应该尽量简单，理想情况下线程之间只用消息传递（例如阻塞队列）方式交互。如果使用锁，那么最好避免一个线程持有超过一把的锁，这样可以彻底防止死锁。

要预先考虑清除一个mutable shared对象将会暴露给哪些线程，每个线程是读还是写，读写有无可能并发进行。
