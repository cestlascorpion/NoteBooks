# kubenetes

## 概述

K8S 要做的事情：自动化运维管理 Docker（容器化）程序。

![k8s](../Resource/K8S-Overview.jpg)

K8S: 控制平面（ETCD、API Server、Scheduler、Controller） + 工作节点 （Kubelet、kube-proxy、container runtime）+ 附加组件（dns、dashboard、ingress、headpster、etc）。系统组件之间只能通过 API Server 通信。API Server 适合 ETCD 通信的唯一组件，CLI 交互也是在与 API Server 通信。

控制平面的组件可以简单的分布在多台服务器上，还可以有多个实例保证高可用。ETCD 和 API Server 可以多个实例同时工作，Scheduler Controller 只能有一个运行。控制平面组件和 kube-proxy 可以直接部署在系统上或者作为 pod 运行，Kubelet 是唯一一直作为常规系统组件运行的组件，它把其他组件都作为 pod 运行。为了将控制平面作为 pod 运行，Kubelet 被部署在 Master 节点。

![k8s](../Resource/K8S-Overview2.jpg)

K8S 是属于主从设备模型（Master-Slave 架构），即有 Master 节点负责核心的调度、管理和运维，Slave 节点则在执行用户的程序。但是在 K8S 中，主节点一般被称为 Master Node 或者 Head Node（本文采用 Master Node 称呼方式），而从节点则被称为 Worker Node 或者 Node（本文采用 Worker Node 称呼方式）。

要注意一点：Master Node 和 Worker Node 是分别安装了 K8S 的 Master 和 Woker 组件的实体服务器，每个 Node 都对应了一台实体服务器（虽然 Master Node 可以和其中一个 Worker Node 安装在同一台服务器，但是建议 Master Node 单独部署），所有 Master Node 和 Worker Node 组成了 K8S 集群，同一个集群可能存在多个 Master Node 和 Worker Node。

首先来看 Master Node 都有哪些组件：

>- API Server。K8S 的请求入口服务。API Server 负责接收 K8S 所有请求（来自 UI 界面或者 CLI 命令行工具），然后，API Server 根据用户的具体请求，去通知其他组件干活。
>- Scheduler。K8S 所有 Worker Node 的调度器。当用户要部署服务时，Scheduler 会选择最合适的 Worker Node（服务器）来部署。
Controller Manager。K8S 所有 Worker Node 的监控器。Controller Manager 有很多具体的 Controller，在文章 Components of Kubernetes Architecture 中提到的有 Node Controller、Service Controller、Volume Controller 等。Controller 负责监控和调整在 Worker Node 上部署的服务的状态，比如用户要求 A 服务部署 2 个副本，那么当其中一个服务挂了的时候，Controller 会马上调整，让 Scheduler 再选择一个 Worker Node 重新部署服务。
>- etcd。K8S 的存储服务。etcd 存储了 K8S 的关键配置和用户配置，K8S 中仅 API Server 才具备读写权限，其他组件必须通过 API Server 的接口才能读写数据（见 Kubernetes Works Like an Operating System）。

接着来看 Worker Node 的组件，笔者更赞同 HOW DO APPLICATIONS RUN ON KUBERNETES 文章中提到的组件介绍：

>- Kubelet。Worker Node 的监视器，以及与 Master Node 的通讯器。Kubelet 是 Master Node 安插在 Worker Node 上的 “眼线”，它会定期向 Worker Node 汇报自己 Node 上运行的服务的状态，并接受来自 Master Node 的指示采取调整措施。
Kube-Proxy。K8S 的网络代理。私以为称呼为 Network-Proxy 可能更适合？Kube-Proxy 负责 Node 在 K8S 的网络通讯、以及对外部网络流量的负载均衡。
>- Container Runtime。Worker Node 的运行环境。即安装了容器化所需的软件环境确保容器化程序能够跑起来，比如 Docker Engine。大白话就是帮忙装好了 Docker 运行环境。
>- Logging Layer。K8S 的监控状态收集器。私以为称呼为 Monitor 可能更合适？Logging Layer 负责采集 Node 上所有服务的 CPU、内存、磁盘、网络等监控项信息。
>- Add-Ons。K8S 管理运维 Worker Node 的插件组件。有些文章认为 Worker Node 只有三大组件，不包含 Add-On，但笔者认为 K8S 系统提供了 Add-On 机制，让用户可以扩展更多定制化功能，是很不错的亮点。

总结来看，K8S 的 Master Node 具备：请求入口管理（API Server），Worker Node 调度（Scheduler），监控和自动调节（Controller Manager），以及存储功能（etcd）；而 K8S 的 Worker Node 具备：状态和监控收集（Kubelet），网络和负载均衡（Kube-Proxy）、保障容器化运行环境（Container Runtime）、以及定制化功能（Add-Ons）。

## kubectl

Kubectl 是一个命令行接口，用于对 Kubernetes 集群运行命令。Kubectl 的配置文件在$HOME/.kube 目录。我们可以通过设置 KUBECONFIG 环境变量或设置命令参数--kubeconfig 来指定其他位置的 kubeconfig 文件。基本语法 kubectl [command] [TYPE] [NAME] [flags]

`command`：指定要对一个或多个资源执行的操作，例如 create、get、describe、delete。

`TYPE`：指定资源类型。资源类型不区分大小写，可以指定单数、复数或缩写形式。

`NAME`：指定资源的名称。名称区分大小写。如果省略名称，则显示所有资源的详细信息。

kubectl 是 K8S 的命令行工具，并不需要 kubectl 安装在 K8S 集群的任何 Node 上，但是，需要确保安装 kubectl 的机器和 K8S 的集群能够进行网络互通。kubectl 是通过本地的配置文件来连接到 K8S 集群的，默认保存在$HOME/.kube 目录下；也可以通过 KUBECONFIG 环境变量或设置命令参数--kubeconfig 来指定其他位置的 kubeconfig 文件。如果 --kubeconfig 和 KUBECONFIG 变量都没有， 则会读取 $HOME/.kube/config 文件。但凡某一步找到了有效的 cluster，就中断检查，去连接 K8S 集群了。

## 安装运行

### minikube

[minikube](https://minikube.sigs.k8s.io/docs/start/)

```shell
# 启动集群
minikube start --cache-images=true --cpus=4 --memory=8192m --image-mirror-country=cn --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers

# 网页查看
minikube dashboard
```

### kubeadm

```shell
# install docker
sudo apt-get update
sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common
curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable"

sudo apt-get -y update
sudo apt-get -y install docker-ce

# 安装指定版本的Docker-CE:
# Step 1: 查找Docker-CE的版本:
# sudo apt-cache madison docker-ce
#   docker-ce | 17.03.1~ce-0~ubuntu-xenial | https://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages
#   docker-ce | 17.03.0~ce-0~ubuntu-xenial | https://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages
# Step 2: 安装指定版本的Docker-CE: (VERSION例如上面的17.03.1~ce-0~ubuntu-xenial)
# sudo apt-get -y install docker-ce=[VERSION]

# /etc/docker/daemon.json 配置 cgroupdriver
{
    "exec-opts": [
        "native.cgroupdriver=systemd"
    ]
}

# 安装 kubelet kubeadm kubectl
sudo apt-get update && apt-get install -y apt-transport-https
curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add - 

sudo vim /etc/apt/sources.list.d/kubernetes.list 
# deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main

sudo apt-get update && sudo apt-get install -y kubelet kubeadm kubectl

# 关闭swap分区
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab # 永久 需要重启

# 关闭防火墙
... # 貌似默认未开启

# 配置主机名
sudo hostnamectl set-hostname k8s-master（master主机打命令）
sudo hostnamectl set-hostname k8s-node01（node1主机打命令）
sudo hostnamectl set-hostname k8s-node02 (node2主机打命令)
...
# 写入 hosts
sudo vim /etc/hosts 
192.168.4.34 k8s-master
192.168.4.35 k8s-node01
192.168.4.36 k8s-node02
...

# 配置网桥 
sudo vim /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1

sudo sysctl --system

# master节点 pod-network-cidr不能和主机地址冲突 - 很烦这个
kubeadm init \
--apiserver-advertise-address=192.168.73.138 \
--image-repository registry.aliyuncs.com/google_containers \
--pod-network-cidr=10.244.0.0/16

# 有Error解决，没有的话按照提示拷贝配置文件和添加节点的命令，在node节点上运行

# 坑爹问题：/etc/resolve.conf中存在 nameserver 127.0.0.53回环地址造成循环引用 手动修改无果，每次重启依旧会覆盖
sudo vim /etc/resolv.conf

nameserver 114.114.114.114 # 不能直接删 还要加上可用的ns
nameserver 8.8.8.8

sudo apt install unbound
sudo vim  /etc/NetworkManager/NetworkManager.conf

[main]
dns=unbound

sudo systemctl stop systemd-resolved
sudo systemctl disable systemd-resolved
sudo reboot
```

### 查看集群

```shell
# 集群信息
$ kubectl cluster-info
Kubernetes master is running at https://127.0.0.1:49153
KubeDNS is running at https://127.0.0.1:49153/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

# 查看集群节点
$ kubectl get nodes
NAME       STATUS   ROLES                  AGE   VERSION
minikube   Ready    control-plane,master   17h   v1.20.0

# 查看集群健康狀態
$ kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok                  
controller-manager   Healthy   ok                  
etcd-0               Healthy   {"health":"true"}   

# 查看資源
$ kubectl api-resources
NAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND
bindings                                       v1                                     true         Binding
componentstatuses                 cs           v1                                     false        ComponentStatus
configmaps                        cm           v1                                     true         ConfigMap
endpoints                         ep           v1                                     true         Endpoints
events                            ev           v1                                     true         Event
limitranges                       limits       v1                                     true         LimitRange
namespaces                        ns           v1                                     false        Namespace
nodes                             no           v1                                     false        Node
persistentvolumeclaims            pvc          v1                                     true         PersistentVolumeClaim
persistentvolumes                 pv           v1                                     false        PersistentVolume
pods                              po           v1                                     true         Pod
podtemplat es                                   v1                                     true         PodTemplate
replicationcontrollers            rc           v1                                     true         ReplicationController
resourcequotas                    quota        v1                                     true         ResourceQuota
secrets                                        v1                                     true         Secret
serviceaccounts                   sa           v1                                     true         ServiceAccount
services                          svc          v1                                     true         Service
mutatingwebhookconfigurations                  admissionregistration.k8s.io/v1        false        MutatingWebhookConfiguration
validatingwebhookconfigurations                admissionregistration.k8s.io/v1        false        ValidatingWebhookConfiguration
customresourcedefinitions         crd,crds     apiextensions.k8s.io/v1                false        CustomResourceDefinition
apiservices                                    apiregistration.k8s.io/v1              false        APIService
controllerrevisions                            apps/v1                                true         ControllerRevision
daemonsets                        ds           apps/v1                                true         DaemonSet
deployments                       deploy       apps/v1                                true         Deployment
replicasets                       rs           apps/v1                                true         ReplicaSet
statefulsets                      sts          apps/v1                                true         StatefulSet
tokenreviews                                   authentication.k8s.io/v1               false        TokenReview
localsubjectaccessreviews                      authorization.k8s.io/v1                true         LocalSubjectAccessReview
selfsubjectaccessreviews                       authorization.k8s.io/v1                false        SelfSubjectAccessReview
selfsubjectrulesreviews                        authorization.k8s.io/v1                false        SelfSubjectRulesReview
subjectaccessreviews                           authorization.k8s.io/v1                false        SubjectAccessReview
horizontalpodautoscalers          hpa          autoscaling/v1                         true         HorizontalPodAutoscaler
cronjobs                          cj           batch/v1beta1                          true         CronJob
jobs                                           batch/v1                               true         Job
certificatesigningrequests        csr          certificates.k8s.io/v1                 false        CertificateSigningRequest
leases                                         coordination.k8s.io/v1                 true         Lease
bgpconfigurations                              crd.projectcalico.org/v1               false        BGPConfiguration
bgppeers                                       crd.projectcalico.org/v1               false        BGPPeer
blockaffinities                                crd.projectcalico.org/v1               false        BlockAffinity
clusterinformations                            crd.projectcalico.org/v1               false        ClusterInformation
felixconfigurations                            crd.projectcalico.org/v1               false        FelixConfiguration
globalnetworkpolicies             gnp          crd.projectcalico.org/v1               false        GlobalNetworkPolicy
globalnetworksets                              crd.projectcalico.org/v1               false        GlobalNetworkSet
hostendpoints                                  crd.projectcalico.org/v1               false        HostEndpoint
ipamblocks                                     crd.projectcalico.org/v1               false        IPAMBlock
ipamconfigs                                    crd.projectcalico.org/v1               false        IPAMConfig
ipamhandles                                    crd.projectcalico.org/v1               false        IPAMHandle
ippools                                        crd.projectcalico.org/v1               false        IPPool
kubecontrollersconfigurations                  crd.projectcalico.org/v1               false        KubeControllersConfiguration
networkpolicies                                crd.projectcalico.org/v1               true         NetworkPolicy
networksets                                    crd.projectcalico.org/v1               true         NetworkSet
endpointslices                                 discovery.k8s.io/v1beta1               true         EndpointSlice
events                            ev           events.k8s.io/v1                       true         Event
ingresses                         ing          extensions/v1beta1                     true         Ingress
flowschemas                                    flowcontrol.apiserver.k8s.io/v1beta1   false        FlowSchema
prioritylevelconfigurations                    flowcontrol.apiserver.k8s.io/v1beta1   false        PriorityLevelConfiguration
ingressclasses                                 networking.k8s.io/v1                   false        IngressClass
ingresses                         ing          networking.k8s.io/v1                   true         Ingress
networkpolicies                   netpol       networking.k8s.io/v1                   true         NetworkPolicy
runtimeclasses                                 node.k8s.io/v1                         false        RuntimeClass
poddisruptionbudgets              pdb          policy/v1beta1                         true         PodDisruptionBudget
podsecuritypolicies               psp          policy/v1beta1                         false        PodSecurityPolicy
clusterrolebindings                            rbac.authorization.k8s.io/v1           false        ClusterRoleBinding
clusterroles                                   rbac.authorization.k8s.io/v1           false        ClusterRole
rolebindings                                   rbac.authorization.k8s.io/v1           true         RoleBinding
roles                                          rbac.authorization.k8s.io/v1           true         Role
priorityclasses                   pc           scheduling.k8s.io/v1                   false        PriorityClass
csidrivers                                     storage.k8s.io/v1                      false        CSIDriver
csinodes                                       storage.k8s.io/v1                      false        CSINode
storageclasses                    sc           storage.k8s.io/v1                      false        StorageClass
volumeattachments                              storage.k8s.io/v1                      false        VolumeAttachment
```

添加自動補全。

```shell
# Installing bash completion on Linux
## If bash-completion is not installed on Linux, please install the 'bash-completion' package
## via your distribution's package manager.
## Load the kubectl completion code for bash into the current shell
source <(kubectl completion bash)
## Write bash completion code to a file and source if from .bash_profile
kubectl completion bash > ~/.kube/completion.bash.inc
printf "
# Kubectl shell completion
source '$HOME/.kube/completion.bash.inc'
" >> $HOME/.bash_profile
source $HOME/.bash_profile

```

## 工作上下文

kubeconfig文件由一组上下文组成。上下文包含以下三个元素：集群(cluster)：集群的API服务器的URL；用户(user)：集群的特定用户的身份验证凭据；命名空间(namespace)：连接到集群时使用的命名空间。在任何给定时间，其中一个上下文被设置为当前上下文（通过kubeconfig文件中的专用字段）。

kubectl 会优先检查--kubeconfig，若无则检查KUBECONFIG，若无则最后检查$HOME/.kube/config，如果还是没有，报错。但凡某一步找到了有效的 cluster，就中断检查，去连接 K8S 集群了。如果配置文件只有一个 cluster 是没有任何问题的，但是对于有多个 cluster 怎么办呢？

```shell
# 列出所有上下文信息
kubectl config get-contexts

# 查看当前的上下文信息
kubectl config current-context

# 更改上下文信息
kubectl config use-context ${CONTEXT_NAME}

# 修改上下文的元素。比如可以修改用户账号、集群信息、连接到 K8S 后所在的 namespace
kubectl config set-context ${CONTEXT_NAME}|--current --${KEY}=${VALUE}
```

config set-context可以修改任何在配置文件中的上下文信息，只需要在命令中指定上下文名称就可以。而--current 则指代当前上下文。
上下文信息所包括的内容有：cluster 集群（名称）、用户账号（名称）、连接到 K8S 后所在的 namespace，因此有config set-context严格意义上的用法：kubectl config set-context [NAME|--current] [--cluster=cluster_nickname] [--user=user_nickname] [--namespace=namespace] [options]（备注：[options]可以通过 kubectl options 查看）

或者使用[Kubectx](https://github.com/ahmetb/kubectx/#installation)快速切换上下文。

## 查看资源

```shell
# Show details of a specific resource or group of resources
#  Print a detailed description of the selected resources, including related resources such as events or controllers. You may select a single object by name, all objects of that type, provide a name prefix, or label selector.
kubectl describe (-f FILENAME | TYPE [NAME_PREFIX | -l label] | TYPE/NAME) [options]

# Describe a node
kubectl describe nodes kubernetes-node-emt8.c.myproject.internal

# Describe a pod
kubectl describe pods/nginx

# Describe a pod identified by type and name in "pod.json"
kubectl describe -f pod.json
# 该文件即为创建 pod 使用的描述文件
kubectl describe -f kubia-manual.yaml 

# Describe all pods
kubectl describe pods

# Describe pods by label name=myLabel
kubectl describe po -l name=myLabel
kubectl describe po -l name=kubia-manual

# Describe all pods managed by the 'frontend' replication controller (rc-created pods
# get the name of the rc as a prefix in the pod the name).
kubectl describe pods frontend

# Display one or many resources
# Prints a table of the most important information about the specified resources. You can filter the list using a label selector and the --selector flag. If the desired resource type is namespaced you will only see results in your current namespace unless you pass --all-namespaces.
kubectl get [(-o|--output=)json|yaml|wide|custom-columns=...|custom-columns-file=...|go-template=...|go-template-file=...|jsonpath=...|jsonpath-file=...] (TYPE[.VERSION][.GROUP] [NAME | -l label] | TYPE[.VERSION][.GROUP]/NAME ...) [flags] [options]

# List all pods in ps output format.
kubectl get pods
kubectl get pods --watch

# List all pods in ps output format with more information (such as node name).
kubectl get pods -o wide

# List a single replication controller with specified NAME in ps output format.
kubectl get replicationcontroller web

# List deployments in JSON output format, in the "v1" version of the "apps" API group:
kubectl get deployments.v1.apps -o json

# List a single pod in JSON output format.
kubectl get -o json pod web-pod-13je7
# yaml 格式也可以
kubectl get -o yaml pod kubia-manual

# List a pod identified by type and name specified in "pod.yaml" in JSON output format.
kubectl get -f pod.yaml -o json

# List resources from a directory with kustomization.yaml - e.g. dir/kustomization.yaml.
kubectl get -k dir/

# Return only the phase value of the specified pod.
kubectl get -o template pod/web-pod-13je7 --template={{.status.phase}}

# List resource information in custom columns.
kubectl get pod test-pod -o custom-columns=CONTAINER:.spec.containers[0].name,IMAGE:.spec.containers[0].image

# List all replication controllers and services together in ps output format.
kubectl get rc,services

# List one or more resources by their type and names.
kubectl get rc/web service/frontend pods/web-pod-13je7
```

## 创建资源

```shell
# Create a resource from a file or from stdin.
kubectl create -f FILENAME [options]

# Create a pod using the data in pod.json.
kubectl create -f ./pod.json

# Create a pod based on the JSON passed into stdin.
cat pod.json | kubectl create -f -

# Edit the data in docker-registry.yaml in JSON then create the resource using the edited data.
kubectl create -f docker-registry.yaml --edit -o json
```

## 删除资源

在删除pod的过程中，k8s向进程发送SIGTERM并等待一定时间（默认30s）等待其正常关闭；否则发送SIGKILL终止该进程。删除namespace会删除下辖的所有pod。删除ReplicationController创建的pod会导致rc重建新的pod，此时要删除rc。

```shell
# Delete resources by filenames, stdin, resources and names, or by resources and label selector.
# JSON and YAML formats are accepted. Only one type of the arguments may be specified: filenames, resources and names, or resources and label selector.
kubectl delete ([-f FILENAME] | [-k DIRECTORY] | TYPE [(NAME | -l label | --all)]) [options]

# Delete a pod using the type and name specified in pod.json.
kubectl delete -f ./pod.json

# Delete resources from a directory containing kustomization.yaml - e.g. dir/kustomization.yaml.
kubectl delete -k dir

# Delete a pod based on the type and name in the JSON passed into stdin.
cat pod.json | kubectl delete -f -

# Delete pods and services with same names "baz" and "foo"
kubectl delete pod,service baz foo

# Delete pods and services with label name=myLabel.
kubectl delete pods,services -l name=myLabel

# Delete a pod with minimal delay
kubectl delete pod foo --now

# Force delete a pod on a dead node
kubectl delete pod foo --force

# Delete all pods
kubectl delete pods --all
```

## 标记资源

标签是键值对。如果想添加更复杂的标记，使用注释。

```shell
# Update the labels on a resource.
kubectl label [--overwrite] (-f FILENAME | TYPE NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--resource-version=version] [options]

# Update pod 'foo' with the label 'unhealthy' and the value 'true'.
kubectl label pods foo unhealthy=true

# Update pod 'foo' with the label 'status' and the value 'unhealthy', overwriting any existing value.
kubectl label --overwrite pods foo status=unhealthy

# Update all pods in the namespace
kubectl label pods --all status=unhealthy

# Update a pod identified by the type and name in "pod.json"
kubectl label -f pod.json status=unhealthy

# Update pod 'foo' only if the resource is unchanged from version 1.
kubectl label pods foo status=unhealthy --resource-version=1

# Update pod 'foo' by removing a label named 'bar' if it exists.
# Does not require the --overwrite flag.
kubectl label pods foo bar-
```

```shell
# 列出所有 pod 的指定标签 没有的话为空
kubectl get pods -L <label-name>
```

## 查看日志

pod删除后这些日志便不可查看

```shell
# Print the logs for a container in a pod or specified resource. If the pod has only one container, the container name is optional.

# Return snapshot logs from pod nginx with only one container
kubectl logs nginx

# Return snapshot logs from pod nginx with multi containers
kubectl logs nginx --all-containers=true

# Return snapshot logs from all containers in pods defined by label app=nginx
kubectl logs -lapp=nginx --all-containers=true

# Return snapshot of previous terminated ruby container logs from pod web-1
# -c 制定了容器名称 web-1 是 pod 名称 一个 pod 中可能运行多个容器
# -p 上一个停止的容器
kubectl logs -p -c ruby web-1

# Begin streaming the logs of the ruby container in pod web-1
# 类似与 tail -f
kubectl logs -f -c ruby web-1

# Begin streaming the logs from all containers in pods defined by label app=nginx
kubectl logs -f -lapp=nginx --all-containers=true

# Display only the most recent 20 lines of output in pod nginx
kubectl logs --tail=20 nginx

# Show all logs from pod nginx written in the last hour
kubectl logs --since=1h nginx

# Show logs from a kubelet with an expired serving certificate
kubectl logs --insecure-skip-tls-verify-backend nginx

# Return snapshot logs from first container of a job named hello
kubectl logs job/hello

# Return snapshot logs from container nginx-1 of a deployment named nginx
kubectl logs deployment/nginx -c nginx-1
```

```shell
# Requires that the 'tar' binary is present in your container
# image.  If 'tar' is not present, 'kubectl cp' will fail.
kubectl cp <file-spec-src> <file-spec-dest> [options]

# Copy /tmp/foo local file to /tmp/bar in a remote pod in namespace <some-namespace>
tar cf - /tmp/foo | kubectl exec -i -n <some-namespace> <some-pod> -- tar xf - -C /tmp/bar

# Copy /tmp/foo from a remote pod to /tmp/bar locally
kubectl exec -n <some-namespace> <some-pod> -- tar cf - /tmp/foo | tar xf - -C /tmp/bar

# Copy /tmp/foo_dir local directory to /tmp/bar_dir in a remote pod in the default namespace
kubectl cp /tmp/foo_dir <some-pod>:/tmp/bar_dir

# Copy /tmp/foo local file to /tmp/bar in a remote pod in a specific container
kubectl cp /tmp/foo <some-pod>:/tmp/bar -c <specific-container>

# Copy /tmp/foo local file to /tmp/bar in a remote pod in namespace <some-namespace>
kubectl cp /tmp/foo <some-namespace>/<some-pod>:/tmp/bar

# Copy /tmp/foo from a remote pod to /tmp/bar locally
kubectl cp <some-namespace>/<some-pod>:/tmp/foo /tmp/bar
```

## 端口转发

一般用于调试

```shell
# Forward one or more local ports to a pod. This command requires the node to have 'socat' installed.
# Use resource type/name such as deployment/mydeployment to select a pod. Resource type defaults to 'pod' if omitted.
kubectl port-forward TYPE/NAME [options] [LOCAL_PORT:]REMOTE_PORT [...[LOCAL_PORT_N:]REMOTE_PORT_N]

# Listen on ports 5000 and 6000 locally, forwarding data to/from ports 5000 and 6000 in the pod
kubectl port-forward pod/mypod 5000 6000

# Listen on ports 5000 and 6000 locally, forwarding data to/from ports 5000 and 6000 in a pod selected by the deployment
kubectl port-forward deployment/mydeployment 5000 6000

# Listen on ports 5000 and 6000 locally, forwarding data to/from ports 5000 and 6000 in a pod selected by the service
kubectl port-forward service/myservice 5000 6000

# Listen on port 8888 locally, forwarding to 5000 in the pod
kubectl port-forward pod/mypod 8888:5000

# Listen on port 8888 on all addresses, forwarding to 5000 in the pod
kubectl port-forward --address 0.0.0.0 pod/mypod 8888:5000

# Listen on port 8888 on localhost and selected IP, forwarding to 5000 in the pod
kubectl port-forward --address localhost,10.19.21.23 pod/mypod 8888:5000

# Listen on a random port locally, forwarding to 5000 in the pod
kubectl port-forward pod/mypod :5000
```

## 基本资源

### PO K8S管理调度的基本单位

pod 中的所有容器共享同一个网络和Linux命名空间。pod 中除了用户定义的容器外有一个**基础容器**，基础容器的生命周期和 pod 绑定，其他容器使用基础容器的命名空间，以此保证容器在重启后命名空间不变。

使用描述文件创建资源。

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: kubia-manual
  labels:
    name: kubia-manual
    env: testing
spec:
  containers:
  - name: kubia-container
    image: luksa/kubia
    resources:
      limits:
        memory: "128Mi"
        cpu: "500m"
    ports:
      - containerPort: 8080
        protocol: TCP
```

标签 labels:name: kubia-manual env: testing。配合标签选择器使用。

```shell
kubectl get pods -l env=testing

$ kubectl get pods -l env=testing -o wide
NAME           READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES
kubia-manual   1/1     Running   0          96m   172.17.0.3   minikube   <none>           <none>

# 列出包含env标签的所有pod 反向选择使用 '!env' 单引号不可省略
# 同理可以用 env!=tesing / env in (prod,tesing) / env notin (prod,tesing) 等 多个条件使用逗号隔开
kubectl get pods -l env
```

pod 死亡意味着每个容器将被关闭，容器关闭方式为：执行停止前钩子-向容器主进程发送SIGTERM信号-等待容器优雅关闭或者等待终止宽限期超时（默认30s）-没有优雅关闭则发送SIGKILL强制终止进程。终止宽限期从执行停止前钩子时开始计时。

```shell
# 终止宽限期设置为5s
kubectl delete pod mypod --grace-period=5
# 立即删除 （危险，可能会出现相同pod的两个实例在同一时间运行）
kubectl delete pod mypod --grace-periid=0 --force
```

#### 存活探针

检查容器是否在运行。三种：HTTP GET探针，对容器的IP地址执行HTTP请求，如果响应正常（2xx/3xx）则探测成功。TCP Socket探针，尝试与容器建立连接。Exec探针，在容器内执行命令，检查命令退出状态为0则探测成功。检测失败则会重启pod（删除并创建新的）。

存活探针应当检查应用程序的内部，并保持轻量。

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: kubia-liveness
  labels:
    app: kubia
spec:
  containers:
  - name: kubia
    image: luksa/kubia-unhealthy
    livenessProbe:
        httpGet:
          path: /
          port: 8080
        # 务必设置初始时延 否则容器没创建好就探测 通常不会成功
        initialDelaySeconds: 15
        failureThreshold: 5
    resources:
      limits:
        memory: "128Mi"
        cpu: "500m"
    ports:
      - containerPort: 8080
```

#### 就绪探针

就绪探针会定期调用（默认10s），确定特定的pod是否就绪，准备好接受请求。同样也有三种。如果pod报告其尚未就绪，则会从服务中删除该pod（比如从Endpoint对象中删除），但是不会将其终止或重启。

应当始终定义一个就绪探针。不要将停止pod的逻辑加入就绪探针。当然容器关闭时，就需要让就绪探针失败，这是理所当然但并非必需的，因为只要删除pod，k8s就会从所有服务中移除pod。

```yaml
kind: ReplicaSet
apiVersion: apps/v1
metadata:
  name: kubia-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      name: kubia-pod
      labels:
        app: kubia
    spec:
      containers:
        - name: kubia-container
          image: luksa/kubia
          readinessProbe:
              # 通过检查一个文件是否创建来完成探测
              exec:
                command:
                  - ls
                  - /var/ready
          ports:
            - containerPort: 8080
```

```shell
kubectl get pods
NAME             READY   STATUS              RESTARTS   AGE
kubia-rs-l94c5   0/1     ContainerCreating   0          2m51s
kubia-rs-t5jjb   0/1     ContainerCreating   0          2m51s
kubia-rs-tn8kn   0/1     ContainerCreating   0          2m51s

kubectl describe pods kubia-rs-l94c5
Name:         kubia-rs-l94c5
Namespace:    default
Priority:     0
Node:         minikube/192.168.49.2
Start Time:   Mon, 04 Jan 2021 16:58:00 +0800
Labels:       app=kubia
Annotations:  <none>
Status:       Running
IP:           172.17.0.8
IPs:
  IP:           172.17.0.8
Controlled By:  ReplicaSet/kubia-rs
Containers:
  kubia-container:
    Container ID:   docker://e0ff732c683639051a6527bf7da433089dbd8f0910077565190d79ce8071d607
    Image:          luksa/kubia
    Image ID:       docker-pullable://luksa/kubia@sha256:a923320941524f2bb6a3a40bfdaf0acf5da9b3c91748c8dea56514b7bea53e8a
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Mon, 04 Jan 2021 17:01:05 +0800
    Ready:          False
    Restart Count:  0
    Readiness:      exec [ls /var/ready] delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-pbzvp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  default-token-pbzvp:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-pbzvp
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age    From               Message
  ----     ------     ----   ----               -------
  Normal   Scheduled  3m13s  default-scheduler  Successfully assigned default/kubia-rs-l94c5 to minikube
  Normal   Pulling    3m8s   kubelet            Pulling image "luksa/kubia"
  Normal   Pulled     8s     kubelet            Successfully pulled image "luksa/kubia" in 2m59.018847561s
  Normal   Created    8s     kubelet            Created container kubia-container
  Normal   Started    8s     kubelet            Started container kubia-container
  Warning  Unhealthy  5s     kubelet            Readiness probe failed: ls: cannot access /var/ready: No such file or directory


kubectl exec kubia-rs-l94c5 -- touch /var/ready

# 创建文件后 该pod就绪
kubectl get pods                               
NAME             READY   STATUS    RESTARTS   AGE
kubia-rs-l94c5   1/1     Running   0          6m7s
kubia-rs-t5jjb   0/1     Running   0          6m7s
kubia-rs-tn8kn   0/1     Running   0          6m7s

kubectl describe po  kubia-rs-l94c5 
Name:         kubia-rs-l94c5
Namespace:    default
Priority:     0
Node:         minikube/192.168.49.2
Start Time:   Mon, 04 Jan 2021 16:58:00 +0800
Labels:       app=kubia
Annotations:  <none>
Status:       Running
IP:           172.17.0.8
IPs:
  IP:           172.17.0.8
Controlled By:  ReplicaSet/kubia-rs
Containers:
  kubia-container:
    Container ID:   docker://e0ff732c683639051a6527bf7da433089dbd8f0910077565190d79ce8071d607
    Image:          luksa/kubia
    Image ID:       docker-pullable://luksa/kubia@sha256:a923320941524f2bb6a3a40bfdaf0acf5da9b3c91748c8dea56514b7bea53e8a
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Mon, 04 Jan 2021 17:01:05 +0800
    Ready:          True
    Restart Count:  0
    Readiness:      exec [ls /var/ready] delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-pbzvp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-pbzvp:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-pbzvp
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                  From               Message
  ----     ------     ----                 ----               -------
  Normal   Scheduled  6m17s                default-scheduler  Successfully assigned default/kubia-rs-l94c5 to minikube
  Normal   Pulling    6m12s                kubelet            Pulling image "luksa/kubia"
  Normal   Pulled     3m12s                kubelet            Successfully pulled image "luksa/kubia" in 2m59.018847561s
  Normal   Created    3m12s                kubelet            Created container kubia-container
  Normal   Started    3m12s                kubelet            Started container kubia-container
  Warning  Unhealthy  89s (x11 over 3m9s)  kubelet            Readiness probe failed: ls: cannot access /var/ready: No such file or directory
```

#### init 容器

pod 包含多个容器，在启动时顺序是不可控的。init容器则可以用与初始化 pod，通常意味着向容器写入数据并将这个存储卷挂在到主容器中。init容器的数量是任意的，所有init容器启动执行完毕后才会启动主容器。

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: fortune-client
spec:
  initContainers:
  - name: init
    image: busybox
    command:
    - sh
    - -c
    - 'while true; do echo "Waiting for fortune service to come up..."; wget http://fortune -q -T 1 -O /dev/null >/dev/null 2>/dev/null && break; sleep 1; done; echo "Service is up! Starting main container."'
  containers:
  - image: busybox
    name: main
    command:
    - sh
    - -c
    - 'echo "Main container started. Reading fortune very 10 seconds."; while true; do echo "-------------"; wget -q -O - http://fortune; sleep 10; done'
```

```shell
NAME             READY   STATUS     RESTARTS   AGE
fortune-client   0/1     Init:0/1   0          14s

kubectl get pods
NAME             READY   STATUS     RESTARTS   AGE
fortune-client   0/1     Init:0/1   0          117s
fortune-server   2/2     Running    0          90s

kubectl get pods
NAME             READY   STATUS    RESTARTS   AGE
fortune-client   1/1     Running   0          2m6s
fortune-server   2/2     Running   0          99s

kubectl logs fortune-client -c init
Waiting for fortune service to come up...
Waiting for fortune service to come up...
Waiting for fortune service to come up...
Waiting for fortune service to come up...
Waiting for fortune service to come up...
Waiting for fortune service to come up...
Waiting for fortune service to come up...
Waiting for fortune service to come up...
Waiting for fortune service to come up...
Waiting for fortune service to come up...
Waiting for fortune service to come up...
Service is up! Starting main container.
```

-c 指定 pod 中的容器。

#### 容器生命周期钩子 Hook

init 容器应用于整个pod，Post-start hook/Pre-stop hook是在容器启动后/停止前执行的。

启动后钩子在容器主进程启动之后立即执行，和主进程是并行的。在钩子之习惯完毕之前，容器停留在waiting装台，其原因是containercreating。因此pod的状态会是pending而不是running，钩子运行失败或者返回了非零状态码，主容器会被杀死。

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: pod-with-poststart-hook
spec:
  containers:
  - image: luksa/kubia
    name: kubia
    lifecycle:
      postStart:
        exec:
          command: 
          - sh
          - -c
          - "echo 'hook will fail with exit code 15'; sleep 5 ; exit 15"
```

典型情况下，poststart hook也是执行container中的shell或者二进制程序。如果钩子程序启动的进程将日志输出到标准输出/标准错误终端，则无法被记录，导致了调试生命周期钩子程序非常困难。如果启动失败，只会看到FailedPostStartHook的告警信息。如果想记录日志，需要挂在一个卷来保留这些信息。

停止前钩子在容器终止前执行，仅当执行完毕后，才会向进程发送SIGTERM信号（没有优雅结束则会被SIGKILL杀死）。

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: pod-with-prestop-hook
spec:
  containers:
  - image: luksa/kubia
    name: kubia
    ports:
    - containerPort: 8080
      protocol: TCP
    lifecycle:
      preStop:
        # 默认的的host地址是pod的ip
        httpGet:
          port: 8080
          path: shutdown
```

无论钩子执行成功与否，容器都会被终止。执行失败也只会看到FailedPreStopHook告警，但不久后pod被删除了，此时就什么都看不到了。

一个常见错误做法是，容器启动了一个shell，shell启动真正的进程，这样shell会收到kubelet的SIGTERM信号，进程却接收不到，此时配置一个停止前钩子传递SIGTERM信号。合理的做法是让shell进程传递这个信号，更合理的做法是Dockerfile就应当配置ENTRYPOINT直接执行主进程（而不是借助一个shell）。

### NS 命名空间提供隔离

Kubernetes 支持多个虚拟集群，它们底层依赖于同一个物理集群。 这些虚拟集群被称为名字空间。

```yaml
kind: Namespace
apiVersion: v1
metadata:
  name: custom-namespace
```

```shell
$ kubectl create -f custom-namespace.yaml 
namespace/custom-namespace created

$ kubectl get ns
NAME                   STATUS   AGE
custom-namespace       Active   16s
default                Active   18h
kube-node-lease        Active   18h
kube-public            Active   18h
kube-system            Active   18h
kubernetes-dashboard   Active   17h

$ kubectl create -f kubia-manual.yaml -n custom-namespace
pod/kubia-manual created

$ kubectl get pods -n custom-namespace
NAME           READY   STATUS    RESTARTS   AGE
kubia-manual   1/1     Running   0          19s
```

命名空间可以提供隔离。

### RC 副本控制器

```yaml
kind: ReplicationController
apiVersion: v1
metadata:
  name: kubia-rc
spec:
  replicas: 3
  selector:
    app: kubia
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
        - name: kubia
          image: luksa/kubia
          ports:
            - containerPort: 8080
```

ReplicationController按照描述文件的要求来处理pod。上例中的rc会在集群内维护3个kubia容器，保证满足Selector:app=kubia的容器有三个。

通过更改pod的标签，可以将其从rc的作用域中添加或者删除。

```shell
$ kubectl get rc -n custom-namespace -o wide
NAME       DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES                 SELECTOR
kubia-rc   3         3         3       3m31s   kubia        luksa/kubia   app=kubia

$ kubectl describe rc kubia-rc -n custom-namespace
Name:         kubia-rc
Namespace:    custom-namespace
Selector:     app=kubia
Labels:       app=kubia
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=kubia
  Containers:
   kubia:
    Image:        luksa/kubia
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From                    Message
  ----    ------            ----  ----                    -------
  Normal  SuccessfulCreate  4m9s  replication-controller  Created pod: kubia-rc-8g4g5
  Normal  SuccessfulCreate  4m9s  replication-controller  Created pod: kubia-rc-qdp5c
  Normal  SuccessfulCreate  4m9s  replication-controller  Created pod: kubia-rc-w5bvv

$ kubectl get po --show-labels
NAME             READY   STATUS    RESTARTS   AGE     LABELS
kubia-manual     1/1     Running   0          3h23m   env=testing,name=kubia-manual
kubia-rc-qdp5c   1/1     Running   0          10m     app=kubia
kubia-rc-rrqd9   1/1     Running   0          4m42s   app=kubia
kubia-rc-w5bvv   1/1     Running   0          10m     app=kubia


$ kubectl label pods kubia-manual app=kubia
pod/kubia-manual labeled


$ kubectl get po --show-labels
NAME             READY   STATUS        RESTARTS   AGE     LABELS
kubia-manual     1/1     Running       0          3h23m   app=kubia,env=testing,name=kubia-manual
kubia-rc-qdp5c   1/1     Running       0          10m     app=kubia
kubia-rc-rrqd9   1/1     Terminating   0          5m24s   app=kubia
kubia-rc-w5bvv   1/1     Running       0          10m     app=kubia
```

如果修改了rc的标签选择器，则不满足的pod会立即脱离rc的管理，直到手动将其删除。

```shell
# 修改 rc 的描述文件
kubectl edit rc kubia-rc
# 水平扩容
kubectl scale rc kubia-rc --replicas=10
```

删除rc时，下辖的pod也会消亡，这可能不是所期望的，为了保持运行可以使用 --cascade=false。

### RS 副本控制器v2

ReplicaSet的标签选择器更强大，可以同时匹配env=prod和env=testing，rc则不可以。删除rs时，下辖的pod也会消亡，rs也支持 --cascade=false。

```yaml
kind: ReplicaSet
apiVersion: apps/v1
metadata:
  name: kubia-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      name: kubia-pod
      labels:
        app: kubia
    spec:
      containers:
        - name: kubia-container
          image: luksa/kubia
          ports:
            - containerPort: 8080
```

```shell
$ kubectl get rs
NAME       DESIRED   CURRENT   READY   AGE  
kubia-rs   3         3         3       1m11s

$ kubectl get pods -o wide
NAME             READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES
kubia-rs-47glr   1/1     Running   0          42s   172.17.0.7   minikube   <none>           <none>
kubia-rs-8gjcr   1/1     Running   0          42s   172.17.0.3   minikube   <none>           <none>
kubia-rs-vcrxn   1/1     Running   0          42s   172.17.0.6   minikube   <none>           <none>
```

### DS 指定节点运行pod

使用DaemonSet在每个节点上运行一个pod。比如希望在每个节点上运行一个日志收集和资源监控器。另一个典型的例子是kube-proxy。使用nodeselector还可以在指定的节点运行一个pod。

DaemonSet管理的pod完全绕过了调度器，即使某些节点被设置为不可调度。DaemonSet的目的是运行系统服务，即使在不可调度的节点，系统服务通常也要运行。

```yaml
kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: ssd-monitor
spec:
  selector:
    matchLabels:
      app: ssd-monitor
  template:
    metadata:
      labels: 
        app: ssd-monitor
    spec:
      nodeSelector:
        disk: ssd
      containers:
      - name: main
        image: luksa/ssd-monitor
```

```shell
$ kubectl get ds
NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
ssd-monitor   0         0         0       0            0           disk=ssd        15s

$ kubectl label nodes minikube disk=ssd
node/minikube labeled

$ kubectl get ds
NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
ssd-monitor   1         1         0       1            0           disk=ssd        75s

$ kubectl get pod
NAME                READY   STATUS    RESTARTS   AGE
ssd-monitor-7c7mc   1/1     Running   0          44s
```

### JOB 运行单个任务

```yaml
kind: Job
apiVersion: batch/v1
metadata:
  name: batch-job
spec:
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      restartPolicy: OnFailure
      containers:
      - name: main
        image: luksa/batch-job

```

```shell
$ kubectl get jobs
NAME        COMPLETIONS   DURATION   AGE
batch-job   0/1           10s        10s

$ kubectl get pods
NAME              READY   STATUS              RESTARTS   AGE
batch-job-4j4lc   0/1     ContainerCreating   0          20s

$ kubectl get pods
NAME              READY   STATUS      RESTARTS   AGE  
batch-job-4j4lc   0/1     Completed   0          8m19s

$ kubectl get jobs
NAME        COMPLETIONS   DURATION   AGE
batch-job   1/1           4m45s      10m
```

使用completions控制运行次数；parallelism控制并发数量。下例会确保5个pod成功完成，最多可以有两个pod一起运行。可以运行时水平伸缩，增减并发数量。

```shell
kubectl sacle job batch-job --replicas 3 
```

```yaml
kind: Job
apiVersion: batch/v1
metadata:
  name: batch-job
spec:
  completions: 5
  parallelism: 2
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      restartPolicy: OnFailure
      containers:
      - name: main
        image: luksa/batch-job
```

任务执行完后，job资源并不会自动删除。

### CronJob 定期或定时任务

在计划的时间内，CronJob资源会创建Job资源，然后Job资源创建pod。

## 服务发现

POD的是短暂的，随时启动和关闭；调度到节点上pod启动前k8s才会分配IP地址，无法提前预知；水平伸缩意味着需要负载均衡。

### Service

为一组功能相同的pod提供单一不变的接入点的资源。服务存在时，IP、PORT不会改变。外部与之通信则会被路由到任意pod。

Service 是 K8S 服务的核心，屏蔽了服务细节，统一对外暴露服务接口，真正做到了“微服务”。举个例子，我们的一个服务 A，部署了 3 个备份，也就是 3 个 Pod；对于用户来说，只需要关注一个 Service 的入口就可以，而不需要操心究竟应该请求哪一个 Pod。优势非常明显：一方面外部用户不需要感知因为 Pod 上服务的意外崩溃、K8S 重新拉起 Pod 而造成的 IP 变更，外部用户也不需要感知因升级、变更服务带来的 Pod 替换而造成的 IP 变化，另一方面，Service 还可以做流量负载均衡。

```yaml
kind: Service
apiVersion: v1
metadata:
  name: kubia-svc
spec:
  selector:
    app: kubia
  ports:
  - name: kubia-port
    port: 80
    targetPort: 8080
```

```shell
kubectl get svc
NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP      10.96.0.1       <none>        443/TCP        3d15h
kubia-svc    ClusterIP      10.101.87.239   <none>        80/TCP         13s
```

服务将拥有一个集群内可访问的 IP 地址，端口80。可以通过运行的容器中远程执行命令测试连通性。下例中使用一个集群中的pod访问服务，其请求被k8s代理服务器拦截，然后按照一定的策略选择一个转发（80 -> 8080），然后返回其响应。

```shell
kubectl exec kubia-pod -- curl -s http://10.101.87.239
you have hit kubia-rs-nl6dd
kubectl exec kubia-pod -- curl -s http://10.101.87.239:80
you have hit kubia-rs-4ppgm

#note!
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead
```

双横杠代表kubectl命令结束。后边的内容则是pod内部执行的命令。`sessionAffinity: ClientIP`回话的亲和性，这会使来自同个clientIP的所有请求转发至同一个pod。另一个选项是`None`。k8s工作在传输层，因此无法支持cookie的会话亲和性的选项（Not HTTP）。

暴露多个端口则依次在yaml中添加即可。如果pod的端口定义使用的是名称，则svc的targetport也可以使用名称。这样修改暴露端口号时比较简便。

```yaml
kind: ReplicaSet
apiVersion: apps/v1
metadata:
  name: kubia-rs-namedport
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia-namedport
  template:
    metadata:
      name: kubia-pod-namedport
      labels:
        app: kubia-namedport
    spec:
      containers:
        - name: kubia-container-namedport
          image: luksa/kubia
          ports:
            - name: http
              containerPort: 8080
```

```yaml
kind: Service
apiVersion: v1
metadata:
  name: kubia-svc-namedport
spec:
  selector:
    app: kubia-namedport
  ports:
  - name: http
    port: 80
    targetPort: http
```

```shell
kubectl exec kubia-rs-namedport-44dlj -- curl -s http://10.100.47.164
you have hit kubia-rs-namedport-f6g5x

kubectl exec kubia-rs-namedport-44dlj -- curl -s http://10.100.47.164:80
you have hit kubia-rs-namedport-f6g5x
```

服务也不是直接和pod相连，而是借助Endpoint资源。通过手动配置Endpoint资源来代替公开外部服务。另一种方法是通过其完全限定域名访问外部服务。

```shell
kubectl describe svc kubia-svc         
Name:              kubia-svc
Namespace:         default
Labels:            <none>
Annotations:       <none>
Selector:          app=kubia
Type:              ClusterIP
IP Families:       <none>
IP:                10.109.24.62
IPs:               10.109.24.62
Port:              kubia-port  80/TCP
TargetPort:        8080/TCP
Endpoints:         172.17.0.2:8080,172.17.0.3:8080,172.17.0.4:8080
Session Affinity:  None
Events:            <none>
```

### 集群内部服务发现

在集群内部对一个服务的访问，主要有2种方式，环境变量与DNS。当一个pod创建时，集群中属于同个namespace下的所有service对象信息都会被作为环境变量添加到pod中。pod通过{SVCNAME}_SERVICE_HOST/PORT就可以方便的访问到某个服务。这种访问方式简单易用，可以用来快速测试服务。但最大的问题就是，服务必须先于pod创建，后创建的服务是不会添加到现有pod的环境变量中的。

DNS组件是k8s集群的可选组件，它会不停监控k8s API，在有新服务创建时，自动创建相应的DNS记录。在服务创建时，会创建一条`服务名.命名空间.svc.cluster.local`的dns记录指向服务。而且dns记录作用域是整个集群，不局限在namespace。虽然是可选组件，但DNS生产环境可以说是必备的组件了。

```shell
kubectl exec -it kubia-rs-5k5vd -- bash 
root@kubia-rs-5k5vd:/# curl http://kubia-svc.default.svc.cluster.local
you have hit kubia-rs-tf5jb

root@kubia-rs-5k5vd:/# cat /etc/resolv.conf 
nameserver 10.96.0.10
search default.svc.cluster.local svc.cluster.local cluster.local localdomain
options ndots:5

# 不要通过ping服务的IP来检查服务是否可访问，虚拟IP时无法ping通的
root@kubia-rs-5k5vd:/# ping kubia-svc
PING kubia-svc.default.svc.cluster.local (10.109.24.62): 56 data bytes
^C--- kubia-svc.default.svc.cluster.local ping statistics ---
25 packets transmitted, 0 packets received, 100% packet loss
```

kubia-svc.default.svc.cluster.local: 服务名.命名空间.svc.cluster.local。服务和pod位于相同的名字空间，可以直接使用服务名。

### 连接集群外部服务

访问集群外的服务，比如数据库。endpoint资源会由service自动创建，存储提供服务pod地址。如果创建不包含任何pod的资源，则不会创建endpoint，此时手动创建endpoint，并写入外部服务的地址即可。endpoint名称与service同名。

```yaml
kind: Endpoints
apiVersion: v1
metadata:
  name: external-service
subsets:
  - addresses:
    - ip: 1.1.1.1
    - ip: 2.2.2.2
    ports:
    - port: 80
```

另一种方法是通过完全限定域名（FQDN）访问外部服务。下例中服务创建后，通过enternal-service.defualt.svc.cluster.local访问外部服务。

```yaml
kind: Service
apiVersion: v1
metadata:
  name:  external-service
spec:
  type: ExternalName
  externalName: kubia.scorpion.com
  ports:
  - port:  80
```

### 暴露给外部客户端

设置服务类型，yaml文件的spec中添加属性type，可以选项包括：LoadBalancer | ClusterIP | NodePort。它们都是将集群外部流量导入到集群内的方式，只是实现方式不同。如果没有设置的话，默认为ClusterIP，这种模式下启用K8S的proxy模式来访问。

![clusterip](../Resource/K8S-ClusterIP.jpg)

如上节中的例子，服务被分配了一个clusterip，通过这个IP来访问服务。该IP是一个虚拟的地址，具体的代理请求时iptables转发实现的。在服务创建时，kube-proxy组件会自动创建同名的endpoint对象，动态的匹配selector的一组pod当前ip及端口，并生成相应的iptables KUBE-SVC-xxxx规则。

NodePort 服务是引导外部流量到你的服务的最原始方式。NodePort，正如这个名字所示，在所有节点（虚拟机）上开放一个特定端口，任何发送到该端口的流量都被转发到对应服务。

![nodeport](../Resource/K8S-NodePort.jpg)

```yaml
kind: Service
apiVersion: v1
metadata:
  name: kubia-nodeport
spec:
  selector:
    app: kubia
  type: NodePort
  ports:
  # 通过服务IP:80访问服务 （内部）
  - port: 80
    targetPort: 8080
    # 通过集群节点IP:30123访问服务 （内部和外部）
    nodePort: 30123
```

```shell
kubectl get nodes -n kube-system -o wide
NAME       STATUS   ROLES                  AGE     VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
minikube   Ready    control-plane,master   3d17h   v1.20.0   192.168.49.2   <none>        Ubuntu 20.04.1 LTS   5.4.0-59-generic   docker://20.10.0

# 内部
kubectl exec kubia-rs-5k5vd -- curl -s http://192.168.49.2:30123
you have hit kubia-rs-tf5jb
# 外部
curl -s http://192.168.49.2:30123
you have hit kubia-rs-tf5jb
```

NodePort 服务主要有两点区别于普通的 ClusterIP 服务。第一，它的类型是 NodePort。有一个额外的端口，称为 nodePort，它指定节点上开放的端口值 。如果你不指定这个端口，系统将选择一个随机端口。大多数时候我们应该让 Kubernetes 来选择端口，因为如评论中 thockin 所说，用户自己来选择可用端口代价太大。这种方法有许多缺点：每个端口只能是一种服务；端口范围只能是 30000-32767；指定的节点故障时则访问失败。

LoadBalancer 服务是暴露服务到 internet 的标准方式。负载均衡器拥有独特的公开访问地址，并将所有连接重定向到服务。

![loadbalancer](../Resource/K8S-LoadBalancer.jpg)

```yaml
kind: Service
apiVersion: v1
metadata:
  name: kubia-loadbalancer
spec:
  selector:
    app: kubia
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8080
```

所有通往你指定的端口的流量都会被转发到对应的服务。它没有过滤条件，没有路由等。这意味着你几乎可以发送任何种类的流量到该服务，像 HTTP，TCP，UDP，Websocket，gRPC 或其它任意种类。这个方式的最大缺点是每一个用 LoadBalancer 暴露的服务都会有它自己的 IP 地址，每个用到的 LoadBalancer 都需要付费，这将是非常昂贵的。

### Ingress

Service 主要负责 K8S 集群内部的网络拓扑。那么集群外部怎么访问集群内部呢？这个时候就需要 Ingress 了，官方文档中的解释是：Ingress 是对集群中服务的外部访问进行管理的 API 对象，典型的访问方式是 HTTP；Ingress 可以提供负载均衡、SSL 终结和基于名称的虚拟托管。Ingress 事实上不是一种服务类型。相反，它处于多个服务的前端，扮演着智能路由或者集群入口的角色。你可以用 Ingress 来做许多不同的事情，各种不同类型的 Ingress 控制器也有不同的能力。

GKE 上的默认 ingress 控制器是启动一个 HTTP(S) Load Balancer。它允许你基于路径或者子域名来路由流量到后端服务。例如，你可以将任何发往域名 foo.yourdomain.com 的流量转到 foo 服务，将路径 yourdomain.com/bar/path 的流量转到 bar 服务。

![nodeport](../Resource/K8S-Ingress.jpg)

```yaml
kind: Ingress
apiVersion: extensions/v1beta1
metadata:
  name: kubia-ingress
spec:
  rules:
  # 将该域名映射到你的服务
  - host: kubia.example.com
    http: 
      paths:
        - path: /kubia
          backend: n 
            serviceName: kubia
            # 将所有kubia.example.com/kubia请求转发到kubia服务的80端口
            sercicePort: 80
        - path: /foo
            backend: 
            serviceName: bar
            # 将所有kubia.example.com/foo请求转发到bar服务的80端口
            sercicePort: 80
```

使用NodePort暴露Service的端口时Node IP会漂移的问题。同时，若大量使用NodePort暴露主机端口，管理会非常混乱。好的解决方案就是让外界通过域名去访问Service，而无需关心其Node IP及Port。那为什么不直接使用Nginx？这是因为在K8S集群中，如果每加入一个服务，我们都在Nginx中添加一个配置，其实是一个重复性的体力活，只要是重复性的体力活，我们都应该通过技术将它干掉。

Ingress是Kubernetes集群对外暴露服务的一种方式，Ingress作为一个抽象对象，定义了进入集群的流量的导向，可以视为router，真正处理流量，作为ingress和集群中微服务中介的称之为Ingress Controller，有许多种Controller可供选择，有Kubernetes官方维护的Nginx Ingress Controller，比较常用的还有Traefik Ingress Controller、Kong Ingress Controller，这里我们使用的是Nginx Ingress Controller。

```shell
ku
kubectl create -f kubia-ingress.yaml
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
ingress.extensions/kubia-ingress created

kubectl describe ingress
Name:             kubia-ingress
Namespace:        default
Address:          
Default backend:  default-http-backend:80 (<error: endpoints "default-http-backend" not found>)
Rules:
  Host               Path  Backends
  ----               ----  --------
  kubia.example.com  
                     /kubia   kubia-svc:80 (172.17.0.2:8080,172.17.0.3:8080,172.17.0.4:8080)
Annotations:         <none>
Events:              <none>
```

**minikube使用ingress需要打开支持**。

```shell
minikube addons enable ingress
```

客户端首先对kubia.example.com执行DNS查询，得到控制器的IP；然后向控制器发送HTTP请求，并在HOST头中指定了kubia.example.com。控制器从头部确定客户端访问哪个服务，通过与该服务相关的Endpoint对象查看pod IP，并将请求转发给一个pod。

### headless service

K8S允许客户端通过DNS查找pod的IP，执行服务的DNS时能够得到集群服务的IP；将服务的spec中clusterIP字段设置为None，DNS服务器将会返回podIP。

```yaml
kind: Service
apiVersion: v1
metadata:
  name: kubia-headless
spec:
  clusterIP: None
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
```

## 卷

pod的一部分，与pod共享相同的生命周期，启动时创建，删除时销毁。在容器重启期间，卷的内容保持不变；pod包含的多个容器可以共享卷。

类型：emptyDir用于存储临时数据的简单空目录；hostPath用于将目录从工作节点的文件系统挂载到pod中；gitRepo可以检出git仓库；nfs是挂载到pod中的nfs共享卷；configMap/secret/downwardAPI用于将K8S部分资源和集群信息公开给pod的特殊类型卷；等等。

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: fortune
  labels:
    app: fortune
spec:
  containers:
  - name: html-generator
    image: luksa/fortune
    volumeMounts:
      - mountPath: /var/htdocs
        name: html
    resources:
      limits:
        memory: "128Mi"
        cpu: "500m"
  - name: web-server
    image: nginx:alpine
    volumeMounts:
      - mountPath: /usr/share/nginx/html
        name: html
        readOnly: true
    ports:
      - containerPort: 80
        protocol: TCP
    resources:
      limits:
        memory: "128Mi"
        cpu: "500m"
  volumes:
  # 默认是在承载pod的工作节点的实际磁盘创建的
  - name: html
    emptyDir: {}
  # emptyDir:
  # 使用内存创建卷
  #   medium: Memory
```

```shell
# 访问pod的container查看文件
kubectl exec -it fortune -n default -c html-generator -- bash
ls /var/htdocs

# 开启端口转发
kubectl port-forward fortune 8080:80

# 检查pod
curl http://localhost:8080
```

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: gitrepo
  labels:
    app: gitrepo
spec:
  containers:
  - name: web-server
    image: nginx:alpine
    volumeMounts:
      - mountPath: /usr/share/nginx/html
        name: html
        readOnly: true
    ports:
      - containerPort: 80
        protocol: TCP
    resources:
      limits:
        memory: "128Mi"
        cpu: "500m"
  volumes:
    - name: html
      gitRepo:
        repository: https://github.com/luksa/kubia-website-example.git
        revision: master
        directory: .
```

gitRepo卷创建后，它并不会与仓库保持一致。如果删除pod，rc/rs创建新的，则会得到更新。实现此功能，Pod运行的节点都必需要安装git。换句话说，如果你的Pod指定在哪个节点上运行，则此节点必需安装git；如果不指定，则所有的节点必需安装git。

```shell
# 进入minikube的节点容器 安装git
docker exec -it 05271266c23fdc3bf22b053de87ff633d64d155300d25da9f466f809d5401b14 bash
apt-get update && apt-get install git


kubectl get pods                     
NAME      READY   STATUS    RESTARTS   AGE
gitrepo   1/1     Running   0          27m

kubectl port-forward gitrepo  8080:80

curl localhost:8080
```

gitRepo卷不支持克隆私有的repo，应当使用sidecar容器或类似的方法。

某些系统级别的pod，如ds管理的pod，可能需要访问node上的文件。使用hostpath卷可以实现这一需求。hostpath映射的文件系统在节点上，所以是一种持久性存储。

hostpath卷通常用于尝试单节点集群中的持久化存储。当且仅当节点需要访问节点系统文件时才使用，切勿用来持久化跨pod数据。

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: mongodb 
spec:
  containers:
  - image: mongo
    name: mongodb
    volumeMounts:
    - name: mongodb-data
      mountPath: /data/db
    ports:
    - containerPort: 27017
      protocol: TCP
    resources:
      limits:
        memory: "128Mi"
        cpu: "500m"
  volumes:
  - name: mongodb-data
    hostPath:
      path: /tmp/mongodb
```

### 持久卷的静态配置

在K8S集群中为了使应用能够正常请求存储资源，避免处理基础设施细节，引入了持久卷和持久卷声明。用户在pod中创建PersistentVolume声明清单，指定所需的最低容量和访问模式，然后交予K8S API服务器完成匹配和绑定持久卷工作（当然需要管理员实现创建某类网络存储）。持久卷声明可以当作pod的一个卷使用，其他用户不能使用相同的持久卷，除非先通过删除pv声明来释放。

持久卷pv不属于任何命名空间，同节点一样为集群层面的资源。持久卷声明pvc属于命名空间。

```yaml
kind: PersistentVolume
apiVersion: v1
metadata:
  name: mongodb-pv
spec:
  # 大小
  capacity: 
    storage: 1Gi
  # 可用模式
  # 单个客户端挂载为读写模式
  # 多个客户端挂载为只读模式
  accessModes:
    - ReadWriteOnce
    - ReadOnlyMany
  # pvc释放后 pv保留
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /tmp/mongodb
```

```shell
# node: pv status is Available
kubectl get pv
NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
mongodb-pv   1Gi        RWO,ROX        Retain           Available                                   41s
```

```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  # pvc 名称
  name: mongodb-pvc 
spec:
  resources:
    # 申请空间
    requests:
      storage: 1Gi
  # 申请的使用模式
  accessModes:
  - ReadWriteOnce
  # 指定空的 StorageClass 可以确保 pvc 绑定到已创建的 pv 而不是动态配置新的 pv
  storageClassName: ""
```

```shell
# note: pv status is Bound
kubectl get pv,pvc
NAME                          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                 STORAGECLASS   REASON   AGE
persistentvolume/mongodb-pv   1Gi        RWO,ROX        Retain           Bound    default/mongodb-pvc                           22m

NAME                                STATUS   VOLUME       CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/mongodb-pvc   Bound    mongodb-pv   1Gi        RWO,ROX                       14s
```

创建pv涉及到底层存储细节，pvc描述了使用需求，创建后会被自动的绑定到合适的pv。

>- ReadWriteOnce（RWO）：是最基本的方式，可读可写，但只支持被单个 Pod 挂载。
>- ReadOnlyMany（ROX）：可以以只读的方式被多个 Pod 挂载。
>- ReadWriteMany（RWX）：这种存储可以以读写的方式被多个 Pod 共享。

不是每一种PV都支持这三种方式，例如ReadWriteMany，目前支持的还比较少。在 PVC 绑定 PV 时通常根据两个条件来绑定，一个是存储的大小，另一个就是访问模式。通过pv和pvc使用持久磁盘，pod和pv可以不依赖于基础设施内容。

```shell
kubectl delete pod mongodb
pod "mongodb" deleted

kubectl delete pvc mongodb-pvc
persistentvolumeclaim "mongodb-pvc" deleted

# note: pv status is Released 表示这个pv被使用过
NAME                          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                 STORAGECLASS   REASON   AGE
persistentvolume/mongodb-pv   1Gi        RWO,ROX        Retain           Released   default/mongodb-pvc                           37m
```

PV 的回收策略（persistentVolumeReclaimPolicy，即 PVC 释放卷的时候 PV 该如何操作）也有三种：

>- Retain，不清理, 保留 Volume（需要手动清理）
>- Recycle，删除数据，即 rm -rf /thevolume/*（只有 NFS 和 HostPath 支持）
>- Delete，删除存储资源，比如删除 AWS EBS 卷（只有 AWS EBS, GCE PD, Azure Disk 和 Cinder 支持）

PVC释放卷是指用户删除一个PVC对象时，那么与该PVC对象绑定的PV就会被释放。

### 持久卷的动态配置

上文中我们通过PersistentVolume描述文件创建了一个PV。这样的创建方式我们成为静态创建。这样的创建方式有一个弊端，那就是假如我们创建PV时指定大小为50G，而PVC请求80G的PV，那么此PVC就无法找到合适的PV来绑定。因此产生了了PV的动态创建。

PV的动态创建依赖于StorageClass对象。我们不需要手动创建任何PV，所有的工作都由StorageClass为我们完成。StorageClass也不属于任何命名空间。

在用户创建pvc前，管理员需要创建若干StorageClass对象，然后才能创建pv。

```yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast
# 配置 pv 的插件 置备程序
provisioner: k8s.io/minikube-hostpath
parameters:
  type: pd-ssd
```

```shell
kubectl get sc
NAME                 PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
fast                 k8s.io/minikube-hostpath   Delete          Immediate           false                  4s
standard (default)   k8s.io/minikube-hostpath   Delete          Immediate           false                  17h
```

```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: mongodb-pvc 
spec:
  # 使用这个 sc 如果引用的 sc 不可用 则会看到 ProvisioningFailed
  # 没有配置的话 则会使用集群默认的 StorageClass: Standard(default)
  storageClassName: fast
  resources:
    requests:
      storage: 100Mi
  accessModes:
    - ReadWriteOnce
```

```shell
kubectl get pv,pvc
NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                 STORAGECLASS   REASON   AGE
persistentvolume/pvc-307d8350-e1e2-4b32-864a-e78863da6bb4   100Mi      RWO            Delete           Bound    default/mongodb-pvc   fast                    92s

NAME                                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/mongodb-pvc   Bound    pvc-307d8350-e1e2-4b32-864a-e78863da6bb4   100Mi      RWO            fast           92s
```

动态配置的持久卷和容量是在pvc中要求的，回收策略是默认的 Delete 策略，pv 则是置备程序自动创建的。

```shell
kubectl get sc                
NAME                 PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
fast                 k8s.io/minikube-hostpath   Delete          Immediate           false                  11m
standard (default)   k8s.io/minikube-hostpath   Delete          Immediate           false                  17h
```

standard (default) 是指不指StorageClass情况下创建pvc则会使用默认的sc。动态持久卷的创建步骤：1. 集群管理员设置持久卷置备程序 2. 管理员创建若干StorageClass并将其标记为默认值 3. 用户创建一个 pvc，引用了其中一个 StorageClass 4. K8S 查找引用的 SC 和置备程序，要求置备程序按 pvc 要求创建新的 pv 5. pvc 绑定到该 pv 6. 用户创建 pod，通过名称引用 pvc。

### ConfigMap

Dockerfile定义中指定参数。Docker运行命令可以覆盖内部的参数。

```Dockerfile
ENTRYPOINT ["exec"]
CMD ["arg1", "arg2", "arg3"]
```

```shell
docker run <image> <args>
```

K8S中定义容器中指定参数。

```yaml
kind: Pod
spec:
  containers:
  - image: <IMAGE>
    command: ["COMMAND"]
    args: ["ARG1", "ARG2", "ARG3"]

... 较多参数可以采用如下标记，字符串无需引号标记，数值需要。
    args：
    - foo
    - bar
    - "15"
```

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: fortune2s
spec:
  containers:
  - image: luksa/fortune:args
    args: ["2"]
    name: html-generator
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    ports:
    - containerPort: 80
      protocol: TCP
  volumes:
  - name: html
    emptyDir: {}
```

容器化应用也可使用环境变量作为配置源。容器定义文件中引用环境变量，YAML配置文件中再注入环境变量。注意环境变量只被设置在 Pod 的容器定义中，并非 Pod 级别。

```shell
# !/bin/bash
trap "exit" SIGINT
echo Configured to genrate new fortune every $INTERVAL seconds
mkidr -p /var/htdocs
while :
do 
  echo $(date) Writing fortune to /var/htdocs/index.html
  /usr/games/fortune > /var/htdocs/index.html
  sleep $INTERVAL
done
```

```yaml
kind: Pod
spec:
  containers:
  - images: luksa/fortune:env
    env:
    - name: INTERVAL
      value: "30"
    name: html-generator
```

#### ConfigMap 定义

用于存储配置资源数据的K8S资源成为ConfigMap。ConfigMap 本质上一个键值对映射，值可以是短字面量，也可以是完整的配置文件。

```shell
# 命令行创建 可以从文件、文件夹、字面量创建键值对
# foo.json=<foo.json文件的内容> key为文件名
# bar=<bar.conf文件的内容> 指定了key为bar
# config-opts文件夹下所有的文件都会导入
# some=thing
kubectl create configmap my-config --from-file=foo.json --from-file=bar=bar.conf --from-file=config-opts --from-literal=some=thing
```

使用 yaml 创建 configmap 资源。ConfigMap 属于命名空间下的资源。

```yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: fortune-config
data:
  sleep-interval: "25"
```

#### ConfigMap 注入

注入方式有两种，一种是将configMap通过env中configMapKeyRef注入到容器中，一种将configMap做为存储卷。创建 pod 引用不存在的 configmap 的容器会启动失败，其余容器会正常启动；如果之后创建了这个 configmap，失败的容器会自动启动，无需重新创建 pod。

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: fortune-env-from-configmap
spec:
  containers:
  - image: luksa/fortune:env
    env:
    # 注入容器的环境变量
    - name: INTERVAL
      valueFrom: 
        configMapKeyRef:
          # 引用的 configmap 资源名字
          name: fortune-config
          # 引用的 key 环境变量 INTERVAL 被设置为 sleep-interval 的值
          key: sleep-interval
    name: html-generator
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    ports:
    - containerPort: 80
      protocol: TCP
  volumes:
  - name: html
    emptyDir: {}
```

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: fortune-args-from-configmap
spec:
  containers:
  - image: luksa/fortune:args
    env:
    - name: INTERVAL
      valueFrom: 
        configMapKeyRef:
          name: fortune-config
          key: sleep-interval
    # yaml 文件中可以直接引用定义的环境变量
    args: ["$(INTERVAL)"]
    name: html-generator
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    ports:
    - containerPort: 80
      protocol: TCP
  volumes:
  - name: html
    emptyDir: {}
```

一次性将所有的条目暴露为环境变量。假设 configmap 包含 FOO BAR FOO-BAR 两个键，则下面示例将暴露 CONFIG_FOO CONFIG_BAR 两个环境变量。FOO-BAR 因为包含 -，属于不合法的环境变量名称，会被忽略。

```yaml
spec:
  containers:
  - image: <IMAGE>
    envFrom:
    # 所有环境变量添加前缀 CONFIG_
    - prefix: CONFIG_
      # 引用名为 my-config-map 的 configmap
      configMapRef:
        name: my-config-map
```

环境变量或者命令行参数值作为配置值通常适用于变量值较短的场景。ConfigMap 可以包含完整的配置文件，此时使用 configMap 卷来暴露配置文件。configMap 卷会将 ConfigMap 中的每个条目暴露成为一个文件。运行在容器中的进程可以通过读取文件的方式载入配置。

```shell
$ ls configmap-files
my-niginx-config.conf sleep-interval

$ kubectl create configmap fortune-config --from-file=configmap-files

$ kubectl describe configmap fortune-config
Name:         fortune-config
Namespace:    default       
Labels:       <none>        
Annotations:  <none>        

Data
====
sleep-interval:
----
25

my-nginx-config.conf:
----
server {
    listen              80;
    server_name         www.kubia-example.com;

    gzip on;
    gzip_types text/plain application/xml;

    location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
    }
}

Events:  <none>
```

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: fortune-configmap-volume
spec:
  containers:
  - image: luksa/fortune:env
    env:
    - name: INTERVAL
      valueFrom:
        configMapKeyRef:
          name: fortune-config
          key: sleep-interval
    name: html-generator
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    - name: config
      # 挂载 configMap 卷至这个文件夹下
      mountPath: /etc/nginx/conf.d
      readOnly: true
    - name: config
      mountPath: /tmp/whole-fortune-config-volume
      readOnly: true
    ports:
      - containerPort: 80
        name: http
        protocol: TCP
  volumes:
  - name: html
    emptyDir: {}
  - name: config
    # 卷定义引用 fortune-config ConfigMap
    configMap:
      name: fortune-config
```

```shell
$ kubectl port-forward fortune-configmap-volume 8080:80
Forwarding from 127.0.0.1:8080 -> 80
Forwarding from [::1]:8080 -> 80

$ curl -H "Accept-Encoding: gzip" -I localhost:8080
curl: (3) URL using bad/illegal format or missing URL
HTTP/1.1 200 OK
Server: nginx/1.19.6
Date: Thu, 07 Jan 2021 03:18:06 GMT
Content-Type: text/html
Content-Length: 0
Last-Modified: Thu, 07 Jan 2021 03:18:06 GMT
Connection: keep-alive
ETag: "5ff67d6e-0"
Accept-Ranges: bytes

$ kubectl exec -it fortune-configmap-volume -c web-server -- sh
~ # ls /etc/nginx/conf.d/
my-nginx-config.conf  sleep-interval
```

ConfigMap 的两个条目均作为文件至于/etc/nginx/config.d文件夹下，但是web-server不需要sleep-interval这个条目。下例可以创建仅包 ConfigMap 中部分条目的 configmap 卷，避免产生其他影响。

```yaml
volumes:
- name: config
  configMap:
    name: fortune-config
    # 设置所有文件的权限为 -rw-rw------
    defaultMode: "6600"
    # 选择包含在卷中的条目
    items:
    # 该键对应的条目被包含
    - keys: my-niginx-config.conf
      # 条目的值被存储在该文件中
      path: gzip.conf
```

上述方法的存在一个隐患，**挂载某一文件夹会隐藏该文件夹中已存在的文件**。此时应当使用 volumeMount 额外的 subPath 字段。假设拥有一个包含文件 myconfig.congf 的 configMap 卷，希望添加为 /etc 文件夹下的 someconfig.conf 文件。

```yaml
spec:
  containers:
  - image: <IMAGE>
    volumeMounts:
    - name: myvolume
      # 挂载至某一文件，而不是文件夹
      mountPath: /ect/someconfig.conf
      # 金瓜在指定的条目 myconfig.conf 并非完整的卷
      subPath: myconfig.conf
```

ConfigMap 暴露为卷可以达到热更新的效果，无需重更新创建 Pod 或者重启容器。ConfigMap 更新后引用它的所有文件都会更新（但是需要等待一定时间），进程发现文件改变后也会重载。**如果挂载的是容器中的单个文件而不是完整的卷，则 ConfigMap 更新之后对应的文件不会更新**。

因此最佳实践仍然是：更新配置则删除所有 Pod，重新发一版。

### Secret

Secret 和 ConfigMap 相似，支持环境变量和卷两种注入方式，但是安全性更高，适合存储密钥等。

```shell
$ kubectl create secret generic fortune-https --from-file=fortune-https/https.cert --from-file=fortune-https/https.key 
secret/fortune-https created

$ kubectl describe secret fortune-https
Name:         fortune-https
Namespace:    default
Labels:       <none>
Annotations:  <none>

Type:  Opaque

Data
====
https.cert:  1146 bytes
https.key:   1706 bytes
```

在DockerHub使用私有镜像仓库，需要创建包含Docker镜像仓库证书的Secret；pod定义中的imagePullSecrets字段应用该Secret。

```shell
$ kubectl create secret docker-registry mydockerhubsecret --docker-username=cestlascorpion --docker-password=mypassword --docker-email=cestlascorpion@gmail.com
secret/mydockerhubsecret created

$ kubectl get secret
NAME                  TYPE                                  DATA   AGE
default-token-2kbbs   kubernetes.io/service-account-token   3      4h23m
fortune-https         Opaque                                2      10m
mydockerhubsecret     kubernetes.io/dockerconfigjson        1      31s
```

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: private-pod
spec:
  imagePullSecrets:
  - name: mydockerhubsecret
  containers:
  - image: username/private:tag
    name: main
```

## Deployment

Deployment 是一种更高阶的资源，用于部署应用程序，并以声明式的方法升级应用，而不是通过rs/rc部署。创建一个Deployment时，RS随之创建。Deployment 也是由标签选择器、期望副本数、pod模板组成的。

```yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: kubia
spec:
  replicas: 3
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
      - image: luksa/kubia:v1
        name: nodejs
  selector:
    matchLabels:
      app: kubia
```

```shell
# --record 记录升级历史
$ kubectl create -f kubia-deployment-v1.yaml  --record
deployment.apps/kubia created

$ kubectl get deployment
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
kubia   0/3     3            0           17s

$ kubectl describe deployment
Name:                   kubia
Namespace:              default
CreationTimestamp:      Thu, 07 Jan 2021 15:11:14 +0800
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
                        kubernetes.io/change-cause: kubectl.exe create --filename=kubia-deployment-v1.yaml --record=true
Selector:               app=kubia
Replicas:               3 desired | 3 updated | 3 total | 0 available | 3 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=kubia
  Containers:
   nodejs:
    Image:        luksa/kubia:v1
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      False   MinimumReplicasUnavailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:  <none>
NewReplicaSet:   kubia-74967b5695 (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  25s   deployment-controller  Scaled up replica set kubia-74967b5695 to 3
```

```shell

```

deployment 创建 rs，rs 创建 pod；rs 的名称是 deployment 名字 + pod模板哈希；pod 的名字是 rs 名字 + 随机串。在升级过程中 deployment 会创建多个 rs， 用于管理一个版本的 pod 模板。升级 deployment 则只需要修改 deployment 资源中定义的 pod 模板，系统会自动将实际的状态收敛为定义的状态：创建新的 rs - 创建新的 pod - 滚动升级 pod - 删除旧的 rs，其中还包括一些列的修改标签和名字等操作，deployment 将这些细节全部屏蔽。这是默认的升级策略（RollingUpdate），另一种策略为 Recreate，它会一次性删除所有旧版本 pod 然后创建新的 pod，适用于应用程序不支持多个版本共存的情况，升级过程中服务也会短暂不可用。

### 修改方式

方法 | 作用
:----: | :----:
kubectl edit | 使用默认编辑器打开资源配置文件，修改保存并退出编辑器，资源对象会被更新。eg: kubectl edit deployment kubia
kubectl patch | 修改单个资源属性。eg: kubectl patch deployment kubia -p '{"spec":{"template":{"spec":{"containers":[{"name":"nodejs","image":"luksa/kubia:v2"}]}}}}'
kubectl apply | 通过一个完整的 YAML/JSON 文件，应用其中的新值来修改对象；如果对象不存在则创建。该文件要求定义的完整性。eg: kubectl apply -f kubia-deployment-v2.yaml
kubectl replace | 将原有对象替换为 YAML/JSON 文件定义的新对象。与apple相反，该操作要求执行前对象必须存在。eg: kubectl replace -f kubia-deployment-v2.yaml
kubectl image | 修改Pod、RC、RS、Deployment、DaemonSet、Job内的镜像。 eg: kubectl set image deployment kubia nodejs=luksa/kubia:v2

**Deployment的pod模板引用的ConfigMap/Secret的变动不会触发升级操作**。

升级版本的时候 旧版本的 rs 没有被删除，以便快速回滚。Deployment的 revisionHistoryLimit 控制 revision 数量。

```shell
# 取消升级 回滚到上一个版本
kubectl rollout undo deployment kubia

# 回滚到特定版本
kubectl rollout undo deployment kubia --to-revision=1
```

RollingUpdateStrategy:  25% max unavailable, 25% max surge。maxUnavailable（最多允许的不可用pod数量） maxSurge（最多允许超出的pod数量） 控制了滚动升级中一次性会替换多少个pod。

```shell
# 暂停和恢复 恢复之前 undo不能撤销此次升级
kubectl rollout pause deployment kubia
kubectl rollout resume deployment kubia
```

miniReadySeconds属性指定新创建的Pod至少成功运行多久后才被视为可用（配合就绪探针）。

## StatefulSet

RC、Deployment、DaemonSet都是面向无状态的服务，它们所管理的Pod的IP、名字，启停顺序等都是随机的，而StatefulSet是什么？顾名思义，有状态的集合，管理所有有状态的服务，比如MySQL、MongoDB集群等。

与 RS 创建的 pod 相比，StatefulSet 创建的 pod 有规范的编号命名，发生故障时重启的 pod 将会沿用之前 pod 的名字和主机名。例如 StatefulSet A 创建 pod A-0 pod A-1 pod A-2，可以使用主机名 a-0.<服务名>.<命名空间>.svc.cluster.local 访问，扩容则会增加 pod A-3，缩容则会删除索引最高的 pod。

statefulset pod 的存储是有状态的，需要提供一对一的 pv/pvc。扩容时会自动创建一个或多个与之对应的 pvc，缩容时不会删除这些 pvc，再次扩容时依然会沿用这些 pvc。

通过 StatefulSet 部署应用，需要先创建持久卷（当集群不支持持久卷的动态供应时），然后创建 headless service 用于在有状态的 pod 之间提供网络标识，最后创建 StatefulSet 本身。

```yaml
kind: Service
apiVersion: v1
metadata:
  name: kubia
spec:
  # 指定 clusterip 为 none 标记了 这是一个 headless 服务
  # 用于 pod 之间的互相发现
  clusterIP: None
  selector:
    app: kubia
  ports:
  - name: http
    port: 80
```

```yaml
kind: StatefulSet
apiVersion: apps/v1
metadata:
  name: kubia
spec:
  serviceName: kubia
  replicas: 2
  selector:
    matchLabels:
      app: kubia # has to match .spec.template.metadata.labels
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia-pet
        ports:
        - name: http
          containerPort: 8080
        volumeMounts:
        - name: data
          mountPath: /var/data
  # 如果不支持动态供应，还需要手动创建pv
  # 创建持久卷声明模板 StatefulSet 创建 pod 时会自动将 pvc 添加到 pod 详述中
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      resources:
        requests:
          storage: 1Mi
      accessModes:
      - ReadWriteOnce
```

```shell
$ kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
kubia-0                  1/1     Running   0          42s
kubia-1                  1/1     Running   0          30s

$ kubectl get pv                    
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                  STORAGECLASS   REASON   AGE
pvc-c7dbf53f-fdd9-4256-b0a4-76eab3af2d38   1Mi        RWO            Delete           Bound    default/data-kubia-1   standard                3m59s
pvc-ea8baea8-a09a-4f5b-b175-94059f3e20a1   1Mi        RWO            Delete           Bound    default/data-kubia-0   standard                4m11s

$ kubectl get pvc
NAME           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
data-kubia-0   Bound    pvc-ea8baea8-a09a-4f5b-b175-94059f3e20a1   1Mi        RWO            standard       4m13s
data-kubia-1   Bound    pvc-c7dbf53f-fdd9-4256-b0a4-76eab3af2d38   1Mi        RWO            standard       4m1s
```

headless service 没有服务 ip 来访问，而是直接访问 pod。一种方式是借助另一个 pod 访问，另一种则是借助 API 服务器。

```shell
# 使用 localhost:8001 来代替实际的 API 服务地址
$ kubectl proxy
Starting to serve on 127.0.0.1:8001
```

直接访问 localhost:8001 返回的内容。

```json
{
  "paths": [
    "/.well-known/openid-configuration",
    "/api",
    "/api/v1",
    "/apis",
    "/apis/",
    "/apis/admissionregistration.k8s.io",
    "/apis/admissionregistration.k8s.io/v1",
    "/apis/admissionregistration.k8s.io/v1beta1",
    "/apis/apiextensions.k8s.io",
    "/apis/apiextensions.k8s.io/v1",
    "/apis/apiextensions.k8s.io/v1beta1",
    "/apis/apiregistration.k8s.io",
    "/apis/apiregistration.k8s.io/v1",
    "/apis/apiregistration.k8s.io/v1beta1",
    "/apis/apps",
    "/apis/apps/v1",
    "/apis/authentication.k8s.io",
    "/apis/authentication.k8s.io/v1",
    "/apis/authentication.k8s.io/v1beta1",
    "/apis/authorization.k8s.io",
    "/apis/authorization.k8s.io/v1",
    "/apis/authorization.k8s.io/v1beta1",
    "/apis/autoscaling",
    "/apis/autoscaling/v1",
    "/apis/autoscaling/v2beta1",
    "/apis/autoscaling/v2beta2",
    "/apis/batch",
    "/apis/batch/v1",
    "/apis/batch/v1beta1",
    "/apis/certificates.k8s.io",
    "/apis/certificates.k8s.io/v1",
    "/apis/certificates.k8s.io/v1beta1",
    "/apis/coordination.k8s.io",
    "/apis/coordination.k8s.io/v1",
    "/apis/coordination.k8s.io/v1beta1",
    "/apis/discovery.k8s.io",
    "/apis/discovery.k8s.io/v1beta1",
    "/apis/events.k8s.io",
    "/apis/events.k8s.io/v1",
    "/apis/events.k8s.io/v1beta1",
    "/apis/extensions",
    "/apis/extensions/v1beta1",
    "/apis/flowcontrol.apiserver.k8s.io",
    "/apis/flowcontrol.apiserver.k8s.io/v1beta1",
    "/apis/metrics.k8s.io",
    "/apis/metrics.k8s.io/v1beta1",
    "/apis/networking.k8s.io",
    "/apis/networking.k8s.io/v1",
    "/apis/networking.k8s.io/v1beta1",
    "/apis/node.k8s.io",
    "/apis/node.k8s.io/v1",
    "/apis/node.k8s.io/v1beta1",
    "/apis/policy",
    "/apis/policy/v1beta1",
    "/apis/rbac.authorization.k8s.io",
    "/apis/rbac.authorization.k8s.io/v1",
    "/apis/rbac.authorization.k8s.io/v1beta1",
    "/apis/scheduling.k8s.io",
    "/apis/scheduling.k8s.io/v1",
    "/apis/scheduling.k8s.io/v1beta1",
    "/apis/storage.k8s.io",
    "/apis/storage.k8s.io/v1",
    "/apis/storage.k8s.io/v1beta1",
    "/healthz",
    "/healthz/autoregister-completion",
    "/healthz/etcd",
    "/healthz/log",
    "/healthz/ping",
    "/healthz/poststarthook/aggregator-reload-proxy-client-cert",
    "/healthz/poststarthook/apiservice-openapi-controller",
    "/healthz/poststarthook/apiservice-registration-controller",
    "/healthz/poststarthook/apiservice-status-available-controller",
    "/healthz/poststarthook/bootstrap-controller",
    "/healthz/poststarthook/crd-informer-synced",
    "/healthz/poststarthook/generic-apiserver-start-informers",
    "/healthz/poststarthook/kube-apiserver-autoregistration",
    "/healthz/poststarthook/priority-and-fairness-config-consumer",
    "/healthz/poststarthook/priority-and-fairness-config-producer",
    "/healthz/poststarthook/priority-and-fairness-filter",
    "/healthz/poststarthook/rbac/bootstrap-roles",
    "/healthz/poststarthook/scheduling/bootstrap-system-priority-classes",
    "/healthz/poststarthook/start-apiextensions-controllers",
    "/healthz/poststarthook/start-apiextensions-informers",
    "/healthz/poststarthook/start-cluster-authentication-info-controller",
    "/healthz/poststarthook/start-kube-aggregator-informers",
    "/healthz/poststarthook/start-kube-apiserver-admission-initializer",
    "/livez",
    "/livez/autoregister-completion",
    "/livez/etcd",
    "/livez/log",
    "/livez/ping",
    "/livez/poststarthook/aggregator-reload-proxy-client-cert",
    "/livez/poststarthook/apiservice-openapi-controller",
    "/livez/poststarthook/apiservice-registration-controller",
    "/livez/poststarthook/apiservice-status-available-controller",
    "/livez/poststarthook/bootstrap-controller",
    "/livez/poststarthook/crd-informer-synced",
    "/livez/poststarthook/generic-apiserver-start-informers",
    "/livez/poststarthook/kube-apiserver-autoregistration",
    "/livez/poststarthook/priority-and-fairness-config-consumer",
    "/livez/poststarthook/priority-and-fairness-config-producer",
    "/livez/poststarthook/priority-and-fairness-filter",
    "/livez/poststarthook/rbac/bootstrap-roles",
    "/livez/poststarthook/scheduling/bootstrap-system-priority-classes",
    "/livez/poststarthook/start-apiextensions-controllers",
    "/livez/poststarthook/start-apiextensions-informers",
    "/livez/poststarthook/start-cluster-authentication-info-controller",
    "/livez/poststarthook/start-kube-aggregator-informers",
    "/livez/poststarthook/start-kube-apiserver-admission-initializer",
    "/logs",
    "/metrics",
    "/openapi/v2",
    "/openid/v1/jwks",
    "/readyz",
    "/readyz/autoregister-completion",
    "/readyz/etcd",
    "/readyz/informer-sync",
    "/readyz/log",
    "/readyz/ping",
    "/readyz/poststarthook/aggregator-reload-proxy-client-cert",
    "/readyz/poststarthook/apiservice-openapi-controller",
    "/readyz/poststarthook/apiservice-registration-controller",
    "/readyz/poststarthook/apiservice-status-available-controller",
    "/readyz/poststarthook/bootstrap-controller",
    "/readyz/poststarthook/crd-informer-synced",
    "/readyz/poststarthook/generic-apiserver-start-informers",
    "/readyz/poststarthook/kube-apiserver-autoregistration",
    "/readyz/poststarthook/priority-and-fairness-config-consumer",
    "/readyz/poststarthook/priority-and-fairness-config-producer",
    "/readyz/poststarthook/priority-and-fairness-filter",
    "/readyz/poststarthook/rbac/bootstrap-roles",
    "/readyz/poststarthook/scheduling/bootstrap-system-priority-classes",
    "/readyz/poststarthook/start-apiextensions-controllers",
    "/readyz/poststarthook/start-apiextensions-informers",
    "/readyz/poststarthook/start-cluster-authentication-info-controller",
    "/readyz/poststarthook/start-kube-aggregator-informers",
    "/readyz/poststarthook/start-kube-apiserver-admission-initializer",
    "/readyz/shutdown",
    "/version"
  ]
}
```

```shell
$ curl http://localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/

You've hit kubia-0
Data stored on this pod: No data posted yet
```

使用代理模式需要经过 kubectl proxy - API Server 两个代理。

```shell
$ curl -X POST -d "HEY THERE! kubia-0." http://localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
Data stored on pod kubia-0

$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
You've hit kubia-0
Data stored on this pod: HEY THERE! kubia-0.

$ kubectl delete pod kubia-0
pod "kubia-0" deleted

$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
You've hit kubia-0
Data stored on this pod: HEY THERE! kubia-0.
```

## 问题定位

使用 kubectl describe pod 查看异常的 pod 的状态，在容器列表里看 State 字段，其中 ExitCode 即程序退出时的状态码，正常退出时为0。如果不为0，表示异常退出。

退出状态码的区间

>- 必须在 0-255 之间
>- 0 表示正常退出
>- 外界中断将程序退出的时候状态码区间在 129-255，(操作系统给程序发送中断信号，比如 kill -9 是 SIGKILL，ctrl+c 是 SIGINT)
>- 一般程序自身原因导致的异常退出状态区间在 1-128 (这只是一般约定，程序如果一定要用129-255的状态码也是可以的)

假如写代码指定的退出状态码时不在 0-255 之间，例如: exit(-1)，这时会自动做一个转换，最终呈现的状态码还是会在 0-255 之间。我们把状态码记为 code。当指定的退出时状态码为负数，那么转换公式如下: 256 - (|code| % 256)；当指定的退出时状态码为正数，那么转换公式如下: code % 256

### 常见异常状态码

137：此状态码一般是因为 pod 中容器内存达到了它的资源限制(resources.limits)，一般是内存溢出(OOM)，CPU达到限制只需要不分时间片给程序就可以。因为限制资源是通过 linux 的 cgroup 实现的，所以 cgroup 会将此容器强制杀掉，类似于 kill -9 还可能是宿主机本身资源不够用了(OOM)，内核会选取一些进程杀掉来释放内存。不管是 cgroup 限制杀掉进程还是因为节点机器本身资源不够导致进程死掉，都可以从系统日志中找到记录。ubuntu 的系统日志在 /var/log/syslog，centos 的系统日志在 /var/log/messages，都可以用 journalctl -k 来查看系统日志。

1 和 255：这种可能是一般错误，具体错误原因只能看容器日志，因为很多程序员写异常退出时习惯用 exit(1) 或 exit(-1)，-1 会根据转换规则转成 255

## 安全防护

Service Account为Pod中的进程和外部用户提供身份信息。所有的kubernetes集群中账户分为两类，Kubernetes管理的serviceaccount(服务账户)和useraccount（用户账户）。

ServiceAccount资源作用于单独的命名空间，一般每个命名空间都包含一个名为default的sa（自动创建），每个pod都使用这个default的sa。注意pod只能使用同一个命名空间下的sa。将不同的sa赋予pod时，可以控制pod访问的资源：当API服务器接收到一个带有认证token的请求时，会用这个token验证发送者关联的sa是否具有相应的权限。API服务通过管理员配置的系统级别认证插件来获取这些信息。

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: scorpion

---

apiVersion: v1
kind: ServiceAccount
metadata:
  name: scorpio
  namespace: scorpion
```

```shell
$ kubectl describe -n scorpion sa
Name:                default
Namespace:           scorpion
Labels:              <none>
Annotations:         <none>
Image pull secrets:  <none>
Mountable secrets:   default-token-dqzw9
Tokens:              default-token-dqzw9
Events:              <none>


Name:                scorpio
Namespace:           scorpion
Labels:              <none>
Annotations:         <none>
Image pull secrets:  <none>
Mountable secrets:   scorpio-token-s4zzp
Tokens:              scorpio-token-s4zzp
Events:              <none>

$ kubectl get secret -n scorpion
NAME                  TYPE                                  DATA   AGE
default-token-dqzw9   kubernetes.io/service-account-token   3      9m46s
scorpio-token-s4zzp   kubernetes.io/service-account-token   3      9m46s
```

创建sa的同时还创建了secret。

pod配置secret可以拉取私有镜像源的镜像，默认情况下pod可以挂载任何secret。sa添加注解：kubernetes.io/enforce-mountable-secrets="true" 可以使pod只允许挂载sa中列出的secret。

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-sa
imagePullSecrets:
- name: my-dockerhub-secret
```

添加到sa中的镜像拉取secret会自动添加到关联的所有pod中，无需对每个pod单独配置。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  # 使用my-sa而非默认的sa
  serviceAccountName: my-sa
  containers:
  - name: main
    image: tutum/curl
    command: ["sleep", "99999"]
  
```

### RBAC

RBAC 是基于角色的访问控制（Role-Based Access Control ）在 RBAC 中，权限与角色相关联，用户通过成为适当角色的成员而得到这些角色的权限。这就极大地简化了权限的管理。这样管理都是层级相互依赖的，权限赋予给角色，而把角色又赋予用户，这样的权限设计很清楚，管理起来很方便。

RBAC  认为授权实际上是Who 、What 、How 三元组之间的关系，也就是Who 对What 进行How 的操作，也就是“主体”对“客体”的操作。Who：是权限的拥有者或主体（如：User，Role）。What：是操作或对象（operation，object）。How：具体的权限（Privilege,正向授权与负向授权）。

k8s提供了四种资源，Role、ClusterRole、RoleBinding、ClusterRoleBinding，其中Role/RoleBing属于特定的命名空间。

:----: | :----:
角色类型 | 绑定类型 | 访问的资源
ClusterRole | ClusterRoleBinding | 非资源型URL（/api /healthz ...）
ClusterRole | ClusterRoleBinding | 集群级别的资源（Nodes PersistVolumes ...）
ClusterRole | ClusterRoleBinding | 任何命名空间中的资源
ClusterRole | RoleBinding | RoleBinding 所在ns的资源
Role | RoleBinding | Role/RoleBinding 所在ns的资源

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: foo

kind: Pod
apiVersion: v1
metadata:
  name: test
  namespace: foo
spec:
  containers:
  - image: luksa/kubectl-proxy
---

apiVersion: v1
kind: Namespace
metadata:
  name: bar

kind: Pod
apiVersion: v1
metadata:
  name: test
  namespace: bar
spec:
  containers:
  - image: luksa/kubectl-proxy
```

```shell
$ kubectl exec -it test -n foo -- sh
/ # curl localhost:8001/api/v1/namespaces/foo/services
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {
    
  },
  "status": "Failure",
  "message": "services is forbidden: User \"system:serviceaccount:foo:default\" cannot list resource \"services\" in API group \"\" in the namespace \"foo\"",
  "reason": "Forbidden",
  "details": {
    "kind": "services"
  },
  "code": 403
```

Service Account（system:serviceaccount:foo:default）没有权限列出foo的服务资源。

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: service-reader
  namespace: foo
rules:
  # service资源是核心apiGroup的资源所以为空
- apiGroups: [""]
  # 允许的动作
  verbs: ["get", "list"]
  # 必须使用复数形式
  resources: ["services"]

---

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: service-reader
  namespace: bar
rules:
  # service资源是核心apiGroup的资源所以为空
- apiGroups: [""]
  # 允许的动作
  verbs: ["get", "list"]
  # 必须使用复数形式
  resources: ["services"]

---

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: test
  namespace: foo
roleRef:
  # 引用 service-reader角色
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: service-reader
subjects:
  # 将其绑定到foo命名空间下的default sa上
- kind: ServiceAccount
  name: default
  namespace: foo
  # 绑定的sa可以跨namespace
- kind: ServiceAccount
  name: default
  namespace: bar
```

foo下的service-reader可以获取和列出foo的服务，bar下的service-reader可以获取和列出bar的服务。foo下的rolebinding将foo、bar两个命名空间下的default ServiceAccount绑定，使得foo、bar两个命名空间下的pod都可以扮演service-reader角色。

```shell
$ kubectl exec -it test -n foo -- sh
/ # curl localhost:8001/api/v1/namespaces/foo/services
{
  "kind": "ServiceList",
  "apiVersion": "v1",
  "metadata": {
    "resourceVersion": "356482"
  },
  "items": []
```

一些特定资源不在任何命名空间下，API服务器也对面暴露了一些不表示资源的URL路径。常规角色不能对这些内容进行授权，需要ClusterRole。

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: pv-reader
rules:
  # persistvolumes资源是核心apiGroup的资源所以为空
- apiGroups: [""]
  # 允许的动作 create update watch等
  verbs: ["get", "list"]
  # 必须使用复数形式
  resources: ["persistvolumes"]

---

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: pv-test
roleRef:
  # 引用 pv-reader角色
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: pv-reader
subjects:
  # 将其绑定到foo命名空间下的default sa上
- kind: ServiceAccount
  name: default
  namespace: foo
```

```shell
$ kubectl get clusterrole system:discovery -o yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  creationTimestamp: "2021-01-22T09:44:37Z"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  managedFields:
  - apiVersion: rbac.authorization.k8s.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:rbac.authorization.kubernetes.io/autoupdate: {}
        f:labels:
          .: {}
          f:kubernetes.io/bootstrapping: {}
      f:rules: {}
    manager: kube-apiserver
    operation: Update
    time: "2021-01-22T09:44:37Z"
  name: system:discovery
  resourceVersion: "84"
  uid: d8489382-8e49-4a13-bcd0-bc4811c5b3fc
rules:
# 非资源型URL
- nonResourceURLs:
  - /api
  - /api/*
  - /apis
  - /apis/*
  - /healthz
  - /livez
  - /openapi
  - /openapi/*
  - /readyz
  - /version
  - /version/
  verbs:
  # 非资源型URL允许的动作为HTTP方法 get post patch等
  - get


$ kubectl get clusterrolebinding system:discovery -o yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  creationTimestamp: "2021-01-22T09:44:37Z"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  managedFields:
  - apiVersion: rbac.authorization.k8s.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:rbac.authorization.kubernetes.io/autoupdate: {}
        f:labels:
          .: {}
          f:kubernetes.io/bootstrapping: {}
      f:roleRef:
        f:apiGroup: {}
        f:kind: {}
        f:name: {}
      f:subjects: {}
    manager: kube-apiserver
    operation: Update
    time: "2021-01-22T09:44:37Z"
  name: system:discovery
  resourceVersion: "145"
  uid: f3f81461-9a25-42fe-bd7d-071c1f0218d0
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:discovery
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:authenticated

```

k8s提供了一组默认的clusterrole和clusterrolebinding，每次API服务启动时都会更新，防止被篡改。view、edit、cluster-admin是最重要的clusterrole。view只允许读取一个命名空间中的大多数资源，除了role、rolebinding和secret；edit允许修改一个命名空间的资源资源，允许读取修改secret，单不允许查看修改role、rolebinding；admin提供了一个命名空间的完全控制权，除了ResourceQuota和命名空间资源本身；cluster-admin则提供了集群的完全控制权。

## 容器/pod的安全策略

## 计算资源管理

pod创建时可以通过requests/limits指定cpu与内存的申请量和使用上限。k8s调度pod只会考虑requests，只有满足pod的requests小于节点剩余资源（节点总资源减去申请量之和，而不是实际使用量之和）的节点才会被考虑调度pod。所有满足要求的节点会按照更进一步的策略进一步筛选。

k8s按照申请量而不是限制量执行调度策略，则资源超卖是必然存在的。cpu资源是可压缩资源，控制cpu时间即可控制cpu资源的配比；内存是不可压缩资源，内存不足时必然会杀掉一些pod。

pod的Qos等级有三个：best effort、burstable、guaranteed。pod的所有容器均设置了相同的cpu和memory requests和limits，则为guaranteed；pod的所有容器都没有设置任何requests和limits，则为best effort；其余情况则为burstable。资源保证优先级：guaranteed > burstable > best effort，QoS低者将会先被杀掉；相同 Qos 级别的pod比较 OOM 分数，实际使用内存/申请内存比值越大（超的越离谱）的先被杀掉。

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: kubia-pod
  labels:
    name: kubia-pod
    env: testing
spec:
  containers:
  - name: kubia-container
    image: luksa/kubia
    resources:
      requests:
        memory: "64Mi"
        cpu: "200m"
      limits:
        memory: "128Mi"
        cpu: "500m"
    ports:
      - containerPort: 8080
        protocol: TCP
```

cpu单位 m 是指毫核，1 = 1000 m，如果 cpu = 200 m（1/5核），节点有8核，则 cpu 占用为1/40。memory 有两种单位，Mi = 1024 x 1024 byte，M = 1000 x 1000 byte。

### LimitRange

LimitRange资源可以指定能给容器配置的每种资源的最大和最小限制，还支持在没有显式指定资源requests时为容器设置默认值。API服务器接收带有pod描述信息的POST请求，LimitRanger插件对pod spec进行校验，拒绝校验失败的请求。

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: example
  namespace: default
spec:
  limits:
  # pod中所有容器的最小和最大资源限制
  - type: Pod
    min:
      cpu: 50m
      memory: 5Mi
    max:
      cpu: 1
      memory: 1Gi
  - type: Container
    defaultRequest:
      cpu: 100m
      memory: 10Mi
    # default for limits
    default:
      cpu: 200m
      memory: 100Mi
    min:
      cpu: 50m
      memory: 5Mi
    max:
      cpu: 1
      memory: 1Gi
    # 每种资源的 requests/limits 的最大值
    maxLimitRequestRatio:
      cpu: 4
      memory: 10
  - type: PersistentVolumeClaim
    min: 
      storage: 1Gi
    max:
      storage: 10Gi
```

```shell
$ kubectl describe limitrange example

Name:                  example
Namespace:             default
Type                   Resource  Min  Max   Default Request  Default Limit  Max Limit/Request Ratio
----                   --------  ---  ---   ---------------  -------------  -----------------------
Pod                    memory    5Mi  1Gi   -                -              -
Pod                    cpu       50m  1     -                -              -
Container              cpu       50m  1     100m             200m           4
Container              memory    5Mi  1Gi   10Mi             100Mi          10
PersistentVolumeClaim  storage   1Gi  10Gi  -                -              -
```

LimitRange限制了所在命名空间中单个pod的资源，但不能防止用户创建大量pod吃掉集群资源。

### ResourceQuota

ResourceQuota 可以限制命名空间中的可用资源总量。

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: cpu-and-mem
spec:
  hard:
    requests.cpu: 800m
    requests.memory: 1000Mi
    limits.cpu: 900m
    limits.memory: 1500Mi

```

```shell
$ kubectl describe resourcequota

Name:            cpu-and-mem
Namespace:       default
Resource         Used  Hard
--------         ----  ----
limits.cpu       0     900m
limits.memory    0     1500Mi
requests.cpu     0     800m
requests.memory  0     1000Mi
```

ResourceQuota 隶属命名空间，限制了该命名空间下所有 pod 资源的 requests/limits 总量。创建 ResourceQuota 往往伴随着 limitrange 的创建。否则没有指定配额的 pod 将不允许创建。

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: storage
spec:
  hard:
    requests.storage: 500Gi
    ssd.storageclass.storage.k8s.io/requests.storage: 300Gi
    standard.storageclass.storage.k8s.io/requests.storage: 1Ti
```

同理可以限制 PV 总量，与 cpu/mem 设置的语法相似。每种资源的数量也可以限制，具体种类和 k8s 版本相关。

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: objects
spec:
  hard:
    pods: 10
    replicationcontrollers: 5
    secrets: 10
    configmaps: 10
    persistentvolumeclaims: 5
    services: 5
    services.loadbalancers: 1
    services.nodeports: 2
    ssd.storageclass.storage.k8s.io/persistentvolumeclaims: 2
```

quota scopes可以为特定的pod状态或者Qos等级指定配额。配额的作用范围有四种：BestEffort、NotBestEffort、Terminating、NotTerminating。

前两者用于决定配额是否应用于不同Qos等级的pod，后两者则于pod配置的activeDeadlineSeconds有关。标记为Failed的pod在真正停止之前还可以运行多久由该属性指定，Terminating配额适用于配置了这个属性的pod，NotTerminating配额适用于没有配置的。

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: besteffort-notterminating-pods
spec:
  scopes:
  - BestEffort
  - NotTerminating
  hard:
    pods: 4
```

上例指定了处于BestEffort的Qos等级并且没有设置有效期的pod数量只允许存在四个。BestEffort只能限制pod个数，其他三个范围还可以限制cpu/mem的requests/limits。

### HPA

横向自动伸缩是指由控制器管理的pod副本数量的自动伸缩，由Horizontal控制器执行。创建HorizontalpodAutoScaler资源来启用和配置该控制器。HPA会自动调整Deployment/RS/RC/StatefulSet的replicas字段。Pod 自动扩缩不适用于无法扩缩的对象，比如 DaemonSet。

Pod 水平自动扩缩特性由 Kubernetes API 资源和控制器实现。资源决定了控制器的行为。 控制器会周期性的调整副本控制器或 Deployment 中的副本数量，以使得 Pod 的平均 CPU 利用率与用户所设定的目标值匹配。

HPA（Horizontal Pod Autoscaler）Pod自动弹性伸缩，K8S通过对Pod中运行的容器各项指标（CPU占用、内存占用、网络请求量）的检测，实现对Pod实例个数的动态新增和减少。早期的kubernetes版本，只支持CPU指标的检测，因为它是通过kubernetes自带的监控系统heapster实现的。到了kubernetes 1.8版本后，heapster已经弃用，资源指标主要通过metrics api获取，这时能支持检测的指标就变多了（CPU、内存等核心指标和qps等自定义指标）。

```shell
$ kubectl get deployments.apps -o wide
NAME    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES           SELECTOR
kubia   3/3     3            3           70m   nodejs       luksa/kubia:v1   app=kubia

$ kubectl autoscale deployment kubia --cpu-percent=30 --min=1 --max=5
kuhorizontalpodautoscaler.autoscaling/kubia autoscaled

$ kubectl describe hpa
Name:                                                  kubia
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Fri, 29 Jan 2021 13:46:15 +0800
Reference:                                             Deployment/kubia
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  <unknown> / 30%
Min replicas:                                          1
Max replicas:                                          5
Deployment pods:                                       3 current / 0 desired
Conditions:
  Type           Status  Reason                   Message
  ----           ------  ------                   -------
  AbleToScale    True    SucceededGetScale        the HPA controller was able to get the target's current scale
  ScalingActive  False   FailedGetResourceMetric  the HPA was unable to compute the replica count: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
Events:
  Type     Reason                        Age               From                       Message
  ----     ------                        ----              ----                       -------
  Warning  FailedGetResourceMetric       7s (x3 over 37s)  horizontal-pod-autoscaler  failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  Warning  FailedComputeMetricsReplicas  7s (x3 over 37s)  horizontal-pod-autoscaler  invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
```

Pod 水平自动扩缩器的实现是一个控制回路，由控制器管理器的 --horizontal-pod-autoscaler-sync-period 参数指定周期（默认值为 15 秒）。

每个周期内，控制器管理器根据每个 HorizontalPodAutoscaler 定义中指定的指标查询资源利用率。 控制器管理器可以从资源度量指标 API（按 Pod 统计的资源用量）和自定义度量指标 API（其他指标）获取度量值。

对于按 Pod 统计的资源指标（如 CPU），控制器从资源指标 API 中获取每一个 HorizontalPodAutoscaler 指定的 Pod 的度量值，如果设置了目标使用率， 控制器获取每个 Pod 中的容器资源使用情况，并计算资源使用率。 如果设置了 target 值，将直接使用原始数据（不再计算百分比）。 接下来，控制器根据平均的资源使用率或原始值计算出扩缩的比例，进而计算出目标副本数。需要注意的是，如果 Pod 某些容器不支持资源采集，那么控制器将不会使用该 Pod 的 CPU 使用率。 下面的算法细节章节将会介绍详细的算法。如果 Pod 使用自定义指示，控制器机制与资源指标类似，区别在于自定义指标只使用 原始值，而不是使用率。如果 Pod 使用对象指标和外部指标（每个指标描述一个对象信息）。 这个指标将直接根据目标设定值相比较，并生成一个上面提到的扩缩比例。 在 autoscaling/v2beta2 版本 API 中，这个指标也可以根据 Pod 数量平分后再计算。

通常情况下，控制器将从一系列的聚合 API（metrics.k8s.io、custom.metrics.k8s.io 和 external.metrics.k8s.io）中获取度量值。 metrics.k8s.io API 通常由 Metrics 服务器（需要额外启动）提供。 可以从 metrics-server 获取更多信息。 另外，控制器也可以直接从 Heapster 获取指标。

#### 算法细节

从最基本的角度来看，Pod 水平自动扩缩控制器根据当前指标和期望指标来计算扩缩比例。

期望副本数 = ceil[当前副本数 * (当前指标 / 期望指标)]

例如，当前度量值为 200m，目标设定值为 100m，那么由于 200.0/100.0 == 2.0， 副本数量将会翻倍。 如果当前指标为 50m，副本数量将会减半，因为50.0/100.0 == 0.5。 如果计算出的扩缩比例接近 1.0 （根据--horizontal-pod-autoscaler-tolerance 参数全局配置的容忍值，默认为 0.1）， 将会放弃本次扩缩。

如果 HorizontalPodAutoscaler 指定的是 targetAverageValue 或 targetAverageUtilization， 那么将会把指定 Pod 度量值的平均值做为 currentMetricValue。 然而，在检查容忍度和决定最终扩缩值前，我们仍然会把那些无法获取指标的 Pod 统计进去。

所有被标记了删除时间戳（Pod 正在关闭过程中）的 Pod 和失败的 Pod 都会被忽略。如果某个 Pod 缺失度量值，它将会被搁置，只在最终确定扩缩数量时再考虑。当使用 CPU 指标来扩缩时，任何还未就绪（例如还在初始化）状态的 Pod 或 最近的指标 度量值采集于就绪状态前的 Pod，该 Pod 也会被搁置。

由于受技术限制，Pod 水平扩缩控制器无法准确的知道 Pod 什么时候就绪， 也就无法决定是否暂时搁置该 Pod。 --horizontal-pod-autoscaler-initial-readiness-delay 参数（默认为 30s）用于设置 Pod 准备时间， 在此时间内的 Pod 统统被认为未就绪。 --horizontal-pod-autoscaler-cpu-initialization-period 参数（默认为5分钟） 用于设置 Pod 的初始化时间， 在此时间内的 Pod，CPU 资源度量值将不会被采纳。

在排除掉被搁置的 Pod 后，扩缩比例就会根据 currentMetricValue/desiredMetricValue 计算出来。

如果缺失任何的度量值，我们会更保守地重新计算平均值， 在需要缩小时假设这些 Pod 消耗了目标值的 100%， 在需要放大时假设这些 Pod 消耗了 0% 目标值。 这可以在一定程度上抑制扩缩的幅度。

此外，如果存在任何尚未就绪的 Pod，我们可以在不考虑遗漏指标或尚未就绪的 Pod 的情况下进行扩缩， 我们保守地假设尚未就绪的 Pod 消耗了期望指标的 0%，从而进一步降低了扩缩的幅度。

在扩缩方向（缩小或放大）确定后，我们会把未就绪的 Pod 和缺少指标的 Pod 考虑进来再次计算使用率。 如果新的比率与扩缩方向相反，或者在容忍范围内，则跳过扩缩。 否则，我们使用新的扩缩比例。

注意，平均利用率的原始值会通过 HorizontalPodAutoscaler 的状态体现（ 即使使用了新的使用率，也不考虑未就绪 Pod 和 缺少指标的 Pod)。

如果创建 HorizontalPodAutoscaler 时指定了多个指标， 那么会按照每个指标分别计算扩缩副本数，取最大值进行扩缩。 如果任何一个指标无法顺利地计算出扩缩副本数（比如，通过 API 获取指标时出错）， 并且可获取的指标建议缩容，那么本次扩缩会被跳过。 这表示，如果一个或多个指标给出的 desiredReplicas 值大于当前值，HPA 仍然能实现扩容。

最后，在 HPA 控制器执行扩缩操作之前，会记录扩缩建议信息。 控制器会在操作时间窗口中考虑所有的建议信息，并从中选择得分最高的建议。 这个值可通过 kube-controller-manager 服务的启动参数 --horizontal-pod-autoscaler-downscale-stabilization 进行配置， 默认值为 5 分钟。 这个配置可以让系统更为平滑地进行缩容操作，从而消除短时间内指标值快速波动产生的影响。

```shell
$ kubectl autoscale deployment kubia --cpu-percent=30 --min=2 --max=5
horizontalpodautoscaler.autoscaling/kubia autoscaled

$ kubectl describe hpa
Name:                                                  kubia
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Fri, 29 Jan 2021 15:12:24 +0800
Reference:                                             Deployment/kubia
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  0% (0) / 30%
Min replicas:                                          2
Max replicas:                                          5
Deployment pods:                                       4 current / 4 desired
Conditions:
  Type            Status  Reason               Message
  ----            ------  ------               -------
  AbleToScale     True    ScaleDownStabilized  recent recommendations were higher than current one, applying the highest recent recommendation
  ScalingActive   True    ValidMetricFound     the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)
  ScalingLimited  False   DesiredWithinRange   the desired count is within the acceptable range
Events:           <none>

$ kubectl get hpa -o yaml
apiVersion: v1
items:
- apiVersion: autoscaling/v1
  kind: HorizontalPodAutoscaler

  ...

  spec:
    maxReplicas: 5
    minReplicas: 2
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: kubia
    targetCPUUtilizationPercentage: 30
  status:
    currentCPUUtilizationPercentage: 0
    currentReplicas: 4
    desiredReplicas: 4
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""


$ kubectl get hpa                                                    
NAME    REFERENCE          TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
kubia   Deployment/kubia   0%/30%    2         5         4          4m47s

# 一段时间后发生了缩容
$ kubectl get hpa
NAME    REFERENCE          TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
kubia   Deployment/kubia   0%/30%    2         5         2          8m13s

```

cpu使用率是实际的cpu使用时间除以申请的cpu使用时间，这个比值可以超过100%。如果增加副本数量不能导致观测度量的平均值线性下降，那么autoscaler就不能正常工作。

云服务厂商可能提供了cluster autoscaler，可以动态申请和归还节点。

一些服务要求至少保持一定数量的pod持续运行，对于基于quorum的集群应用尤其如此。Kubernetes可以指定下线操作时需要保持的最少pod数量 —— PodDisruptionBudget。

```shell
$ kubectl create pdb kubia-pdb --selector=app=kubia --min-available=3
poddisruptionbudget.policy/kubia-pdb created

$ kubectl get pdb kubia-pdb                                   
NAME        MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
kubia-pdb   3               N/A               0                     17s

$ kubectl describe pdb kubia-pdb 
Name:           kubia-pdb
Namespace:      default
Min available:  3
Selector:       app=kubia
Status:
    Allowed disruptions:  0
    Current:              2
    Desired:              3
    Total:                2
Events:                   <none>

```
