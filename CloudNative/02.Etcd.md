# ETCD

etcd是一个可靠的分布式kv存储，其底层使用Raft算法保证一致性，主要用户共享配置和服务发现。etcd是CoreOS公司发起的一个开源项目，授权协议为Apache，其源代码地址为[etcd-io/etcd](https://github.com/etcd-io/etcd)。

目前提供配置共享和服务发现功能的组件还是比较多，其中应用最为广泛、大家最为熟悉的应该就是Zookeeper了，很多开源项目也都是在不同程度上依赖了Zookeeper，例如，Dubbo、Kafka等。

在Golang社区中，etcd则是唯一一个可以媲美Zookeeper的组件，在有些方面，etcd甚至超越了Zookeeper，给开发者眼前一亮的感觉。下面简单列举一下etcd相较于Zookeeper的优势。

>- 一致性协议：一致性协议是配置共享和服务发现组件的核心，etcd底层采用Raft协议，而Zookeeper使用ZAB协议，ZAB协议是一种类似Paxos的一致性协议。目前公认的是Raft比Paxos协议易于理解，工程化也较为容易。
>- API接口：etcd v2版本中提供了HTTP+JSON的调用方式，在etcd v3版本的客户端中则使用GRPC与服务端进行交互，而GRPC本身就跨平台的。
>- 性能：在官方提供的基准测试数据中，etcd集群可以支持每秒10000+次的写入，性能相当可观，优于Zookeeper。
>- 安全性：etcd支持TLS访问，而Zookeeper在权限控制方面做得略显粗糙。

## RAFT

Raft协议的工作模式是一个Leader节点和多个Follower节点的模式，也就是常说的Leader-Follower模式，在Raft协议中，每个节点都维护了一个状态机，该状态机有三种状态，分别是Leader状态、Follower状态和Candidate状态。

Leader节点负责处理所有客户端的请求，当接收到客户端的写入请求时，Leader节点会在本地追加一条相应的日志，然后将其封装成消息发送到集群中其他的Follower节点。当Follower节点收到该消息时会对其进行响应。如果集群中多数（超过半数）节点都已收到该请求对应的日志记录时，则Leader节点认为该条日志记录已提交（committed），可以向客户端返回响应。Leader节点的另一项工作是定期向集群中的Follewer节点发送心跳消息，这主要是为了防止进群中的其他Follower几点选举计时器超时而触发新一轮选举。

Follower节点不会发送任何请求，它们只是简单地响应来自Leader或者Candidate的请求：Follower节点也不处理Client的请求，而是将请求重定向到集群的Leader节点进行处理。

Candidate节点是由Follower节点转换而来的，当Follower节点长时间没有收到Leader节点发送的心跳消息时，则该节点的选举计时器就会过期，同时会将自身状态转换成Candidate，发起新一轮选举。

## 核心API

目前，核心API以服务类型划分为6大部分，具体参考包内proto文件的定义：github.com/coreos/etcd/etcdserver/etcdserverpb/rpc.proto

```go
// KV：键值相关操作
type KV interface {
    Put(ctx context.Context, key, val string, opts ...OpOption) (*PutResponse, error)
    Get(ctx context.Context, key string, opts ...OpOption) (*GetResponse, error)
    Delete(ctx context.Context, key string, opts ...OpOption) (*DeleteResponse, error)
    Compact(ctx context.Context, rev int64, opts ...CompactOption) (*CompactResponse, error)
    Do(ctx context.Context, op Op) (OpResponse, error)
    Txn(ctx context.Context) Txn
}
// Watch：观察者模式，监听数据变化
type Watcher interface {
    Watch(ctx context.Context, key string, opts ...OpOption) WatchChan
    Close() error
}
// Lease：租约相关操作
type Lease interface {
    Grant(ctx context.Context, ttl int64) (*LeaseGrantResponse, error)
    Revoke(ctx context.Context, id LeaseID) (*LeaseRevokeResponse, error)
    TimeToLive(ctx context.Context, id LeaseID, opts ...LeaseOption) (*LeaseTimeToLiveResponse, error)
    Leases(ctx context.Context) (*LeaseLeasesResponse, error)
    KeepAlive(ctx context.Context, id LeaseID) (<-chan *LeaseKeepAliveResponse, error)
    KeepAliveOnce(ctx context.Context, id LeaseID) (*LeaseKeepAliveResponse, error)
    Close() error
}
// Cluster：集群管理相关操作
type Cluster interface {
    MemberList(ctx context.Context) (*MemberListResponse, error)
    MemberAdd(ctx context.Context, peerAddrs []string) (*MemberAddResponse, error)
    MemberRemove(ctx context.Context, id uint64) (*MemberRemoveResponse, error)
    MemberUpdate(ctx context.Context, id uint64, peerAddrs []string) (*MemberUpdateResponse, error)
}
// Maintenance：维护操作
// 不常用，具体可参考包内文件：`github.com/coreos/etcd/clientv3/maintenance.go`
// Auth：用户及权限管理操作
// 不常用，具体可参考包内文件：`github.com/coreos/etcd/clientv3/auth.go`
```

## 并发API

```go
// Lock：分布式锁
type LockServer interface {
    Lock(context.Context, *LockRequest) (*LockResponse, error)
    Unlock(context.Context, *UnlockRequest) (*UnlockResponse, error)
}

type lockServer struct {
    c *clientv3.Client
}

// Election：选举
type ElectionClient interface {
    Campaign(ctx context.Context, in *CampaignRequest, opts ...grpc.CallOption) (*CampaignResponse, error)
    Proclaim(ctx context.Context, in *ProclaimRequest, opts ...grpc.CallOption) (*ProclaimResponse, error)
    Leader(ctx context.Context, in *LeaderRequest, opts ...grpc.CallOption) (*LeaderResponse, error)
    Observe(ctx context.Context, in *LeaderRequest, opts ...grpc.CallOption) (Election_ObserveClient, error)
    Resign(ctx context.Context, in *ResignRequest, opts ...grpc.CallOption) (*ResignResponse, error)
}

type electionServer struct {
    c *clientv3.Client
}
```

## Etcd-cpp-client

别用，没戏，转golang。
