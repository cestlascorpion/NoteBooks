# Kafka

## 为什么需要消息系统

>- 解耦：允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。
>- 冗余：消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的"插入-获取-删除"范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。
>- 扩展性：消息队列解耦了处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。
>- 灵活性 & 峰值处理能力：在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。
>- 可恢复性：系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。
>- 顺序保证：在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka 保证一个 Partition 内的消息的有序性）
>- 缓冲：有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。
>- 异步通信：很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。

## kafka架构

![kafka](../source/kafka.png)

## 相关概念

>- producer：消息生产者，发布消息到kafka集群的终端或服务。
>- broker：kafka集群中包含的服务器。
>- topic：每条发布到kafka集群的消息属于的类别，即kafka是面向topic的。
>- partition：partition是物理上的概念，每个topic包含一个或多个partition。kafka分配的单位是partition。
>- consumer：从kafka集群中消费消息的终端或服务。
>- Consumer group：high-level consumer API中，每个consumer都属于一个consumer group，每条消息只能被consumer group中的一个Consumer消费，但可以被多个consumer group消费。
>- replica：partition的副本，保障partition的高可用。
>- leader：replica中的一个角色， producer和consumer只跟leader交互。
>- follower：replica中的一个角色，从leader中复制数据。
>- controller：kafka集群中的其中一个服务器，用来进行leader election以及各种failover。
>- zookeeper：kafka通过zookeeper来存储集群的meta信息。

## kafka在zk中的存储结构

![kafka_zk](../source/kafka_zk.png)

## producer发布消息

写入方式

producer采用push模式将消息发布到broker，每条消息都被append到patition中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障kafka吞吐率）。

消息路由

producer发送消息到broker时，会根据分区算法选择将其存储到哪一个partition。其路由机制为：

>1. 指定了patition，则直接使用；
>2. 未指定patition但指定key，通过对key的value进行hash选出一个patition。
>3. patition和key都未指定，使用轮询选出一个patition。

写入流程

>1. producer先从zookeeper的"/brokers/.../state"节点找到该partition的leader。
>2. producer将消息发送给该leader。
>3. leader将消息写入本地log。
>4. followers从leader pull消息，写入本地log后leader发送ACK。
>5. leader收到所有ISR中的replica的ACK后，增加HW（high watermark，最后commit的offset）并向producer发送ACK

![kafka](../source/kafka_push.png)

producer delivery guarantee

一般情况下存在三种情况：

>1. At most once 消息可能会丢，但绝不会重复传输。
>2. At least one 消息绝不会丢，但可能会重复传输。
>3. Exactly once 每条消息肯定会被传输一次且仅传输一次。

当producer向broker发送消息时，一旦这条消息被commit，由于replication的存在，它就不会丢。但是如果producer发送数据给broker后，遇到网络问题而造成通信中断，那Producer就无法判断该条消息是否已经commit。虽然Kafka无法确定网络故障期间发生了什么，但是producer可以生成一种类似于主键的东西，发生故障时幂等性的重试多次，这样就做到了Exactly once，但目前还并未实现。所以目前默认情况下一条消息从producer到broker是确保了At least once，可通过设置producer异步发送实现At most once。

## broker保存消息

存储方式

物理上把topic分成一个或多个patition，每个patition物理上对应一个文件夹（该文件夹存储该patition的所有消息和索引文件）。

存储策略

无论消息是否被消费，kafka都会保留所有消息。有两种策略可以删除旧数据：

>1. 基于时间：log.retention.hours=168
>2. 基于大小：log.retention.bytes=1073741824

需要注意的是，因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高Kafka性能无关。

topic创建

>1. controller在ZooKeeper的/brokers/topics节点上注册watcher，当topic被创建，则controller会通过watch得到该topic的partition/replica分配。
>2. controller从/brokers/ids读取当前所有可用的broker列表，对于set_p中的每一个partition：a)从分配给该partition的所有replica（称为AR）中任选一个可用的broker作为新的leader，并将AR设置为新的ISR; b)将新的leader和ISR写入/brokers/topics/[topic]/partitions/[partition]/state
>3. controller通过RPC向相关的broker发送LeaderAndISRRequest。

![kafka_topic](../source/kafka_topic.png)

topic删除

>1. controller在zooKeeper的/brokers/topics节点上注册watcher，当topic被删除，则controller会通过watch得到该topic的partition/replica分配。
>2. 若delete.topic.enable=false，结束；否则controller注册在/admin/delete_topics上的watch被fire，controller通过回调向对应的broker发送StopReplicaRequest。

![kafka_topic2](../source/kafka_topic2.png)

## kafka HA

replication

同一个partition可能会有多个replica，没有replica的情况下，一旦broker宕机，其上所有patition的数据都不可被消费，同时producer也不能再将数据存于其上的patition。引入replication之后，同一个partition可能会有多个replica，而这时需要在这些replica之间选出一个leader，producer和consumer只与这个leader交互，其它replica作为follower从leader中复制数据。

Kafka分配Replica的算法如下：

>1. 将所有broker（假设共n个broker）和待分配的partition排序。
>2. 将第i个partition分配到第（i mod n）个broker上。
>3. 将第i个partition的第j个replica分配到第（(i+j)moden）个broker上。

leader failover

当partition对应的leader宕机时，需要从follower中选举出新leader。在选举新leader时，一个基本的原则是，新的leader必须拥有旧leadercommit过的所有消息。

kafka在zookeeper中（/brokers/.../state）动态维护了一个ISR（in-sync replicas），由写入流程可知ISR里面的所有replica都跟上了leader，只有ISR里面的成员才能选为leader。对于f+1个replica，一个partition可以在容忍f个replica失效的情况下保证消息不丢失。

当所有replica都不工作时，有两种可行的方案：

>1. 等待ISR中的任一个replica活过来，并选它作为leader。可保障数据不丢失，但时间可能相对较长。
>2. 选择第一个活过来的replica（不一定是ISR成员）作为leader。无法保障数据不丢失，但相对不可用时间较短。

kafka0.8.*使用第二种方式。

kafka通过Controller来选举leader，流程请参考如下。

broker failover

>1. controller在zookeeper的/brokers/ids/[brokerId]节点注册Watcher，当broker宕机时zookeeper会fire watch。
>2. controller从/brokers/ids节点读取可用broker。
>3. controller决定set_p，该集合包含宕机broker上的所有partition。
>4. 对set_p中的每一个partition。a)从/brokers/topics/[topic]/partitions/[partition]/state节点读取ISR; b)决定新leader; c)将新leader、ISR、controller_epoch和leader_epoch等信息写入state节点。
>5. 通过RPC向相关broker发送leaderAndISRRequest命令。

![kafka_failover](../source/kafka_failover.png)

controller failover

当controller宕机时会触发controller failover。每个broker都会在zookeeper的"/controller"节点注册watcher，当controller宕机时zookeeper中的临时节点消失，所有存活的broker收到fire的通知，每个broker都尝试创建新的controller path，只有一个竞选成功并当选为controller。

当新的controller当选时，会触发KafkaController.onControllerFailover方法，在该方法中完成如下操作：

>1. 读取并增加Controller Epoch。
>2. 在reassignedPartitions Patch(/admin/reassign_partitions)上注册watcher。
>3. 在preferredReplicaElection Path(/admin/preferred_replica_election)上注册 watcher。
>4. 通过partitionStateMachine在broker Topics Patch(/brokers/topics)上注册>watcher。
>5. 若delete.topic.enable=true（默认值是false），则partitionStateMachine在Delete Topic Patch(/admin/delete_topics)上注册watcher。
>6. 通过replicaStateMachine在Broker Ids Patch(/brokers/ids)上注册Watch。
>7. 初始化ControllerContext对象，设置当前所有topic，活着的broker列表，所有partition的leader及ISR等。
>8. 启动replicaStateMachine和partitionStateMachine。
>9. 将brokerState状态设置为RunningAsController。
>10. 将每个partition的Leadership信息发送给所有活着的broker。
>11. 若auto.leader.rebalance.enable=true（默认值是true），则启动partition-rebalance线程。
>12. 若delete.topic.enable=true且Delete Topic Patch(/admin/delete_topics)中有值，则删除相应的Topic。

## consumer消费消息

kafka提供了两套consumer API：The high-level Consumer API和The SimpleConsumer API。其中high-level consumer API提供了一个从kafka消费数据的高层抽象，而SimpleConsumer API则需要开发人员更多地关注细节。

high-level consumer API提供了consumer group的语义，一个消息只能被group内的一个consumer所消费，且consumer消费消息时不关注offset，最后一个offset由zookeeper保存。如果消费线程大于patition数量，则有些线程将收不到消息。如果patition数量大于线程数，则有些线程多收到多个patition的消息。如果一个线程消费多个patition，则无法保证你收到的消息的顺序，而一个patition内的消息是有序的。

consumer group

kafka的分配单位是patition。每个consumer都属于一个group，一个partition只能被同一个group内的一个consumer所消费（也就保障了一个消息只能被group内的一个consuemr所消费），但是多个group可以同时消费这个partition。

kafka的设计目标之一就是同时实现离线处理和实时处理，根据这一特性，可以使用spark/Storm这些实时处理系统对消息在线处理，同时使用Hadoop批处理系统进行离线处理，还可以将数据备份到另一个数据中心，只需要保证这三者属于不同的consumergroup。

消费方式

consumer采用pull模式从broker中读取数据。push模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。对于Kafka而言，pull模式更合适，它可简化broker的设计，consumer可自主控制消费消息的速率，同时consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。

consumer delivery guarantee

如果将consumer设置为autocommit，consumer一旦读到数据立即自动commit。如果只讨论这一读取消息的过程，那Kafka确保了Exactly once。但实际使用中应用程序并非在consumer读取完数据就结束了，而是要进行进一步处理，而数据处理与commit的顺序在很大程度上决定了consumer delivery guarantee。

>- 读完消息先commit再处理消息。这种模式下，如果consumer在commit后还没来得及处理消息就crash了，下次重新开始工作后就无法读到刚刚已提交而未处理的消息，这就对应于At most once。
>- 读完消息先处理再commit。这种模式下，如果在处理完消息之后commit之前consumer crash了，下次重新开始工作时还会处理刚刚未commit的消息，实际上该消息已经被处理过了。这就对应于At least once。
>- 如果一定要做到Exactly once，就需要协调offset和实际操作的输出。经典做法是引入两阶段提交。如果能让offset和操作输入存在同一个地方，会更简洁和通用。这种方式可能更好，因为许多输出系统可能不支持两阶段提交。比如，consumer拿到数据后可能把数据放到HDFS，如果把最新的offset和数据本身一起写到HDFS，那就可以保证数据的输出和offset的更新要么都完成，要么都不完成，间接实现Exactly once。（目前就high-level API而言，offset是存于Zookeeper中的，无法存于HDFS，而SimpleConsuemr API的 offset是由自己去维护的，可以将之存于HDFS中）。

总之，Kafka默认保证At least once，并且允许通过设置producer异步提交来实现At most once。而Exactly once要求与外部存储系统协作，幸运的是kafka提供的offset可以非常直接非常容易得使用这种方式。

consumer rebalance

当有consumer加入或退出、以及partition的改变（如broker加入或退出）时会触发rebalance。consumer rebalance算法如下：

>1. 将目标topic下的所有partirtion排序，存于PT。
>2. 对某consumer group下所有consumer排序，存于CG，第i个consumer记为Ci。
>3. N=size(PT)/size(CG)，向上取整。
>4. 解除Ci对原来分配的partition的消费权（i从0开始）。
>5. 将第i*N到(i+1)*N-1个partition分配给Ci。

在0.8.* 版本，每个consumer都只负责调整自己所消费的partition，为了保证整个consumer group的一致性，当一个consumer触发了rebalance时，该consumer group内的其它所有其它consumer也应该同时触发rebalance。这会导致以下几个问题：任何broker或者consumer的增减都会触发所有的consumer的rebalance；每个consumer分别单独通过zookeeper判断哪些broker和consumer宕机了，那么不同consumer在同一时刻从zookeeper看到的view就可能不一样，这是由zookeeper的特性决定的，这就会造成不正确的reblance尝试；调整结果不可控，所有的consumer都并不知道其它consumer的rebalance是否成功，这可能会导致kafka工作在一个不正确的状态。基于以上问题，kafka 设计者考虑在0.9.* 版本开始使用中心coordinator来控制consumer rebalance，然后又从简便性和验证要求两方面考虑，计划在consumer客户端实现分配方案。
